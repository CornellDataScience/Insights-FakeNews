{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from collections import OrderedDict\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"./test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_distinct_dict(rows):\n",
    "    counts = dict()\n",
    "    for x in rows:\n",
    "        xs = x[-1]\n",
    "        if xs not in counts: \n",
    "            #Add it to the counts dictionary\n",
    "            counts[xs] = 0\n",
    "        counts[xs] += 1\n",
    "    return counts\n",
    "\n",
    "def entropy(X):\n",
    "    \"\"\"\n",
    "    Calculate Entropy (as per Octavian)\n",
    "    \"\"\"\n",
    "    counts = n_distinct_dict(X)\n",
    "    log_2 = lambda x: log(x)/log(2)\n",
    "    #Declare entropy value\n",
    "    entropy = 0.0\n",
    "    \n",
    "    for c in counts:\n",
    "        #Calculate P(C_i)\n",
    "        p = float(counts[c])/len(X)\n",
    "        entropy = entropy -  p*log_2(p)\n",
    "    return entropy\n",
    "\n",
    "def gini(X):\n",
    "    \"\"\"\n",
    "    Calculate Gini Index\n",
    "    \"\"\"\n",
    "    total = len(X)\n",
    "    counts = n_distinct_dict(X)\n",
    "    imp = 0.0\n",
    "    \n",
    "    for k1 in counts:\n",
    "        p1 = float(counts[k1])/total  \n",
    "        for k2 in counts:\n",
    "            if k1 == k2: continue\n",
    "            p2 = float(counts[k2])/total\n",
    "            imp += p1*p2\n",
    "    return imp\n",
    "\n",
    "def var(X):\n",
    "    \"\"\"\n",
    "    Calculate Variance\n",
    "    \"\"\"\n",
    "    if len(X) == 0:\n",
    "        return 0\n",
    "    x = [float(c[len(c) - 1]) for c in X]\n",
    "    mean = sum(x) / len(x)\n",
    "\n",
    "    variance = sum([(d - mean)**2 for d in x]) / len(x)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Test1', 'Test2']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_to_dict(x):\n",
    "    dictionary = dict()\n",
    "    for col in x:\n",
    "        dictionary.update({col:x[col]})\n",
    "    return dictionary\n",
    "test = df_to_dict(df)\n",
    "list(OrderedDict.fromkeys(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test1': 0    1\n",
      "1    2\n",
      "2    3\n",
      "3    1\n",
      "Name: Test1, dtype: int64, 'Test2': 0    1\n",
      "1    2\n",
      "2    3\n",
      "3    1\n",
      "Name: Test2, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    \"\"\"\n",
    "    Decision Tree class\n",
    "    \"\"\"\n",
    "    def __init__(self, col=-1, value=None, right_branch=None, left_branch=None, results=None):\n",
    "        self.col = col\n",
    "        self.value = value\n",
    "        self.right_branch = right_branch\n",
    "        self.left_branch = left_branch\n",
    "        self.results = results\n",
    "\n",
    "\n",
    "def prune_tree(tree, least_gain, eval_fun = entropy):\n",
    "    \"\"\"\n",
    "    tree : type Tree\n",
    "    eval_fun : entropy(X) or gini(X)\n",
    "    least_gain : float\n",
    "    \"\"\"\n",
    "    \n",
    "    if tree.right_branch.results == None: #if the right branch is a node\n",
    "        prune_tree(tree.right_branch, eval_fun, least_gain)\n",
    "    if tree.left_branch.results == None: #if the left branch is a node\n",
    "        prune_tree(tree.left_branch, eval_fun, least_gain)\n",
    "    if (tree.trueBranch.results != None) and (tree.falseBranch.results != None):\n",
    "        right, left = [], []\n",
    "        for v, c in tree.right_branch.results.items(): \n",
    "            right += [[v]] * c\n",
    "        for v, c in tree.left_branch.results.items(): \n",
    "            left += [[v]] * c\n",
    "        p = float(len(right)) / len(left + right)\n",
    "        diff_entropy = evaluationFunction(tb+fb) - p*evaluationFunction(tb) - (1-p)*evaluationFunction(fb)\n",
    "        if diff_entropy < least_gain:\n",
    "            tree.right_branch, tree.left_branch = None, None\n",
    "            tree.results = uniqueCounts(left + right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHelper functions\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helper functions\n",
    "\"\"\"\n",
    "def remove_duplicates():\n",
    "\n",
    "\n",
    "def C45(X, Y):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition(r, c, val):\n",
    "    \"\"\"\n",
    "    Function to partition the data based on value\n",
    "    \"\"\"\n",
    "    #Declare anonymous function\n",
    "    split_fun = None\n",
    "    if isinstance(val, float) or isinstance(val, int): \n",
    "        #Anonymous function for numeric values\n",
    "        split_fun = lambda row : row[c] >= val\n",
    "    else: \n",
    "        #For string values\n",
    "        split_fun = lambda row : row[c] == val\n",
    "    list1 = [row for row in r if split_fun(row)]\n",
    "    list2 = [row for row in r if not split_fun(row)]\n",
    "    return (list1, list2)\n",
    "\n",
    "\n",
    "\n",
    "def construct_tree(df, criteria = entropy):\n",
    "    \"\"\"\n",
    "    Decision tree construction - by default, the entropy function is used to calculate the criteria for splitting. \n",
    "    df : dataframe with the last column reserved for labels\n",
    "    criteria : entropy or gini calculation function\n",
    "    \"\"\"\n",
    "    #Base Case: Empty Set\n",
    "    if len(df) == 0: \n",
    "        return Tree()\n",
    "    \n",
    "    #Calculate Entropy/Gini of current X, declare A_best, create sets/gain accordingly\n",
    "    score = criteria(df)\n",
    "    Attribute_best = None\n",
    "    Set_best = None\n",
    "    Gain_best = 0.0\n",
    "    \n",
    "\n",
    "    num_col = len(df[0]) - 1  # last column of x is labels\n",
    "    for col in range(0, num_col):\n",
    "        col_val = [row[col] for row in df]\n",
    "\n",
    "        for value in col_val:\n",
    "            #Split dataset\n",
    "            (set1, set2) = partition(X, col, value)\n",
    "\n",
    "            # Calculate Gain\n",
    "            p = float(len(set1)) / len(df)\n",
    "            gain = score - p*criteria(set1) - (1-p)*criteria(set2)\n",
    "            if gain>Gain_best and len(set1)>0 and len(set2)>0:\n",
    "                Gain_best = gain\n",
    "                Attribute_best = (col, value)\n",
    "                Set_best = (set1, set2)\n",
    "\n",
    "    if Gain_best > 0:\n",
    "        #Recursive Call on partitioned Sets\n",
    "        right_branch = construct_tree(Set_best[0])\n",
    "        left_branch = construct_tree(Set_best[1])\n",
    "        return Tree(col=Attribute_best[0], value=Attribute_best[1], right_branch=right_branch, left_branch=left_branch)\n",
    "    \n",
    "    else:\n",
    "        return Tree(results=uniqueCounts(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "Pruning\n",
    "Apply Decision Tree\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The base cases are the following:\n",
    "\n",
    "•  All the examples from the training set belong to the same class ( a tree leaf labeled with that class is returned ).\n",
    "\n",
    "•  The training set is empty ( returns a tree leaf called failure ).\n",
    "\n",
    "•  The attribute list is empty ( returns a leaf labeled with the most frequent class or the disjuction of all the classes).\n",
    "https://octaviansima.wordpress.com/2011/03/25/decision-trees-c4-5/\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
