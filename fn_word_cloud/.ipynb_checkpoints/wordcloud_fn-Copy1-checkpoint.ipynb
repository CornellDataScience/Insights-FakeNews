{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import preprocessing as pp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = pd.read_csv('train_bodies.csv')\n",
    "stances = pd.read_csv('train_stances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Posting photos of a gun-toting child online, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody\n",
       "0        0  A small meteorite crashed into a wooded area i...\n",
       "1        4  Last week we hinted at what was to come as Ebo...\n",
       "2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
       "3        6  Posting photos of a gun-toting child online, I...\n",
       "4        7  At least 25 suspected Boko Haram insurgents we..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bodies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody\n",
       "2        5  (NEWSER) – Wonder how long a Quarter Pounder w..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bodies.loc[bodies['Body ID'] == 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merges the two data frame based on body ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(bodies, stances, on='Body ID', how='outer').to_csv('bodies_stances.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_stances = pd.read_csv('bodies_stances.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removes unrelated from the data frame and drops duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_stances = bodies_stances[bodies_stances.Stance != 'unrelated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_stances.to_csv('bodies_stances_new.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bodies_stances = bodies_stances.drop_duplicates(subset=['articleBody'], keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverts article body strings to lowercase and removes special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_bodies_clean = bodies_stances.articleBody.str.lower().str.replace(r\"[^a-zA-z]+\", \" \").str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_bodies_clean = bodies_stances.articleBody.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting by separating sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_stances.articleBody = article_bodies_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbody = bodies_stances['articleBody'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agree_bodies_stances = bodies_stances[bodies_stances.Stance == 'agree']\n",
    "disagree_bodies_stances = bodies_stances[bodies_stances.Stance == 'disagree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_textbody = disagree_bodies_stances['articleBody'].str.cat(sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizes to a sentence\n",
    "disagree_sentences = sent_tokenize(disagree_textbody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizes each word in each sentence\n",
    "disagree_sentences = [nltk.tokenize.word_tokenize(disagree_sentences[i]) for i in range(len(disagree_sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags each word \n",
    "disagree_pos = [nltk.pos_tag(disagree_sentences[i]) for i in range(len(disagree_sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercases the words \n",
    "disagree_pos = [[(re.sub('[\\W_]+', ' ', word, flags=re.UNICODE).lower(), tag) for word, tag in element]\n",
    "                    for element in disagree_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "disagree_pos = [disagree_pos[i][j] for i in range(len(disagree_pos)) for j in range(len(disagree_pos[i])) if disagree_pos[i][j][0] not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes words that are less than 3 letters \n",
    "disagree_pos = [disagree_pos[i] for i in range(len(disagree_pos)) if len(disagree_pos[i][0]) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rumour'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disagree_pos[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172548"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a list of all the words \n",
    "disagree_words = [disagree_pos[i][0] for i in range(len(disagree_pos))]\n",
    "len(disagree_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_word_dist = nltk.FreqDist(disagree_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreepos_df = pd.DataFrame(disagree_pos, columns =['Word', 'POS_Tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_word_dist_df = pd.DataFrame(list(disagree_word_dist.items()), columns = ['Word','Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_dist_freq = pd.merge(disagreepos_df, disagree_word_dist_df, on='Word', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS_Tag</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rumour</td>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rumour</td>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rumour</td>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rumour</td>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rumour</td>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rumour</td>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rumour</td>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rumour</td>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rumour</td>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rumour</td>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>JJ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>JJ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>JJ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>JJ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>JJ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>JJ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>JJ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>JJ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>JJ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>JJ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pretty</td>\n",
       "      <td>JJ</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pretty</td>\n",
       "      <td>RB</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pretty</td>\n",
       "      <td>RB</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pretty</td>\n",
       "      <td>RB</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pretty</td>\n",
       "      <td>JJ</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>pretty</td>\n",
       "      <td>RB</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pretty</td>\n",
       "      <td>JJ</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pretty</td>\n",
       "      <td>RB</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pretty</td>\n",
       "      <td>RB</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>pretty</td>\n",
       "      <td>JJ</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172518</th>\n",
       "      <td>rigs</td>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172519</th>\n",
       "      <td>profiling</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172520</th>\n",
       "      <td>compare</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172521</th>\n",
       "      <td>databases</td>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172522</th>\n",
       "      <td>surveil</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172523</th>\n",
       "      <td>taser</td>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172524</th>\n",
       "      <td>neutralized</td>\n",
       "      <td>VBN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172525</th>\n",
       "      <td>alphanumeric</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172526</th>\n",
       "      <td>signatures</td>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172527</th>\n",
       "      <td>infrared</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172528</th>\n",
       "      <td>lidar</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172529</th>\n",
       "      <td>lidar</td>\n",
       "      <td>NNP</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172530</th>\n",
       "      <td>measurements</td>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172531</th>\n",
       "      <td>helps</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172532</th>\n",
       "      <td>maintain</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172533</th>\n",
       "      <td>remote sensing</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172534</th>\n",
       "      <td>measure</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172535</th>\n",
       "      <td>analyzing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172536</th>\n",
       "      <td>analyzing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172537</th>\n",
       "      <td>reflected</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172538</th>\n",
       "      <td>refuel</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172539</th>\n",
       "      <td>guidance</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172540</th>\n",
       "      <td>predictive</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172541</th>\n",
       "      <td>accounting</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172542</th>\n",
       "      <td>abusing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172543</th>\n",
       "      <td>judiciously</td>\n",
       "      <td>RB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172544</th>\n",
       "      <td>requiring</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172545</th>\n",
       "      <td>higher level</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172546</th>\n",
       "      <td>hands on</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172547</th>\n",
       "      <td>tactical</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172548 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Word POS_Tag  Frequency\n",
       "0               rumour      NN         10\n",
       "1               rumour      NN         10\n",
       "2               rumour      NN         10\n",
       "3               rumour      NN         10\n",
       "4               rumour      NN         10\n",
       "5               rumour      NN         10\n",
       "6               rumour      NN         10\n",
       "7               rumour      NN         10\n",
       "8               rumour      NN         10\n",
       "9               rumour      NN         10\n",
       "10          ridiculous      JJ         10\n",
       "11          ridiculous      JJ         10\n",
       "12          ridiculous      JJ         10\n",
       "13          ridiculous      JJ         10\n",
       "14          ridiculous      JJ         10\n",
       "15          ridiculous      JJ         10\n",
       "16          ridiculous      JJ         10\n",
       "17          ridiculous      JJ         10\n",
       "18          ridiculous      JJ         10\n",
       "19          ridiculous      JJ         10\n",
       "20              pretty      JJ         50\n",
       "21              pretty      RB         50\n",
       "22              pretty      RB         50\n",
       "23              pretty      RB         50\n",
       "24              pretty      JJ         50\n",
       "25              pretty      RB         50\n",
       "26              pretty      JJ         50\n",
       "27              pretty      RB         50\n",
       "28              pretty      RB         50\n",
       "29              pretty      JJ         50\n",
       "...                ...     ...        ...\n",
       "172518            rigs     NNS          1\n",
       "172519       profiling      NN          1\n",
       "172520         compare      VB          1\n",
       "172521       databases     NNS          1\n",
       "172522         surveil      VB          1\n",
       "172523           taser     NNP          1\n",
       "172524     neutralized     VBN          1\n",
       "172525    alphanumeric      JJ          1\n",
       "172526      signatures     NNS          1\n",
       "172527        infrared      JJ          1\n",
       "172528           lidar     NNP          2\n",
       "172529           lidar     NNP          2\n",
       "172530    measurements     NNS          1\n",
       "172531           helps     VBZ          1\n",
       "172532        maintain      VB          1\n",
       "172533  remote sensing      JJ          1\n",
       "172534         measure      VB          1\n",
       "172535       analyzing     VBG          2\n",
       "172536       analyzing     VBG          2\n",
       "172537       reflected      JJ          1\n",
       "172538          refuel      VB          1\n",
       "172539        guidance      NN          1\n",
       "172540      predictive      JJ          1\n",
       "172541      accounting     VBG          1\n",
       "172542         abusing     VBG          1\n",
       "172543     judiciously      RB          1\n",
       "172544       requiring     VBG          1\n",
       "172545    higher level      NN          1\n",
       "172546        hands on      JJ          1\n",
       "172547        tactical      JJ          1\n",
       "\n",
       "[172548 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disagree_dist_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_dist_freq = disagree_dist_freq.drop_duplicates(subset=['Word'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_dist_freq.to_csv('disagree_dist.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(textbody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "pos = [] \n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    tokenized.append(nltk.tokenize.word_tokenize(sentences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(tokenized)):\n",
    "    pos.append(nltk.pos_tag(tokenized[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in l for item in sublist]\n",
    "for sublist in l:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_pos = [part for word in pos for part in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame(flat_pos, columns =['Word', 'POS_Tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = nltk.tokenize.word_tokenize(pp.clean(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentence1 = nltk.pos_tag(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "for word in sentence1: \n",
    "    clean.append(pp.clean(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removes Stop Words from Article Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_bodies_clean = article_bodies_clean.str.split(' ').apply(lambda x: ' '.join(k for k in x if k not in stop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_stances.articleBody = article_bodies_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splits up article bodies based on stance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agree_bodies_stances = bodies_stances[bodies_stances.Stance == 'agree']\n",
    "disagree_bodies_stances = bodies_stances[bodies_stances.Stance == 'disagree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = disagree_bodies_stances['articleBody'].str.cat(sep=' ')\n",
    "b = agree_bodies_stances['articleBody'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_words = pp.get_clean_tokens(a)\n",
    "# disagree_word_dist = nltk.FreqDist(disagree_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_words = nltk.tokenize.word_tokenize(a)\n",
    "disagree_word_dist = nltk.FreqDist(disagree_words)\n",
    "\n",
    "agree_words = nltk.tokenize.word_tokenize(b)\n",
    "agree_word_dist = nltk.FreqDist(agree_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags part of speech to each word in articles that disagree \n",
    "pos_disagree = nltk.pos_tag(disagree_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_disagreedf = pd.DataFrame(pos_disagree, columns =['Word', 'POS_Tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_disagreedf.to_csv('pos_disagree.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_disagree[28641]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.is_noun(pos_disagree[28641][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of non nouns \n",
    "non_nouns = []\n",
    "for i in range(len(pos_disagree)):\n",
    "    if pp.is_noun(pos_disagree[i][1]) == False:\n",
    "        non_nouns.append(pos_disagree[i][0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_res = pd.DataFrame(disagree_word_dist.most_common(7000),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "disagree_res.to_csv('disagree_wordcounts.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "agree_res = pd.DataFrame(agree_word_dist.most_common(7000),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "agree_res.to_csv('agree_wordcounts.csv', sep=',', encoding='utf-8')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_res = pd.DataFrame(disagree_word_dist.most_common(7000),\n",
    "                    columns=['Word', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds a column with boolean values \n",
    "disagree_res['not_noun'] = disagree_res.Word.isin(non_nouns)\n",
    "agree_bodies_stances = bodies_stances[bodies_stances.Stance == 'agree']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnoun_disagree_res = disagree_res[disagree_res.not_noun == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnoun_disagree_res.to_csv('nonnoun_disagree_wordcounts.csv', sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
