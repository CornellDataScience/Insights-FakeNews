{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stance Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannyyang/Library/Python/3.6/lib/python/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import preprocessing, feature_engineering, helpers\n",
    "import importlib\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import itertools\n",
    "import utils\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(preprocessing)\n",
    "importlib.reload(feature_engineering)\n",
    "importlib.reload(helpers)\n",
    "importlib.reload(utils)\n",
    "preprocess = preprocessing.Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13427, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>158</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>1923</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'Nasa Confirms Earth Will Experience 6 Days of...</td>\n",
       "      <td>154</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Banksy 'Arrested &amp; Real Identity Revealed' Is ...</td>\n",
       "      <td>1739</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gateway Pundit</td>\n",
       "      <td>2327</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline  Body ID    Stance\n",
       "1   Hundreds of Palestinians flee floods in Gaza a...      158     agree\n",
       "4   Spider burrowed through tourist's stomach and ...     1923  disagree\n",
       "5   'Nasa Confirms Earth Will Experience 6 Days of...      154     agree\n",
       "8   Banksy 'Arrested & Real Identity Revealed' Is ...     1739     agree\n",
       "10                                     Gateway Pundit     2327   discuss"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stances = pd.read_csv(\"fn_data/train_stances.csv\")\n",
    "train_stances = train_stances.loc[lambda x: x.Stance != \"unrelated\"]\n",
    "print(train_stances.shape)\n",
    "train_stances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1683, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Posting photos of a gun-toting child online, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody\n",
       "0        0  A small meteorite crashed into a wooded area i...\n",
       "1        4  Last week we hinted at what was to come as Ebo...\n",
       "2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
       "3        6  Posting photos of a gun-toting child online, I...\n",
       "4        7  At least 25 suspected Boko Haram insurgents we..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bodies = pd.read_csv(\"fn_data/train_bodies.csv\")\n",
    "print(train_bodies.shape)\n",
    "train_bodies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10726, 3), (2701, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stances_tr, stances_val = preprocess.train_test_split(train_bodies, train_stances)\n",
    "stances_tr.shape, stances_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'discuss': 1715, 'agree': 798, 'disagree': 188})\n",
      "0.6349500185116623\n"
     ]
    }
   ],
   "source": [
    "ct = Counter(stances_val['Stance'])\n",
    "print(ct)\n",
    "print(ct.most_common(1)[0][1]/len(list(stances_val[\"Stance\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_word_stance(word, glove_dict):\n",
    "    #50d word vector\n",
    "    if word in glove_dict:\n",
    "        wv = glove_dict[word]\n",
    "    else:\n",
    "        wv = np.zeros((50, ))\n",
    "    #4d sentiment\n",
    "    sent = preprocess.get_sentiment(word)\n",
    "    #36d one-hot encoding of part of speech\n",
    "    pos = nltk.pos_tag(word)[1]\n",
    "    pos_encoding = [(1 if tag == pos else 0) for tag in preprocess.pos_tags]\n",
    "    #boolean flag for negating word\n",
    "    stemmed_word = preprocess.stem_word(word)\n",
    "    is_neg = (1 if stemmed_word in preprocess.negating_words_stemmed else 0)\n",
    "    is_refuting = (1 if stemmed_word in preprocess.refuting_words_stemmed else 0)\n",
    "    embedding = np.concatenate([wv, [sent[\"pos\"], sent[\"neg\"], sent[\"neu\"], sent[\"compound\"], is_neg, is_refuting], pos_encoding])\n",
    "    return embedding\n",
    "\n",
    "def process_text_stance(text, glove_dict, n_words = 25):\n",
    "    tokens = preprocess.get_clean_tokens(text, False)\n",
    "    if len(tokens)>=n_words:\n",
    "        tokens = tokens[:n_words]\n",
    "        encoding = np.array([process_word_stance(token, glove_dict) for token in tokens])\n",
    "    elif len(tokens)<n_words:\n",
    "        padding = [np.zeros((92,))]*(n_words-len(tokens))\n",
    "        encoding = np.array([process_word_stance(token, glove_dict) for token in tokens]+padding)\n",
    "    return encoding\n",
    "\n",
    "def process_bodies_stance(df, glove_dict):\n",
    "    body_info = {}\n",
    "    ids = list(df[\"Body ID\"])\n",
    "    for i in range(len(ids)):\n",
    "        if i % 100 == 0 and i != 0:\n",
    "            print(\"processed \"+str(i))\n",
    "        body_info[ids[i]] = process_text_stance(preprocess.get_body(ids[i],df), glove_dict, 50)\n",
    "    print(\"done! processed \" + str(len(ids)))\n",
    "    return body_info\n",
    "\n",
    "def process_feats_stance(data, body_dict, glove_dict):\n",
    "    headline, body_id = data[0], int(data[1])\n",
    "    padding = [np.zeros((92,))] #1 word of padding\n",
    "    return np.concatenate([process_text_stance(headline, glove_dict), np.array(padding), body_dict[body_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100\n",
      "processed 200\n",
      "processed 300\n",
      "processed 400\n",
      "processed 500\n",
      "processed 600\n",
      "processed 700\n",
      "processed 800\n",
      "processed 900\n",
      "processed 1000\n",
      "processed 1100\n",
      "processed 1200\n",
      "processed 1300\n",
      "processed 1400\n",
      "processed 1500\n",
      "processed 1600\n",
      "done! processed 1683\n"
     ]
    }
   ],
   "source": [
    "body_dict = process_bodies_stance(train_bodies, glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_feats = [process_feats_stance(i, body_dict, glove_dict) for i in stances_tr.values]\n",
    "val_feats = [process_feats_stance(i, body_dict, glove_dict) for i in stances_val.values]\n",
    "end = time.time()\n",
    "print(int(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To do: get the new get batch function\n",
    "def get_batch(data, targets, i,batch_size):\n",
    "    batches = data[i*batch_size:i*batch_size+batch_size]\n",
    "    results = targets[i*batch_size:i*batch_size+batch_size]\n",
    "    results = [(2 if result == \"agree\" else (1 if result == \"discuss\" else 0)) for result in results]\n",
    "    return np.array(batches),np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "def eval_model(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_x_test,batch_y_test = get_batch(val_feats,[str(x[-1]) for x in stances_val.values],0,len(stances_val))\n",
    "    model.eval()\n",
    "    predicted = None\n",
    "    with torch.no_grad():\n",
    "        inputs = Variable(torch.FloatTensor(batch_x_test))\n",
    "        labels = torch.LongTensor(batch_y_test)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy: %d %%' % (100 * correct / total))\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_model(predictions):    \n",
    "    #use FNC scorer to generate score report\n",
    "    label_prediction = [(\"agree\" if x == 2 else (\"discuss\" if x == 1 else \"disagree\")) for x in predictions]\n",
    "    label_actual = pd.DataFrame(stances_val)['Stance']\n",
    "    matrix = confusion_matrix(label_actual,label_prediction)\n",
    "    print('confusion matrix: \\n{}\\n'.format(matrix))\n",
    "    score.report_score(label_actual, label_prediction)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "\n",
    "\"\"\"\n",
    "    correct = (preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Siamese(nn.Module):\n",
    "    # hidden dim: dimension of one RNN\n",
    "    # 2 * hidden dim: concatenated outputs\n",
    "    def __init__(self,embedding_dim, hidden_dim_head, hidden_dim_bod, dropout, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # RNN dimensions [embedding_dim, hidden_dim]\n",
    "        # To-Do: try varying hidden, output dims for each network\n",
    "        self.rnn1 = nn.RNN(20, hidden_dim_head, dropout=dropout)\n",
    "        self.rnn2 = nn.RNN(72, hidden_dim_bod, dropout=dropout)\n",
    "        # Note: output dim could technically be higher now? cause runnign through a loss function/MLP\n",
    "        #self.fc1 = nn.Linear(hidden_dim, output_dim)\n",
    "        #self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim_head+hidden_dim_bod, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x originally = [batch size, nubmer of words, embedding dim]\n",
    "        # swap first two dimensions to match rnn specs\n",
    "        #print(f'x shape before: ${x.shape}')\n",
    "        x = x.permute([1,0,2])\n",
    "        \n",
    "        #print(f'x shape: ${x.shape}')\n",
    "        header = x[:, :, :20]\n",
    "        article = x[:, :, 20:]\n",
    "        \n",
    "        #print(f'header shape: ${header.shape}')\n",
    "        #print(f'article shape: ${article.shape}')\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        output1, hidden1 = self.rnn1(header)\n",
    "        output2, hidden2 = self.rnn2(article)\n",
    "        #print(f'Hidden1 shape: {hidden1.squeeze(0).shape}')\n",
    "        #print(f'Hidden2 shape: {hidden2.squeeze(0).shape}')\n",
    "     \n",
    "        hidden = self.dropout(torch.cat((hidden1.squeeze(0), hidden2.squeeze(0)),1))\n",
    "        #print(f'Hidden shape: {hidden.shape}')\n",
    "\n",
    "        #out = self.fc1(hidden1.squeeze(0))\n",
    "        #out = out.append(self.fc2(hidden2.squeeze(0)))\n",
    "        \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To do: re-implement with LSTMs for each branch\n",
    "class Siamese_LSTM(nn.Module):\n",
    "    # hidden dim: dimension of one RNN\n",
    "    # 2 * hidden dim: concatenated outputs\n",
    "    def __init__(self, hidden_dim_head, hidden_dim_bod, \n",
    "                 n_layers_head, n_layers_bod, dropout, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # RNN dimensions [embedding_dim, hidden_dim]\n",
    "        # To-Do: try varying hidden, output dims for each network\n",
    "        \n",
    "        self.rnn1 = nn.LSTM(92, hidden_dim_head, num_layers=n_layers_head, bidirectional=True, dropout=dropout)\n",
    "        self.rnn2 = nn.LSTM(92, hidden_dim_bod, num_layers=n_layers_bod, bidirectional=True, dropout=dropout)\n",
    "        # Note: output dim could technically be higher now? cause runnign through a loss function/MLP\n",
    "       \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear((hidden_dim_head+hidden_dim_bod)*2, (hidden_dim_head+hidden_dim_bod)) #doubled for bidirectional\n",
    "        self.fc2 = nn.Linear((hidden_dim_head+hidden_dim_bod), output_dim)\n",
    "        \n",
    "    def set_n_layers(self, head, bod):\n",
    "        self.rnn1.num_layers = head\n",
    "        self.rnn2.num_layers = bod\n",
    "        \n",
    "    def set_n_hidden(self, head, bod):\n",
    "        self.rnn1.hidden_size = head\n",
    "        self.rnn2.hidden_size = bod\n",
    "        \n",
    "    def set_dropout(self, droupout):\n",
    "        self.droupout = droupout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x originally = [batch size, nubmer of words, embedding dim]\n",
    "        # swap first two dimensions to match rnn specs\n",
    "        #print(f'x shape before: ${x.shape}')\n",
    "        x = x.permute([1,0,2])\n",
    "        \n",
    "        #print(f'x shape: ${x.shape}')\n",
    "        header = x[:20, :, :]\n",
    "        article = x[20:, :, :]\n",
    "        \n",
    "        #print(f'header shape: ${header.shape}')\n",
    "        #print(f'article shape: ${article.shape}')\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        output1, (hidden1, cell1) = self.rnn1(header)\n",
    "        output2, (hidden2, cell2) = self.rnn2(article)\n",
    "        \n",
    "        \"\"\"print(f'Hidden1 shape: {hidden1.squeeze(0).shape}')\n",
    "        print(f'Hidden2 shape: {hidden2.squeeze(0).shape}')\"\"\"\n",
    "        \n",
    "        # Concats the last forward and backward hidden layers\n",
    "        # Wouldn't you only technically want to do this if it is in fact bidirectional? \n",
    "        hidden = torch.cat((hidden1[-2,:,:], hidden1[-1,:,:], hidden2[-2,:,:], hidden2[-1,:,:]), dim=1)\n",
    "\n",
    "        \"\"\"print(f'Hidden1 shape front/back: {hidden1.shape}')\n",
    "        print(f'Hidden2 shape front/back: {hidden2.shape}')\"\"\"\n",
    "        # TODO: Check if concatenating along dim 1 might be better if first squeeze each hidden\n",
    "        \"\"\"print(f'Concatenated shape: {hidden_merge.shape}')\"\"\"\n",
    "     \n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        # Todo: add more layers beforehand, that's way too many to concatenate down to 3\n",
    "        fc1 = self.fc1(hidden)\n",
    "        \"\"\"print(f'FC shape: {fc1.shape}')\"\"\"\n",
    "        fc1 = F.relu(fc1)\n",
    "        fc2 = self.fc2(fc1)\n",
    "        return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_DIM_HEAD = 128\n",
    "HIDDEN_DIM_BOD = 128\n",
    "\n",
    "OUTPUT_DIM = 3\n",
    "num_epochs = 5\n",
    "batch_size = 500\n",
    "display_step = 1\n",
    "\n",
    "N_LAYERS = 2\n",
    "#BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = Siamese_LSTM(hidden_dim_head=HIDDEN_DIM_HEAD, hidden_dim_bod=HIDDEN_DIM_BOD, \n",
    "                dropout=DROPOUT, output_dim=OUTPUT_DIM, n_layers_head=1, n_layers_bod=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# model1 = Siamese_LSTM(hidden_dim_head=HIDDEN_DIM_HEAD, hidden_dim_bod=HIDDEN_DIM_BOD, \n",
    "#                 dropout=DROPOUT, output_dim=OUTPUT_DIM, n_layers_head=2, n_layers_bod=3)\n",
    "# optimizer1 = torch.optim.Adam(model1.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# # Plateaus at 69.7 validation, some decrease in loss but not enough\n",
    "# model2 = Siamese_LSTM(hidden_dim_head=250, hidden_dim_bod=250, \n",
    "#                 dropout=DROPOUT, output_dim=OUTPUT_DIM, n_layers_head=2, n_layers_bod=2)\n",
    "# optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# model3 = Siamese_LSTM(hidden_dim_head=100, hidden_dim_bod=100, \n",
    "#                 dropout=DROPOUT, output_dim=OUTPUT_DIM, n_layers_head=2, n_layers_bod=2)\n",
    "# optimizer3 = torch.optim.Adam(model3.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# model4 = Siamese_LSTM(hidden_dim_head=HIDDEN_DIM_HEAD, hidden_dim_bod=HIDDEN_DIM_BOD, \n",
    "#                 dropout=0.3, output_dim=OUTPUT_DIM, n_layers_head=2, n_layers_bod=2)\n",
    "# optimizer4 = torch.optim.Adam(model4.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = [(model, optimizer)] #,(model2,optimizer2),(model3,optimizer3),(model4,optimizer4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, total_batch, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for i in range(total_batch):\n",
    "        batch_x,batch_y = get_batch(train_feats, [str(x[-1]) for x in stances_tr.values],i,batch_size)\n",
    "        inputs = Variable(torch.FloatTensor(batch_x))\n",
    "        labels = Variable(torch.LongTensor(batch_y))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs)\n",
    "        loss = criterion(predictions, labels)\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "        acc = binary_accuracy(predicted, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / total_batch, epoch_acc / total_batch\n",
    "\n",
    "def evaluate(model, total_batch, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(total_batch):\n",
    "            batch_x,batch_y = get_batch(val_feats, [str(x[-1]) for x in stances_val.values],i,batch_size)\n",
    "            inputs = Variable(torch.FloatTensor(batch_x))\n",
    "            labels = Variable(torch.LongTensor(batch_y))\n",
    "            predictions = model(inputs)\n",
    "            loss = criterion(predictions, labels)\n",
    "            _, predicted = torch.max(predictions.data, 1)\n",
    "            acc = binary_accuracy(predicted, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / total_batch, epoch_acc / total_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0\n",
      "| Epoch: 01 | Train Loss: 1.072 | Train Acc: 67.04% | Val. Loss: 1.074 | Val. Acc: 63.04% |\n",
      "| Epoch: 02 | Train Loss: 1.062 | Train Acc: 67.13% | Val. Loss: 1.065 | Val. Acc: 63.04% |\n",
      "| Epoch: 03 | Train Loss: 1.052 | Train Acc: 67.13% | Val. Loss: 1.056 | Val. Acc: 63.04% |\n",
      "| Epoch: 04 | Train Loss: 1.042 | Train Acc: 67.13% | Val. Loss: 1.046 | Val. Acc: 63.04% |\n",
      "| Epoch: 05 | Train Loss: 1.030 | Train Acc: 67.13% | Val. Loss: 1.036 | Val. Acc: 63.04% |\n",
      "173\n",
      "Baseline Train = 0.67%, Test = 0.63%\n"
     ]
    }
   ],
   "source": [
    "batches_train= int(len(train_feats)/batch_size)\n",
    "batches_val = int(len(val_feats)/batch_size)\n",
    "    \n",
    "\n",
    "for i in range(len(queue)):\n",
    "    start = time.time()\n",
    "    model = queue[i][0]\n",
    "    optimizer = queue[i][1]\n",
    "    print(f'model {i}')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss, train_acc = train(model, batches_train, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, batches_val, criterion)\n",
    "\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "    print(int(time.time()-start))\n",
    "    baseline_tr = Counter(stances_tr['Stance']).most_common(1)[0][1]/len(list(stances_tr[\"Stance\"]))\n",
    "    baseline_val = Counter(stances_val['Stance']).most_common(1)[0][1]/len(list(stances_val[\"Stance\"]))\n",
    "    print(f'Baseline Train = {baseline_tr:.2f}%, Test = {baseline_val:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "predicted = eval_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2701})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_l = [i.item() for i in list(predicted)]\n",
    "Counter(predicted_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      "[[   0    0  798]\n",
      " [   0    0  188]\n",
      " [   0    0 1715]]\n",
      "\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |     0     |     0     |    798    |     0     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     0     |     0     |    188    |     0     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |     0     |     0     |   1715    |     0     |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     0     |     0     |     0     |     0     |\n",
      "-------------------------------------------------------------\n",
      "Score: 1961.5 out of 2701.0\t(72.62125138837467%)\n",
      "Normalized confusion matrix\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAEYCAYAAAApuP8NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n80DonMt5/w/yN6hfOkxRxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1034ffc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix = score_model(predicted_l)\n",
    "utils.plot_confusion_matrix(matrix, classes=[\"Agree\",\"Disagree\", \"Discuss\"],\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 188, 0], [0, 1715, 0], [0, 798, 0]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label = [(2 if x[-1] == \"agree\" else (1 if x[-1] == \"discuss\" else 0)) for x in stances_val.values]\n",
    "[list(x) for x in list(confusion_matrix(true_label,predicted_l))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train finalized model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final = CNN(EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fts = train_feats+val_feats\n",
    "tr_labels = [str(x[-1]) for x in stances_tr.values]+[str(x[-1]) for x in stances_val.values]\n",
    "\n",
    "def train_final(model, total_batch, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model_final.train()\n",
    "    for i in range(total_batch):\n",
    "        batch_x,batch_y = get_batch(fts, tr_labels,i,batch_size)\n",
    "        inputs = Variable(torch.FloatTensor(batch_x))\n",
    "        labels = Variable(torch.LongTensor(batch_y))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model_final(inputs)\n",
    "        loss = criterion(predictions, labels)\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "        acc = binary_accuracy(predicted, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / total_batch, epoch_acc / total_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.727 | Train Acc: 69.69% |\n",
      "| Epoch: 02 | Train Loss: 0.555 | Train Acc: 77.40% |\n",
      "| Epoch: 03 | Train Loss: 0.465 | Train Acc: 80.90% |\n"
     ]
    }
   ],
   "source": [
    "batches_train= int(len(fts)/batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_final(model_f, batches_train, optimizer, criterion)\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_final.state_dict(), './Siamese.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
