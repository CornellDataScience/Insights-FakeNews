{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mixed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle the model files and load here to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannyyang/Library/Python/3.6/lib/python/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import score as sc\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, pairwise, f1_score, precision_score\n",
    "from scipy.spatial import distance\n",
    "from preprocessing.utils import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import importlib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import en_coref_md\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "coref = en_coref_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[21]:\n",
    "\n",
    "\n",
    "negating_words = set([\n",
    "    \"n't\", \"not\", \"no\", \n",
    "    \"never\", \"nobody\", \"non\", \"nope\"])\n",
    "doubting_words = set([\n",
    "    'fake','fraud', 'hoax', \n",
    "    'false', 'deny', 'denies', \n",
    "    'despite', 'doubt', \n",
    "    'bogus', 'debunk', 'prank', \n",
    "    'retract', 'scam', \"withdrawn\",\n",
    "    \"misinformation\"])\n",
    "hedging_words = set([\n",
    "    'allege', 'allegedly','apparently',\n",
    "    'appear','claim','could',\n",
    "    'evidently','largely','likely',\n",
    "    'mainly','may', 'maybe', 'might',\n",
    "    'mostly','perhaps','presumably',\n",
    "    'probably','purport', 'purportedly',\n",
    "    'reported', 'reportedly',\n",
    "    'rumor', 'rumour', 'rumored', 'rumoured',\n",
    "    'says','seem','somewhat',\n",
    "    'unconfirmed'])\n",
    "sus_words = doubting_words.union(hedging_words)\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(sentence):\n",
    "    sent =  vader.polarity_scores(sentence)\n",
    "    return [sent[\"pos\"],sent[\"neg\"],sent[\"neu\"],sent[\"compound\"]]\n",
    "\n",
    "def get_avg_sentiment(lst):\n",
    "    sents = np.array([get_sentiment(s) for s in lst])\n",
    "    return list(np.mean(sents, axis = 0))\n",
    "\n",
    "def get_diff_sentiment(a,b):\n",
    "    return list(np.array(a) - np.array(b))\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "def train_test_split(bodies, stances, split=0.8):\n",
    "    idx = np.random.permutation(np.arange(len(bodies)))\n",
    "    bodies = bodies.values[idx]\n",
    "    train = int(len(bodies)*0.8)\n",
    "    bodies_tr = set([i[0] for i in bodies[:train]])\n",
    "    bodies_val = set([i[0] for i in bodies[train:]])\n",
    "    stances_tr = stances.loc[stances[\"Body ID\"].isin(bodies_tr), :]\n",
    "    stances_val = stances.loc[stances[\"Body ID\"].isin(bodies_val), :]\n",
    "    return stances_tr, stances_val\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "def get_body(n):\n",
    "    return test_bodies.loc[lambda x: x[\"Body ID\"] == n, \"articleBody\"].item()\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace(\"' \",' ')\n",
    "    text = text.replace(\"'\\n\",'\\n')\n",
    "    text = text.replace(\" '\",' ')\n",
    "    text = text.replace('\"',' ')\n",
    "    text = text.replace('“',' ')\n",
    "    text = text.replace('”', ' ')\n",
    "    text = text.replace(\":\", \". \")\n",
    "    text = text.replace(\";\", \". \")\n",
    "    text = text.replace(\"...\", \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "    if all([a == 0 for a in x]) or all([a == 0 for a in y]):\n",
    "        return 0\n",
    "    return 1 - np.nan_to_num(distance.cosine(x,y))\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "def get_topics(doc):\n",
    "    \"\"\"\n",
    "    get topics of a sentence\n",
    "    input: spacy doc\n",
    "    output: dictionary with nouns as the key, and the set of noun chunks that contain the noun as the value\n",
    "    special entry _vocab has the set of all tokens in the dict\n",
    "    \"\"\"\n",
    "    subjs = {}\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk.root.text) > 2 and chunk.root.pos_ not in [\"NUM\", \"SYM\",\"PUNCT\"]:\n",
    "            txt = chunk.root.lemma_.lower()\n",
    "            if txt not in subjs:\n",
    "                subjs[txt] = set([txt])\n",
    "            subjs[txt].add(chunk.text.lower())\n",
    "    subjects_= []\n",
    "    for word in subjs:\n",
    "        for phrase in subjs[word]:\n",
    "            subjects_ += phrase.split(\" \")\n",
    "    subjs[\"_vocab\"] = set(subjects_)\n",
    "    return subjs\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "def get_svos(sent):\n",
    "    \"\"\"\n",
    "    input: Spacy processed sentence\n",
    "    output: dict of subj, dict of v, dict of obj (each word is lemmatized and lowercased)\n",
    "    each entry in dict has key of lemmatized token, value is actual token (to do traversals with later if needed)\n",
    "    \"\"\"\n",
    "    s = {}\n",
    "    v = {}\n",
    "    o = {}\n",
    "    for token in sent:\n",
    "        if token.dep_ == 'ROOT':\n",
    "            v[token.lemma_.lower()] = token\n",
    "        elif token.dep_ in [\"nsubj\", \"nsubjpass\", \"csubj\",\"csubjpass\", \"agent\",\"compound\"]:\n",
    "            s[token.lemma_.lower()] = token\n",
    "        elif token.dep_ in [\"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\"]:\n",
    "            o[token.lemma_.lower()] = token\n",
    "    # https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md\n",
    "    return (s,v,o)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "def build_graph(doc):\n",
    "    \"\"\"\n",
    "    build a NetworkX graph of the dependency tree\n",
    "    input: spacy Doc\n",
    "    output: networkx graph\n",
    "    \"\"\"\n",
    "    edges = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE']:\n",
    "            for child in token.children:\n",
    "                if child.pos_ not in ['SPACE']:\n",
    "                    edges.add((token.lemma_.lower(),child.lemma_.lower()))\n",
    "    graph = nx.DiGraph(list(edges))\n",
    "    return graph\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "def get_edges(doc):\n",
    "    \"\"\"\n",
    "    return list of edges\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT', 'SYM']:\n",
    "            for child in token.children:\n",
    "                if child.pos_ not in ['SPACE', 'PUNCT', 'SYM']:\n",
    "                    edges.append((\n",
    "                        {\"token\":token.lemma_.lower(), \"dep\":token.dep_ , \"pos\":token.pos_},\n",
    "                        {\"token\":child.lemma_.lower(), \"dep\":child.dep_ , \"pos\":child.pos_}\n",
    "                    ))\n",
    "    return edges\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "def get_summary(doc, subjects, n = 5):\n",
    "    \"\"\"\n",
    "    get summary of n sentences in document\n",
    "    first meaningful sentence will always be returned\n",
    "    \"\"\"\n",
    "    subjects_ = subjects\n",
    "    def score_sentence(sent):\n",
    "        # not very robust right now\n",
    "        score = 0\n",
    "        word_count = 0\n",
    "        for token in sent:\n",
    "            word_count += 1\n",
    "            t = token.lemma_.lower()\n",
    "            if t in subjects_:\n",
    "                score += 1\n",
    "            elif t in negating_words or t in doubting_words or t in hedging_words:\n",
    "                score += 1\n",
    "        return score/word_count if word_count > 4 else 0\n",
    "    sentences = [s for s in doc.sents]\n",
    "    scored_sentences = [[idx, sent, score_sentence(sent)] for idx, sent in enumerate(sentences)]\n",
    "    scored_sentences = [s for s in scored_sentences if s[2] > 0 and s[0] > 0] #filter out non-scoring sentences\n",
    "    scored_sentences.sort(key = lambda x: x[2], reverse = True)\n",
    "    top = scored_sentences[:n]\n",
    "    top.sort(key = lambda x: x[0])\n",
    "    scored_sentences.sort(key = lambda x: x[0])\n",
    "    result = None\n",
    "    if len(scored_sentences) == 0:\n",
    "        result = [sentences[0]]\n",
    "    else:\n",
    "        result = [scored_sentences[0][1]] + [s[1] for s in top]\n",
    "    return result\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "def get_shortest_path_to_negating(graph, subjects):\n",
    "    \"\"\"\n",
    "    get the shortest path from each subject to any negating or doubting/hedging word\n",
    "    returns: dictionary with subject as key, and 2-element list of path lengths [negating, doubting]\n",
    "    - if a subject does not exist in graph or have a path to any negating word, then the value will be [None, None]\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for s in subjects:\n",
    "        results[s] = [None, None, None]\n",
    "        if graph.has_node(s):\n",
    "            for word in negating_words:\n",
    "                if word in graph:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(graph, source = s, target = word)\n",
    "                        if results[s][0] == None or len(path) < results[s][0]:\n",
    "                            results[s][0] = len(path)\n",
    "                    except:\n",
    "                        continue\n",
    "            for word in hedging_words:\n",
    "                if word in graph:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(graph, source = s, target = word)\n",
    "                        if results[s][1] == None or len(path) < results[s][1]:\n",
    "                            results[s][1] = len(path)\n",
    "                    except:\n",
    "                        continue\n",
    "            for word in doubting_words:\n",
    "                if word in graph:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(graph, source = s, target = word)\n",
    "                        if results[s][2] == None or len(path) < results[s][2]:\n",
    "                            results[s][2] = len(path)\n",
    "                    except:\n",
    "                        continue\n",
    "    return results\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "def root_distance(graph, root):\n",
    "    \"\"\"\n",
    "    as implemented in the Emergent paper - return the shortest distance between the given root and any \n",
    "    doubting or hedging words in the graph, or None if no such path exists\n",
    "    \"\"\"\n",
    "    if root == None:\n",
    "        return None\n",
    "    min_dist = None\n",
    "    for word in sus_words:\n",
    "        if word in graph:\n",
    "            try:\n",
    "                path = nx.shortest_path(graph, source = root, target = word)\n",
    "                if min_dist == None or len(path) < min_dist:\n",
    "                    min_dist = len(path)\n",
    "            except:\n",
    "                continue\n",
    "    return min_dist\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "def get_neg_ancestors(doc):\n",
    "    \"\"\"\n",
    "    get the ancestors of every negating word\n",
    "    input: spacy Doc\n",
    "    returns: tuple  - set of words that were in the ancestor list of negating words, \n",
    "    set of words that were in ancestor list of refuting words, # negating words, # refuting words\n",
    "    \"\"\"\n",
    "    results = [set(), set(), set(), 0, 0, 0]\n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() in negating_words:\n",
    "            results[0] = results[0].union(\n",
    "                set([ancestor.lemma_.lower() for ancestor in token.ancestors if len(ancestor) > 2]).union(\n",
    "                    set([child.lemma_.lower() for child in token.head.children if child.text != token.text and len(child) > 2])\n",
    "                )\n",
    "            )\n",
    "            results[3] += 1\n",
    "        elif token.lemma_.lower() in doubting_words:\n",
    "            results[1] = results[1].union(\n",
    "                set([ancestor.lemma_.lower() for ancestor in token.ancestors if len(ancestor) > 2]).union(\n",
    "                    set([child.lemma_.lower() for child in token.head.children if child.text != token.text and len(child) > 2])\n",
    "                )\n",
    "            )\n",
    "            results[4] += 1\n",
    "        elif token.lemma_.lower() in hedging_words:\n",
    "            results[2] = results[1].union(\n",
    "                set([ancestor.lemma_.lower() for ancestor in token.ancestors if len(ancestor) > 2]).union(\n",
    "                    set([child.lemma_.lower() for child in token.head.children if child.text != token.text and len(child) > 2])\n",
    "                )\n",
    "            )\n",
    "            results[5] += 1\n",
    "    return tuple(results)\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "from sentic import SenticPhrase\n",
    "text = \"Hello, World!\"\n",
    "sp = SenticPhrase(text)\n",
    "\n",
    "def get_sentics(sent):\n",
    "    \"\"\"\n",
    "        input: Spacy processed sentence\n",
    "        output: a tuple containing the polarity score and a list of sentic values \n",
    "            (pleasantness, attention, sensitiviy, aptitude )\n",
    "    \"\"\"\n",
    "    info = sp.info(sent)\n",
    "          \n",
    "    # Sometimes sentic doesn't returns any sentics values, seems to be only when purely neutral. \n",
    "    # Some sort of tag to make sure this is true could help with classiciation! (if all 0's not enough)\n",
    "    sentics = {\"pleasantness\":0, \"attention\":0, \"sensitivity\":0, \"aptitude\":0}\n",
    "    sentics.update(info[\"sentics\"])\n",
    "    return [info['polarity'], sentics['aptitude'], sentics['attention'], sentics['sensitivity'], sentics['pleasantness']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25413, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
       "      <td>2008</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
       "      <td>1550</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
       "      <td>2</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
       "      <td>1793</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
       "      <td>37</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance\n",
       "0  Ferguson riots: Pregnant woman loses eye after...     2008  unrelated\n",
       "1  Crazy Conservatives Are Sure a Gitmo Detainee ...     1550  unrelated\n",
       "2  A Russian Guy Says His Justin Bieber Ringtone ...        2  unrelated\n",
       "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...     1793  unrelated\n",
       "4  Argentina's President Adopts Boy to End Werewo...       37  unrelated"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load test data\n",
    "test_stances = pd.read_csv(\"fn_data/competition_test_stances.csv\")\n",
    "print(test_stances.shape)\n",
    "test_stances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(904, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Al-Sisi has denied Israeli reports stating tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CNBC is reporting Tesla has chosen Nevada as t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A 4-inch version of the iPhone 6 is said to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>GR editor’s Note\\n\\nThere are no reports in th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody\n",
       "0        1  Al-Sisi has denied Israeli reports stating tha...\n",
       "1        2  A bereaved Afghan mother took revenge on the T...\n",
       "2        3  CNBC is reporting Tesla has chosen Nevada as t...\n",
       "3       12  A 4-inch version of the iPhone 6 is said to be...\n",
       "4       19  GR editor’s Note\\n\\nThere are no reports in th..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bodies = pd.read_csv(\"fn_data/competition_test_bodies.csv\")\n",
    "print(test_bodies.shape)\n",
    "test_bodies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0\n",
      "Processed 2500\n",
      "Processed 5000\n",
      "Processed 7500\n",
      "Processed 10000\n",
      "Processed 12500\n",
      "Processed 15000\n",
      "Processed 17500\n",
      "Processed 20000\n",
      "Processed 22500\n",
      "Processed 25000\n",
      "Done!\n",
      "Processed 0\n",
      "Processed 100\n",
      "Processed 200\n",
      "Processed 300\n",
      "Processed 400\n"
     ]
    }
   ],
   "source": [
    "headline_info = {}\n",
    "body_info = {}\n",
    "start = time.time()\n",
    "stance_data = list(test_stances.values)\n",
    "body_data = list(test_bodies.values)\n",
    "for headline in range(len(stance_data)):\n",
    "    if headline % 2500 == 0:\n",
    "        print(\"Processed \"+str(headline))\n",
    "    h, b_id, s = tuple(stance_data[headline])\n",
    "    if h not in headline_info:\n",
    "        nlp_h = nlp(preprocess(h))\n",
    "        headline_edges = get_edges(nlp_h)\n",
    "        headline_graph = nx.DiGraph(list(set([(e[0]['token'], e[1]['token']) for e in headline_edges])))\n",
    "        headline_subj = get_topics(nlp_h)\n",
    "        headline_svo = get_svos(nlp_h)\n",
    "        headline_root_dist = root_distance(headline_graph, list(headline_svo[1].keys())[0])\n",
    "        headline_neg_ancestors = get_neg_ancestors(nlp_h)\n",
    "        headline_info[h] = (nlp_h, headline_graph, headline_subj, headline_svo, headline_root_dist, headline_neg_ancestors, headline_edges)\n",
    "print(\"Done!\")\n",
    "for body in range(len(body_data)):\n",
    "    if body % 100 == 0:\n",
    "        print(\"Processed \"+str(body))\n",
    "    b_id, txt = tuple(body_data[body])\n",
    "    nlp_a = coref(preprocess(txt))\n",
    "    nlp_b = nlp(nlp_a._.coref_resolved.lower())\n",
    "    body_graph = build_graph(nlp_b)\n",
    "    body_info[b_id] = (nlp_b, body_graph)\n",
    "print(\"Done!\")\n",
    "end = time.time()\n",
    "print(int(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def build_idf(body_data):\n",
    "    idf = {}\n",
    "    for body in range(len(body_data)):\n",
    "        if body % 100 == 0:\n",
    "            print(\"Processed \"+str(body))\n",
    "        b_id, txt = tuple(body_data[body])\n",
    "        nlp_b = nlp(preprocess(txt))\n",
    "        tokens = [t for t in nlp_b if not t.is_stop and t.pos_ not in ['PUNCT','NUM','SYM','SPACE','PART']]\n",
    "        lemmatized = set([token.lemma_.lower() for token in tokens])\n",
    "        for tok in lemmatized:\n",
    "            if tok not in idf:\n",
    "                idf[tok] = 0\n",
    "            idf[tok] += 1\n",
    "    avg = float(sum(idf.values())) / len(idf)\n",
    "    for i in idf:\n",
    "        idf[i] = math.log(len(body_data)/idf[i])\n",
    "    idf[\"_avg\"] = math.log(len(body_data)/avg)\n",
    "    return idf\n",
    "idf = build_idf(list(train_bodies.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    svo = get_svos(sentence)\n",
    "\n",
    "    # list of words that belong to that part of speech\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    tokens = []\n",
    "    for token in sentence:\n",
    "        if not token.is_stop and token.pos_ not in ['PUNCT', 'NUM', 'SYM','SPACE','PART']:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nouns.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADV\":\n",
    "                adverbs.append(token.lemma_.lower())\n",
    "            tokens.append(token.lemma_.lower())   \n",
    "    \n",
    "    bigram = list(nltk.bigrams(tokens))\n",
    "    bigram_str = [x[0]+' '+x[1] for x in bigram]\n",
    "\n",
    "    return {\n",
    "        \"raw\": sentence.text,\n",
    "        \"tokens\": tokens,\n",
    "        \"bigrams\": bigram_str,\n",
    "        \"nouns\": nouns,\n",
    "        \"verbs\": verbs,\n",
    "        \"adjectives\": adjectives,\n",
    "        \"adverbs\": adverbs,\n",
    "        \"svo\": [list(item) for item in svo]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_sentence_idf(sent, idf):\n",
    "    # not very robust right now\n",
    "    score = 0\n",
    "    word_count = 0\n",
    "    for token in sent:\n",
    "        word_count += 1\n",
    "        t = token.lemma_.lower()\n",
    "        if t in idf:\n",
    "            score += idf[t]\n",
    "    return score/word_count if word_count > 4 else 0\n",
    "\n",
    "def process_body(body, idf):\n",
    "    sentences = [s for s in body.sents]\n",
    "    if len(sentences) == 0:\n",
    "        sentences = [body]\n",
    "\n",
    "    # first sentence of article\n",
    "    first_sentence_data = process_sentence(sentences[0])\n",
    "\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    tokens = []\n",
    "    for token in body:\n",
    "        if not token.is_stop and token.pos_ not in ['PUNCT', 'NUM', 'SYM','SPACE','PART']:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nouns.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADV\":\n",
    "                adverbs.append(token.lemma_.lower())\n",
    "            tokens.append(token.lemma_.lower())   \n",
    "    \n",
    "    bigram = list(nltk.bigrams(tokens))\n",
    "    bigram_str = [x[0]+' '+x[1] for x in bigram]\n",
    "\n",
    "    doc_len = len(tokens)\n",
    "    n_counter = Counter(nouns)\n",
    "    v_counter = Counter(verbs)\n",
    "    b_counter = Counter(bigram)\n",
    "    t_counter = Counter(tokens)\n",
    "\n",
    "    avg_idf = idf[\"_avg\"]\n",
    "    n_tfidf, v_tfidf, t_tfidf = {}, {}, {}\n",
    "    for n in n_counter:\n",
    "        n_tfidf[n] = (n_counter[n]/doc_len) *             (idf[n] if n in idf else avg_idf)\n",
    "    for v in v_counter:\n",
    "        v_tfidf[v] = (v_counter[v]/doc_len) *             (idf[v] if v in idf else avg_idf)\n",
    "    for t in t_counter:\n",
    "        t_tfidf[t] = (t_counter[t]/doc_len) *             (idf[t] if t in idf else avg_idf)\n",
    "    \n",
    "    common_nouns = sorted(n_tfidf, key=n_tfidf.get, reverse=True)[:5]\n",
    "    common_verbs = sorted(v_tfidf, key=v_tfidf.get, reverse=True)[:5]\n",
    "    common_tokens = sorted(t_tfidf, key=t_tfidf.get, reverse=True)[:5]\n",
    "\n",
    "    # no idf for bigrams increase \"common\" count to 10\n",
    "    common_bigrams = [x[0] for x in b_counter.most_common(10)]\n",
    "    \n",
    "    scored_sentences = [[idx, sent, score_sentence_idf(sent, idf)] for idx, sent in enumerate(sentences)]\n",
    "    scored_sentences = [s for s in scored_sentences] #filter out non-scoring sentences\n",
    "    scored_sentences.sort(key = lambda x: x[2], reverse = True)\n",
    "    most_significant_sentence_data = process_sentence(scored_sentences[0][1])\n",
    "\n",
    "    return {\n",
    "        \"raw\" : body.text,\n",
    "        \"tokens\": tokens,\n",
    "        \"bigrams\": bigram_str,\n",
    "        \"nouns\": nouns,\n",
    "        \"verbs\": verbs,\n",
    "        \"first_sentence\": first_sentence_data,\n",
    "        \"significant_sentence\": most_significant_sentence_data,\n",
    "        \"vocabulary\": list(set(tokens)),\n",
    "        \"common_tokens\": common_tokens,\n",
    "        \"common_nouns\": common_nouns,\n",
    "        \"common_verbs\": common_verbs,\n",
    "        \"common_bigrams\": common_bigrams,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_info_rel = {}\n",
    "body_info_rel = {}\n",
    "start = time.time()\n",
    "stance_data = list(test_stances.values)\n",
    "body_data = list(test_bodies.values)\n",
    "for headline in range(len(stance_data)):\n",
    "    if headline % 2500 == 0:\n",
    "        print(\"Processed \"+str(headline))\n",
    "    h, b_id, s = tuple(stance_data[headline])\n",
    "    if h not in headline_info_rel:\n",
    "        nlp_h = nlp(preprocess(h))\n",
    "        headline_processed = process_sentence(nlp_h)\n",
    "        headline_info_rel[h] = headline_processed\n",
    "print(\"Done!\")\n",
    "for body in range(len(body_data)):\n",
    "    if body % 100 == 0:\n",
    "        print(\"Processed \"+str(body))\n",
    "    b_id, txt = tuple(body_data[body])\n",
    "    nlp_a = coref(preprocess(txt))\n",
    "    nlp_b = nlp(nlp_a._.coref_resolved.lower())\n",
    "    body_processed = process_body(nlp_b, idf)\n",
    "    body_info_rel[b_id] = body_processed\n",
    "print(\"Done!\")\n",
    "end = time.time()\n",
    "print(int(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_rel(stance_df):\n",
    "    start = time.time()\n",
    "    data = list(stance_df.values)\n",
    "    features = []\n",
    "    actual = []\n",
    "    for item in data:\n",
    "        h, b, s = tuple(item)\n",
    "        fts = get_feats_rel(h, b)\n",
    "        features.append(fts)\n",
    "        actual.append(s)\n",
    "    end = time.time()\n",
    "    print(int(end-start))\n",
    "    return features, actual\n",
    "\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "def bow_cos_similarity(a, b):\n",
    "    vocab = list(set(a).union(set(b)))\n",
    "    a_bow, b_bow = set(a), set(b)\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return -1\n",
    "    a_vec = [(1 if i in a_bow else 0) for i in vocab]\n",
    "    b_vec = [(1 if i in b_bow else 0) for i in vocab]\n",
    "    return 1 - distance.cosine(a_vec, b_vec)\n",
    "\n",
    "def get_feats_rel(headline, body_id):\n",
    "    headline_data = headline_info_rel[headline]\n",
    "    body_data = body_info_rel[body_id]\n",
    "\n",
    "    shared_common_nouns = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['common_nouns'])))\n",
    "    shared_common_verbs = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['common_verbs'])))\n",
    "    shared_common_tokens = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['common_tokens'])))\n",
    "    shared_bigrams = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['common_bigrams'])))\n",
    "\n",
    "    shared_nouns_first = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['first_sentence']['nouns'])))\n",
    "    shared_verbs_first = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['first_sentence']['verbs'])))\n",
    "    shared_bigrams_first = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['first_sentence']['bigrams'])))\n",
    "    shared_tokens_first = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['first_sentence']['tokens'])))\n",
    "\n",
    "    shared_nouns_sig = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['significant_sentence']['nouns'])))\n",
    "    shared_verbs_sig = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['significant_sentence']['verbs'])))\n",
    "    shared_bigrams_sig = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['significant_sentence']['bigrams'])))\n",
    "    shared_tokens_sig = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['significant_sentence']['tokens'])))\n",
    "\n",
    "    headline_svo = headline_data['svo']\n",
    "    body_fst_svo = body_data['first_sentence']['svo']\n",
    "    body_sig_svo = body_data['significant_sentence']['svo']\n",
    "\n",
    "    # cosine similarity - no verbs because relatively few per sentence\n",
    "    cos_nouns_first = bow_cos_similarity(\n",
    "        headline_data['nouns'], body_data['first_sentence']['nouns'])\n",
    "    cos_bigrams_first = bow_cos_similarity(\n",
    "        headline_data['bigrams'], body_data['first_sentence']['bigrams'])\n",
    "    cos_tokens_first = bow_cos_similarity(\n",
    "        headline_data['tokens'], body_data['first_sentence']['tokens'])\n",
    "\n",
    "    cos_nouns_sig = bow_cos_similarity(\n",
    "        headline_data['nouns'], body_data['significant_sentence']['nouns'])\n",
    "    cos_bigrams_sig = bow_cos_similarity(\n",
    "        headline_data['bigrams'], body_data['significant_sentence']['bigrams'])\n",
    "    cos_tokens_sig = bow_cos_similarity(\n",
    "        headline_data['tokens'], body_data['significant_sentence']['tokens'])\n",
    "    \n",
    "    svo_cos_sim_fst = bow_cos_similarity(\n",
    "        body_fst_svo[0]+body_fst_svo[1]+body_fst_svo[2], \n",
    "        headline_svo[0]+headline_svo[1]+headline_svo[2])\n",
    "\n",
    "    svo_cos_sim_sig = bow_cos_similarity(\n",
    "        body_sig_svo[0]+body_sig_svo[1]+body_sig_svo[2], \n",
    "        headline_svo[0]+headline_svo[1]+headline_svo[2])\n",
    "    \n",
    "    svo_s_fst = len(set(body_fst_svo[0]).intersection(set(headline_svo[0]))) \n",
    "    svo_v_fst = len(set(body_fst_svo[1]).intersection(set(headline_svo[1])))\n",
    "    svo_o_fst = len(set(body_fst_svo[2]).intersection(set(headline_svo[2])))\n",
    "    svo_s_sig = len(set(body_sig_svo[0]).intersection(set(headline_svo[0])))\n",
    "    svo_v_sig = len(set(body_sig_svo[1]).intersection(set(headline_svo[1])))\n",
    "    svo_o_sig = len(set(body_sig_svo[2]).intersection(set(headline_svo[2])))\n",
    "    \n",
    "    return {\n",
    "        'shared_nouns': shared_common_nouns,\n",
    "        'shared_verbs': shared_common_verbs,\n",
    "        'shared_bigrams': shared_bigrams,\n",
    "        'shared_tokens': shared_common_tokens,\n",
    "\n",
    "        'shared_nouns_fst': shared_nouns_first,\n",
    "        'shared_verbs_fst': shared_verbs_first,\n",
    "        'shared_bigrams_fst': shared_bigrams_first,\n",
    "        'shared_tokens_fst': shared_tokens_first,\n",
    "\n",
    "        'shared_nouns_sig': shared_nouns_sig,\n",
    "        'shared_verbs_sig': shared_verbs_sig,\n",
    "        'shared_bigrams_sig': shared_bigrams_sig,\n",
    "        'shared_tokens_sig': shared_tokens_sig,\n",
    "\n",
    "        'cos_nouns_sig': cos_nouns_sig,\n",
    "        'cos_bigrams_sig': cos_bigrams_sig,\n",
    "        'cos_tokens_sig': cos_tokens_sig,\n",
    "\n",
    "        'cos_nouns_fst': cos_nouns_first,\n",
    "        'cos_bigrams_fst': cos_bigrams_first,\n",
    "        'cos_tokens_fst': cos_tokens_first,\n",
    "\n",
    "        'svo_cos_sim_fst' : svo_cos_sim_fst,\n",
    "        'svo_cos_sim_sig' : svo_cos_sim_sig,\n",
    "        \n",
    "        'svo_s_fst': svo_s_fst,\n",
    "        'svo_v_fst': svo_v_fst,\n",
    "        'svo_o_fst': svo_o_fst,\n",
    "\n",
    "        'svo_s_sig': svo_s_sig,\n",
    "        'svo_v_sig': svo_v_sig,\n",
    "        'svo_o_sig': svo_o_sig,\n",
    "    }\n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "stance_info_rel = get_features_rel(test_stances)\n",
    "stance_dict_rel = {}\n",
    "for idx, d in enumerate(list(test_stances.values)):\n",
    "    h, b, s = d\n",
    "    stance_dict_rel[(h, b)] = stance_info_rel[0][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vec(s):\n",
    "    vecs = [token.vector for token in s]\n",
    "    return np.nan_to_num(np.product(vecs, axis = 0))\n",
    "\n",
    "def get_features_stance(stance_df, n_sent = 5):\n",
    "    start = time.time()\n",
    "    data = list(stance_df.values)\n",
    "    features = []\n",
    "    actual = []\n",
    "    for item in data:\n",
    "        h, b, s = tuple(item)\n",
    "        headline, headline_graph, headline_subjs, headline_svo, headline_root_dist, headline_neg_ancestors, headline_edges  = headline_info[h]\n",
    "        body, body_graph = body_info[b]\n",
    "        \n",
    "        h_important_words = set(headline_subjs[\"_vocab\"]).union(set(headline_svo[0])).union(set(headline_svo[1])).union(set(headline_svo[2]))\n",
    "        \n",
    "        #sometimes the coref deletes bodies that are one sentence\n",
    "        if len(body) == 0:\n",
    "            body = nlp(preprocess(get_body(b)))\n",
    "            body_graph = build_graph(body)\n",
    "\n",
    "        #return the shortest path to negating word for each subject in headline_subjs, if one exists\n",
    "        neg_h = get_shortest_path_to_negating(headline_graph, h_important_words)\n",
    "        neg_b = get_shortest_path_to_negating(body_graph, h_important_words)\n",
    "\n",
    "        #body summary\n",
    "        summary = get_summary(body, h_important_words, n_sent)\n",
    "        first_summ_sentence = summary[0]\n",
    "        \n",
    "        summary_svos = [get_svos(s) for s in summary]\n",
    "        summary_root_dist = [root_distance(body_graph, list(s[1].keys())[0]) for s in summary_svos]\n",
    "        summary_neg_ancestors = [get_neg_ancestors(s) for s in summary]\n",
    "        summary_neg_counts = [s[3:] for s in summary_neg_ancestors]\n",
    "        \n",
    "        summary_neg_ancestors_superset = [set(), set(), set()]\n",
    "        for a in summary_neg_ancestors:\n",
    "            summary_neg_ancestors_superset[0] = summary_neg_ancestors_superset[0].union(a[0])\n",
    "            summary_neg_ancestors_superset[1] = summary_neg_ancestors_superset[1].union(a[1])\n",
    "            summary_neg_ancestors_superset[2] = summary_neg_ancestors_superset[2].union(a[2])\n",
    "            \n",
    "        #ancestors\n",
    "        h_anc = [[1 if w in headline_neg_ancestors[0] else -1 for w in h_important_words],\n",
    "                [1 if w in headline_neg_ancestors[1] else -1 for w in h_important_words],\n",
    "                [1 if w in headline_neg_ancestors[2] else -1 for w in h_important_words]]\n",
    "        b_anc = [[1 if w in summary_neg_ancestors_superset[0] else -1 for w in h_important_words],\n",
    "                [1 if w in summary_neg_ancestors_superset[1] else -1 for w in h_important_words],\n",
    "                [1 if w in summary_neg_ancestors_superset[2] else -1 for w in h_important_words]]    \n",
    "        neg_anc_sim = cosine_similarity(h_anc[0], b_anc[0])\n",
    "        doubt_anc_sim = cosine_similarity(h_anc[1], b_anc[1])\n",
    "        hedge_anc_sim = cosine_similarity(h_anc[2], b_anc[2])\n",
    "        neg_anc_overlap = len(headline_neg_ancestors[0].union(summary_neg_ancestors_superset[0]))\n",
    "        doubt_anc_overlap = len(headline_neg_ancestors[1].union(summary_neg_ancestors_superset[1]))\n",
    "        hedge_anc_overlap = len(headline_neg_ancestors[2].union(summary_neg_ancestors_superset[2]))\n",
    "        #svo\n",
    "        body_s, body_v, body_o = {}, {}, {}\n",
    "        headline_s, headline_v, headline_o = headline_svo\n",
    "        for svo in summary_svos:\n",
    "            body_s.update(svo[0])\n",
    "            body_v.update(svo[1])\n",
    "            body_o.update(svo[2])\n",
    "        body_s_vec = list(np.sum([body_s[s].vector for s in body_s], axis = 0)) if len(body_s) > 0 else np.zeros(384)\n",
    "        body_v_vec = list(np.sum([body_v[s].vector for s in body_v], axis = 0)) if len(body_v) > 0 else np.zeros(384)\n",
    "        body_o_vec = list(np.sum([body_o[s].vector for s in body_o], axis = 0)) if len(body_o) > 0 else np.zeros(384)\n",
    "    \n",
    "        headline_s_vec = list(np.sum([headline_s[s].vector for s in headline_s], axis = 0)) if len(headline_s) > 0 else np.zeros(384)\n",
    "        headline_v_vec = list(np.sum([headline_v[s].vector for s in headline_v], axis = 0)) if len(headline_v) > 0 else np.zeros(384)\n",
    "        headline_o_vec = list(np.sum([headline_o[s].vector for s in headline_o], axis = 0)) if len(headline_o) > 0 else np.zeros(384)\n",
    "        \n",
    "        cos_sim_s = cosine_similarity(body_s_vec, headline_s_vec)\n",
    "        cos_sim_v = cosine_similarity(body_v_vec, headline_v_vec)\n",
    "        cos_sim_o = cosine_similarity(body_o_vec, headline_o_vec)\n",
    "        \n",
    "        #negating paths\n",
    "        headline_paths = [neg_h[x] for x in neg_h]\n",
    "        headline_neg_paths = [1 if x[0] != None else -1 for x in headline_paths]\n",
    "        headline_doubt_paths = [1 if x[1] != None else -1 for x in headline_paths]\n",
    "        headline_hedge_paths = [1 if x[2] != None else -1 for x in headline_paths]\n",
    "        body_paths = [neg_b[x] for x in neg_h]\n",
    "        body_neg_paths = [1 if x[0] != None else -1 for x in body_paths]\n",
    "        body_doubt_paths = [1 if x[1] != None else -1 for x in body_paths]\n",
    "        body_hedge_paths = [1 if x[2] != None else -1 for x in body_paths]\n",
    "        \n",
    "        neg_path_cos_sim = cosine_similarity(headline_neg_paths, body_neg_paths)\n",
    "        hedge_path_cos_sim = cosine_similarity(headline_hedge_paths, body_hedge_paths)\n",
    "        doubt_path_cos_sim = cosine_similarity(headline_doubt_paths, body_doubt_paths)\n",
    "        \n",
    "        #root distance\n",
    "        summary_root_dists = [x if x != None else 15 for x in summary_root_dist]\n",
    "        avg_summary_root_dist = sum(summary_root_dists)/len(summary_root_dists)\n",
    "        root_dist_feats = [headline_root_dist, avg_summary_root_dist]\n",
    "        root_dist_feats = [x/15 if x != None else 1 for x in root_dist_feats]\n",
    "        root_dist_feats = root_dist_feats + [int(headline_root_dist == None), len([x for x in summary_root_dist if x != None])]\n",
    "    \n",
    "        #sentiment\n",
    "        headline_sent = get_sentiment(headline.text)\n",
    "        body_sents = [get_sentiment(s.text) for s in summary]\n",
    "        avg_body_sent = list(np.mean(body_sents, axis = 0))\n",
    "        diff_avg_sents = list(np.array(headline_sent) - avg_body_sent)\n",
    "        diff_sents = list(np.sum([get_diff_sentiment(headline_sent, s) for s in body_sents], axis = 0))\n",
    "        sent_cos_sim = cosine_similarity(headline_sent, avg_body_sent)\n",
    "\n",
    "        headline_sentics = get_sentics(headline.text)\n",
    "        body_sentics = [get_sentics(s.text) for s in summary]\n",
    "        avg_body_sentics = list(np.mean(body_sentics, axis = 0))\n",
    "        diff_avg_sentics = list(np.array(headline_sentics) - avg_body_sentics)\n",
    "        diff_sentics = list(np.sum([get_diff_sentiment(headline_sentics, s) for s in body_sentics], axis = 0))\n",
    "        sentics_cos_sim = cosine_similarity(headline_sentics, avg_body_sentics)\n",
    "        \n",
    "        #bow\n",
    "        headline_vocab = set([tok.lemma_.lower() for tok in headline])\n",
    "        fst_summ_vocab = set([tok.lemma_.lower() for tok in first_summ_sentence])\n",
    "        total_vocab = list(headline_vocab.union(fst_summ_vocab))\n",
    "        headline_embedding = [1 if tok in headline_vocab else -1 for tok in total_vocab]\n",
    "        fst_summ_embedding = [1 if tok in fst_summ_vocab else -1 for tok in total_vocab]\n",
    "        bow_cos_sim = cosine_similarity(headline_embedding, fst_summ_embedding)\n",
    "        \n",
    "        #word vecs\n",
    "        cos_sims = [cosine_similarity(get_sentence_vec(s), headline.vector) for s in summary]\n",
    "        fst_cos_sim = cos_sims[0]\n",
    "        avg_cos_sim = sum(cos_sims)/len(cos_sims)\n",
    "        \n",
    "        #neg_hedge_doubt distributions\n",
    "        hd_dist = list(headline_neg_ancestors[3:])\n",
    "        body_dist = list(np.sum(summary_neg_counts, axis = 0))\n",
    "        dist_sim = cosine_similarity(hd_dist, body_dist)\n",
    "        #build final features list\n",
    "        fts = (\n",
    "            [fst_cos_sim, avg_cos_sim, bow_cos_sim, \n",
    "                neg_path_cos_sim, hedge_path_cos_sim, doubt_path_cos_sim,\n",
    "                neg_anc_sim, hedge_anc_sim, doubt_anc_sim,\n",
    "                neg_anc_overlap, hedge_anc_overlap, doubt_anc_overlap,\n",
    "                cos_sim_s, cos_sim_v, cos_sim_o,\n",
    "                dist_sim, sent_cos_sim, sentics_cos_sim] + \n",
    "            diff_avg_sents + diff_sents + diff_avg_sentics + diff_sentics + \n",
    "            root_dist_feats + hd_dist + body_dist +\n",
    "            headline_sent + avg_body_sent + headline_sentics + avg_body_sentics\n",
    "        )\n",
    "        features.append(fts)\n",
    "        actual.append(s)\n",
    "    end = time.time()\n",
    "    print(int(end-start))\n",
    "    return features, actual\n",
    "\n",
    "\n",
    "# # Generate Features from headline/body pairs\n",
    "\n",
    "# In[96]:\n",
    "\n",
    "\n",
    "stance_data_stance = get_features_stance(test_stances, 5)\n",
    "stance_dict_stance = {}\n",
    "for idx, d in enumerate(list(test_stances.values)):\n",
    "    h, b, s = d\n",
    "    stance_dict_stance[(h, b)] = stance_data_stance[0][idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "rel_model = load('saved_models/relevance_detection_trained.joblib')\n",
    "stance_model = load('saved_models/stance_detection_trained.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data = [[],[],[]]\n",
    "for h,b,s in list(test_stances.values):\n",
    "    testing_data[0].append(list(stance_dict_rel[(h,b)].values()))\n",
    "    testing_data[1].append(stance_dict_stance[(h,b)])\n",
    "    testing_data[2].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_predicted = rel_model.predict(testing_data[0])\n",
    "stance_predicted = stance_model.predict(np.nan_to_num(np.array(testing_data[1]).astype(\"float32\")))\n",
    "predicted = []\n",
    "for i in range(len(rel_predicted)):\n",
    "    r = rel_predicted[i]\n",
    "    s = stance_predicted[i]\n",
    "    if r == \"unrelated\":\n",
    "        predicted.append(r)\n",
    "    else:\n",
    "        predicted.append(s)\n",
    "actual = testing_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.report_score(actual, predicted)\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(actual, predicted, average = None))\n",
    "print(\"Avg Precision Score\")\n",
    "print(precision_score(actual, predicted, average = None))\n",
    "matrix = confusion_matrix(actual,predicted)\n",
    "plot_confusion_matrix(matrix, classes=[\"agree\",\"disagree\", \"discuss\"],\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
