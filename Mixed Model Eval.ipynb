{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mixed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle the model files and load here to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannyyang/Library/Python/3.6/lib/python/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import score as sc\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, pairwise, f1_score, precision_score\n",
    "from scipy.spatial import distance\n",
    "from preprocessing.utils import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import importlib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import en_coref_md\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "coref = en_coref_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[21]:\n",
    "\n",
    "\n",
    "negating_words = set([\n",
    "    \"n't\", \"not\", \"no\", \n",
    "    \"never\", \"nobody\", \"non\", \"nope\"])\n",
    "doubting_words = set([\n",
    "    'fake','fraud', 'hoax', \n",
    "    'false', 'deny', 'denies', \n",
    "    'despite', 'doubt', \n",
    "    'bogus', 'debunk', 'prank', \n",
    "    'retract', 'scam', \"withdrawn\",\n",
    "    \"misinformation\"])\n",
    "hedging_words = set([\n",
    "    'allege', 'allegedly','apparently',\n",
    "    'appear','claim','could',\n",
    "    'evidently','largely','likely',\n",
    "    'mainly','may', 'maybe', 'might',\n",
    "    'mostly','perhaps','presumably',\n",
    "    'probably','purport', 'purportedly',\n",
    "    'reported', 'reportedly',\n",
    "    'rumor', 'rumour', 'rumored', 'rumoured',\n",
    "    'says','seem','somewhat',\n",
    "    'unconfirmed'])\n",
    "sus_words = doubting_words.union(hedging_words)\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(sentence):\n",
    "    sent =  vader.polarity_scores(sentence)\n",
    "    return [sent[\"pos\"],sent[\"neg\"],sent[\"neu\"],sent[\"compound\"]]\n",
    "\n",
    "def get_avg_sentiment(lst):\n",
    "    sents = np.array([get_sentiment(s) for s in lst])\n",
    "    return list(np.mean(sents, axis = 0))\n",
    "\n",
    "def get_diff_sentiment(a,b):\n",
    "    return list(np.array(a) - np.array(b))\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "def train_test_split(bodies, stances, split=0.8):\n",
    "    idx = np.random.permutation(np.arange(len(bodies)))\n",
    "    bodies = bodies.values[idx]\n",
    "    train = int(len(bodies)*0.8)\n",
    "    bodies_tr = set([i[0] for i in bodies[:train]])\n",
    "    bodies_val = set([i[0] for i in bodies[train:]])\n",
    "    stances_tr = stances.loc[stances[\"Body ID\"].isin(bodies_tr), :]\n",
    "    stances_val = stances.loc[stances[\"Body ID\"].isin(bodies_val), :]\n",
    "    return stances_tr, stances_val\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "def get_body(n):\n",
    "    return test_bodies.loc[lambda x: x[\"Body ID\"] == n, \"articleBody\"].item()\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace(\"' \",' ')\n",
    "    text = text.replace(\"'\\n\",'\\n')\n",
    "    text = text.replace(\" '\",' ')\n",
    "    text = text.replace('\"',' ')\n",
    "    text = text.replace('“',' ')\n",
    "    text = text.replace('”', ' ')\n",
    "    text = text.replace(\":\", \". \")\n",
    "    text = text.replace(\";\", \". \")\n",
    "    text = text.replace(\"...\", \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "    if all([a == 0 for a in x]) or all([a == 0 for a in y]):\n",
    "        return 0\n",
    "    return 1 - np.nan_to_num(distance.cosine(x,y))\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "def get_topics(doc):\n",
    "    \"\"\"\n",
    "    get topics of a sentence\n",
    "    input: spacy doc\n",
    "    output: dictionary with nouns as the key, and the set of noun chunks that contain the noun as the value\n",
    "    special entry _vocab has the set of all tokens in the dict\n",
    "    \"\"\"\n",
    "    subjs = {}\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk.root.text) > 2 and chunk.root.pos_ not in [\"NUM\", \"SYM\",\"PUNCT\"]:\n",
    "            txt = chunk.root.lemma_.lower()\n",
    "            if txt not in subjs:\n",
    "                subjs[txt] = set([txt])\n",
    "            subjs[txt].add(chunk.text.lower())\n",
    "    subjects_= []\n",
    "    for word in subjs:\n",
    "        for phrase in subjs[word]:\n",
    "            subjects_ += phrase.split(\" \")\n",
    "    subjs[\"_vocab\"] = set(subjects_)\n",
    "    return subjs\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "def get_svos(sent):\n",
    "    \"\"\"\n",
    "    input: Spacy processed sentence\n",
    "    output: dict of subj, dict of v, dict of obj (each word is lemmatized and lowercased)\n",
    "    each entry in dict has key of lemmatized token, value is actual token (to do traversals with later if needed)\n",
    "    \"\"\"\n",
    "    s = {}\n",
    "    v = {}\n",
    "    o = {}\n",
    "    for token in sent:\n",
    "        if token.dep_ == 'ROOT':\n",
    "            v[token.lemma_.lower()] = token\n",
    "        elif token.dep_ in [\"nsubj\", \"nsubjpass\", \"csubj\",\"csubjpass\", \"agent\",\"compound\"]:\n",
    "            s[token.lemma_.lower()] = token\n",
    "        elif token.dep_ in [\"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\"]:\n",
    "            o[token.lemma_.lower()] = token\n",
    "    # https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md\n",
    "    return (s,v,o)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "def build_graph(doc):\n",
    "    \"\"\"\n",
    "    build a NetworkX graph of the dependency tree\n",
    "    input: spacy Doc\n",
    "    output: networkx graph\n",
    "    \"\"\"\n",
    "    edges = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE']:\n",
    "            for child in token.children:\n",
    "                if child.pos_ not in ['SPACE']:\n",
    "                    edges.add((token.lemma_.lower(),child.lemma_.lower()))\n",
    "    graph = nx.DiGraph(list(edges))\n",
    "    return graph\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "def get_edges(doc):\n",
    "    \"\"\"\n",
    "    return list of edges\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT', 'SYM']:\n",
    "            for child in token.children:\n",
    "                if child.pos_ not in ['SPACE', 'PUNCT', 'SYM']:\n",
    "                    edges.append((\n",
    "                        {\"token\":token.lemma_.lower(), \"dep\":token.dep_ , \"pos\":token.pos_},\n",
    "                        {\"token\":child.lemma_.lower(), \"dep\":child.dep_ , \"pos\":child.pos_}\n",
    "                    ))\n",
    "    return edges\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "def get_summary(doc, subjects, n = 5):\n",
    "    \"\"\"\n",
    "    get summary of n sentences in document\n",
    "    first meaningful sentence will always be returned\n",
    "    \"\"\"\n",
    "    subjects_ = subjects\n",
    "    def score_sentence(sent):\n",
    "        # not very robust right now\n",
    "        score = 0\n",
    "        word_count = 0\n",
    "        for token in sent:\n",
    "            word_count += 1\n",
    "            t = token.lemma_.lower()\n",
    "            if t in subjects_:\n",
    "                score += 1\n",
    "            elif t in negating_words or t in doubting_words or t in hedging_words:\n",
    "                score += 1\n",
    "        return score/word_count if word_count > 4 else 0\n",
    "    sentences = [s for s in doc.sents]\n",
    "    scored_sentences = [[idx, sent, score_sentence(sent)] for idx, sent in enumerate(sentences)]\n",
    "    scored_sentences = [s for s in scored_sentences if s[2] > 0 and s[0] > 0] #filter out non-scoring sentences\n",
    "    scored_sentences.sort(key = lambda x: x[2], reverse = True)\n",
    "    top = scored_sentences[:n]\n",
    "    top.sort(key = lambda x: x[0])\n",
    "    scored_sentences.sort(key = lambda x: x[0])\n",
    "    result = None\n",
    "    if len(scored_sentences) == 0:\n",
    "        result = [sentences[0]]\n",
    "    else:\n",
    "        result = [scored_sentences[0][1]] + [s[1] for s in top]\n",
    "    return result\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "def get_shortest_path_to_negating(graph, subjects):\n",
    "    \"\"\"\n",
    "    get the shortest path from each subject to any negating or doubting/hedging word\n",
    "    returns: dictionary with subject as key, and 2-element list of path lengths [negating, doubting]\n",
    "    - if a subject does not exist in graph or have a path to any negating word, then the value will be [None, None]\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for s in subjects:\n",
    "        results[s] = [None, None, None]\n",
    "        if graph.has_node(s):\n",
    "            for word in negating_words:\n",
    "                if word in graph:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(graph, source = s, target = word)\n",
    "                        if results[s][0] == None or len(path) < results[s][0]:\n",
    "                            results[s][0] = len(path)\n",
    "                    except:\n",
    "                        continue\n",
    "            for word in hedging_words:\n",
    "                if word in graph:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(graph, source = s, target = word)\n",
    "                        if results[s][1] == None or len(path) < results[s][1]:\n",
    "                            results[s][1] = len(path)\n",
    "                    except:\n",
    "                        continue\n",
    "            for word in doubting_words:\n",
    "                if word in graph:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(graph, source = s, target = word)\n",
    "                        if results[s][2] == None or len(path) < results[s][2]:\n",
    "                            results[s][2] = len(path)\n",
    "                    except:\n",
    "                        continue\n",
    "    return results\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "def root_distance(graph, root):\n",
    "    \"\"\"\n",
    "    as implemented in the Emergent paper - return the shortest distance between the given root and any \n",
    "    doubting or hedging words in the graph, or None if no such path exists\n",
    "    \"\"\"\n",
    "    if root == None:\n",
    "        return None\n",
    "    min_dist = None\n",
    "    for word in sus_words:\n",
    "        if word in graph:\n",
    "            try:\n",
    "                path = nx.shortest_path(graph, source = root, target = word)\n",
    "                if min_dist == None or len(path) < min_dist:\n",
    "                    min_dist = len(path)\n",
    "            except:\n",
    "                continue\n",
    "    return min_dist\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "def get_neg_ancestors(doc):\n",
    "    \"\"\"\n",
    "    get the ancestors of every negating word\n",
    "    input: spacy Doc\n",
    "    returns: tuple  - set of words that were in the ancestor list of negating words, \n",
    "    set of words that were in ancestor list of refuting words, # negating words, # refuting words\n",
    "    \"\"\"\n",
    "    results = [set(), set(), set(), 0, 0, 0]\n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() in negating_words:\n",
    "            results[0] = results[0].union(\n",
    "                set([ancestor.lemma_.lower() for ancestor in token.ancestors if len(ancestor) > 2]).union(\n",
    "                    set([child.lemma_.lower() for child in token.head.children if child.text != token.text and len(child) > 2])\n",
    "                )\n",
    "            )\n",
    "            results[3] += 1\n",
    "        elif token.lemma_.lower() in doubting_words:\n",
    "            results[1] = results[1].union(\n",
    "                set([ancestor.lemma_.lower() for ancestor in token.ancestors if len(ancestor) > 2]).union(\n",
    "                    set([child.lemma_.lower() for child in token.head.children if child.text != token.text and len(child) > 2])\n",
    "                )\n",
    "            )\n",
    "            results[4] += 1\n",
    "        elif token.lemma_.lower() in hedging_words:\n",
    "            results[2] = results[1].union(\n",
    "                set([ancestor.lemma_.lower() for ancestor in token.ancestors if len(ancestor) > 2]).union(\n",
    "                    set([child.lemma_.lower() for child in token.head.children if child.text != token.text and len(child) > 2])\n",
    "                )\n",
    "            )\n",
    "            results[5] += 1\n",
    "    return tuple(results)\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "from sentic import SenticPhrase\n",
    "text = \"Hello, World!\"\n",
    "sp = SenticPhrase(text)\n",
    "\n",
    "def get_sentics(sent):\n",
    "    \"\"\"\n",
    "        input: Spacy processed sentence\n",
    "        output: a tuple containing the polarity score and a list of sentic values \n",
    "            (pleasantness, attention, sensitiviy, aptitude )\n",
    "    \"\"\"\n",
    "    info = sp.info(sent)\n",
    "          \n",
    "    # Sometimes sentic doesn't returns any sentics values, seems to be only when purely neutral. \n",
    "    # Some sort of tag to make sure this is true could help with classiciation! (if all 0's not enough)\n",
    "    sentics = {\"pleasantness\":0, \"attention\":0, \"sensitivity\":0, \"aptitude\":0}\n",
    "    sentics.update(info[\"sentics\"])\n",
    "    return [info['polarity'], sentics['aptitude'], sentics['attention'], sentics['sensitivity'], sentics['pleasantness']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25413, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
       "      <td>2008</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
       "      <td>1550</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
       "      <td>2</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
       "      <td>1793</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
       "      <td>37</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance\n",
       "0  Ferguson riots: Pregnant woman loses eye after...     2008  unrelated\n",
       "1  Crazy Conservatives Are Sure a Gitmo Detainee ...     1550  unrelated\n",
       "2  A Russian Guy Says His Justin Bieber Ringtone ...        2  unrelated\n",
       "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...     1793  unrelated\n",
       "4  Argentina's President Adopts Boy to End Werewo...       37  unrelated"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load test data\n",
    "test_stances = pd.read_csv(\"fn_data/competition_test_stances.csv\")\n",
    "print(test_stances.shape)\n",
    "test_stances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(904, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Al-Sisi has denied Israeli reports stating tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CNBC is reporting Tesla has chosen Nevada as t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A 4-inch version of the iPhone 6 is said to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>GR editor’s Note\\n\\nThere are no reports in th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody\n",
       "0        1  Al-Sisi has denied Israeli reports stating tha...\n",
       "1        2  A bereaved Afghan mother took revenge on the T...\n",
       "2        3  CNBC is reporting Tesla has chosen Nevada as t...\n",
       "3       12  A 4-inch version of the iPhone 6 is said to be...\n",
       "4       19  GR editor’s Note\\n\\nThere are no reports in th..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bodies = pd.read_csv(\"fn_data/competition_test_bodies.csv\")\n",
    "print(test_bodies.shape)\n",
    "test_bodies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0\n",
      "Processed 2500\n",
      "Processed 5000\n",
      "Processed 7500\n",
      "Processed 10000\n",
      "Processed 12500\n",
      "Processed 15000\n",
      "Processed 17500\n",
      "Processed 20000\n",
      "Processed 22500\n",
      "Processed 25000\n",
      "Done!\n",
      "Processed 0\n",
      "Processed 100\n",
      "Processed 200\n",
      "Processed 300\n",
      "Processed 400\n",
      "Processed 500\n",
      "Processed 600\n",
      "Processed 700\n",
      "Processed 800\n",
      "Processed 900\n",
      "Done!\n",
      "356\n"
     ]
    }
   ],
   "source": [
    "headline_info = {}\n",
    "body_info = {}\n",
    "start = time.time()\n",
    "stance_data = list(test_stances.values)\n",
    "body_data = list(test_bodies.values)\n",
    "for headline in range(len(stance_data)):\n",
    "    if headline % 2500 == 0:\n",
    "        print(\"Processed \"+str(headline))\n",
    "    h, b_id, s = tuple(stance_data[headline])\n",
    "    if h not in headline_info:\n",
    "        nlp_h = nlp(preprocess(h))\n",
    "        headline_edges = get_edges(nlp_h)\n",
    "        headline_graph = nx.DiGraph(list(set([(e[0]['token'], e[1]['token']) for e in headline_edges])))\n",
    "        headline_subj = get_topics(nlp_h)\n",
    "        headline_svo = get_svos(nlp_h)\n",
    "        headline_root_dist = root_distance(headline_graph, list(headline_svo[1].keys())[0])\n",
    "        headline_neg_ancestors = get_neg_ancestors(nlp_h)\n",
    "        headline_info[h] = (nlp_h, headline_graph, headline_subj, headline_svo, headline_root_dist, headline_neg_ancestors, headline_edges)\n",
    "print(\"Done!\")\n",
    "for body in range(len(body_data)):\n",
    "    if body % 100 == 0:\n",
    "        print(\"Processed \"+str(body))\n",
    "    b_id, txt = tuple(body_data[body])\n",
    "    nlp_a = coref(preprocess(txt))\n",
    "    nlp_b = nlp(nlp_a._.coref_resolved.lower())\n",
    "    body_graph = build_graph(nlp_b)\n",
    "    body_info[b_id] = (nlp_b, body_graph)\n",
    "print(\"Done!\")\n",
    "end = time.time()\n",
    "print(int(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0\n",
      "Processed 100\n",
      "Processed 200\n",
      "Processed 300\n",
      "Processed 400\n",
      "Processed 500\n",
      "Processed 600\n",
      "Processed 700\n",
      "Processed 800\n",
      "Processed 900\n",
      "Processed 1000\n",
      "Processed 1100\n",
      "Processed 1200\n",
      "Processed 1300\n",
      "Processed 1400\n",
      "Processed 1500\n",
      "Processed 1600\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def build_idf(body_data):\n",
    "    idf = {}\n",
    "    for body in range(len(body_data)):\n",
    "        if body % 100 == 0:\n",
    "            print(\"Processed \"+str(body))\n",
    "        b_id, txt = tuple(body_data[body])\n",
    "        nlp_b = nlp(preprocess(txt))\n",
    "        tokens = [t for t in nlp_b if not t.is_stop and t.pos_ not in ['PUNCT','NUM','SYM','SPACE','PART']]\n",
    "        lemmatized = set([token.lemma_.lower() for token in tokens])\n",
    "        for tok in lemmatized:\n",
    "            if tok not in idf:\n",
    "                idf[tok] = 0\n",
    "            idf[tok] += 1\n",
    "    avg = float(sum(idf.values())) / len(idf)\n",
    "    for i in idf:\n",
    "        idf[i] = math.log(len(body_data)/idf[i])\n",
    "    idf[\"_avg\"] = math.log(len(body_data)/avg)\n",
    "    return idf\n",
    "idf = build_idf(list(train_bodies.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    svo = get_svos(sentence)\n",
    "\n",
    "    # list of words that belong to that part of speech\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    tokens = []\n",
    "    for token in sentence:\n",
    "        if not token.is_stop and token.pos_ not in ['PUNCT', 'NUM', 'SYM','SPACE','PART']:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nouns.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADV\":\n",
    "                adverbs.append(token.lemma_.lower())\n",
    "            tokens.append(token.lemma_.lower())   \n",
    "    \n",
    "    bigram = list(nltk.bigrams(tokens))\n",
    "    bigram_str = [x[0]+' '+x[1] for x in bigram]\n",
    "\n",
    "    return {\n",
    "        \"raw\": sentence.text,\n",
    "        \"tokens\": tokens,\n",
    "        \"bigrams\": bigram_str,\n",
    "        \"nouns\": nouns,\n",
    "        \"verbs\": verbs,\n",
    "        \"adjectives\": adjectives,\n",
    "        \"adverbs\": adverbs,\n",
    "        \"svo\": [list(item) for item in svo]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_sentence_idf(sent, idf):\n",
    "    # not very robust right now\n",
    "    score = 0\n",
    "    word_count = 0\n",
    "    for token in sent:\n",
    "        word_count += 1\n",
    "        t = token.lemma_.lower()\n",
    "        if t in idf:\n",
    "            score += idf[t]\n",
    "    return score/word_count if word_count > 4 else 0\n",
    "\n",
    "def process_body(body, idf):\n",
    "    sentences = [s for s in body.sents]\n",
    "    if len(sentences) == 0:\n",
    "        sentences = [body]\n",
    "\n",
    "    # first sentence of article\n",
    "    first_sentence_data = process_sentence(sentences[0])\n",
    "\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    tokens = []\n",
    "    for token in body:\n",
    "        if not token.is_stop and token.pos_ not in ['PUNCT', 'NUM', 'SYM','SPACE','PART']:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nouns.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADV\":\n",
    "                adverbs.append(token.lemma_.lower())\n",
    "            tokens.append(token.lemma_.lower())   \n",
    "    \n",
    "    bigram = list(nltk.bigrams(tokens))\n",
    "    bigram_str = [x[0]+' '+x[1] for x in bigram]\n",
    "\n",
    "    doc_len = len(tokens)\n",
    "    n_counter = Counter(nouns)\n",
    "    v_counter = Counter(verbs)\n",
    "    b_counter = Counter(bigram)\n",
    "    t_counter = Counter(tokens)\n",
    "\n",
    "    avg_idf = idf[\"_avg\"]\n",
    "    n_tfidf, v_tfidf, t_tfidf = {}, {}, {}\n",
    "    for n in n_counter:\n",
    "        n_tfidf[n] = (n_counter[n]/doc_len) *             (idf[n] if n in idf else avg_idf)\n",
    "    for v in v_counter:\n",
    "        v_tfidf[v] = (v_counter[v]/doc_len) *             (idf[v] if v in idf else avg_idf)\n",
    "    for t in t_counter:\n",
    "        t_tfidf[t] = (t_counter[t]/doc_len) *             (idf[t] if t in idf else avg_idf)\n",
    "    \n",
    "    common_nouns = sorted(n_tfidf, key=n_tfidf.get, reverse=True)[:5]\n",
    "    common_verbs = sorted(v_tfidf, key=v_tfidf.get, reverse=True)[:5]\n",
    "    common_tokens = sorted(t_tfidf, key=t_tfidf.get, reverse=True)[:5]\n",
    "\n",
    "    # no idf for bigrams increase \"common\" count to 10\n",
    "    common_bigrams = [x[0] for x in b_counter.most_common(10)]\n",
    "    \n",
    "    scored_sentences = [[idx, sent, score_sentence_idf(sent, idf)] for idx, sent in enumerate(sentences)]\n",
    "    scored_sentences = [s for s in scored_sentences] #filter out non-scoring sentences\n",
    "    scored_sentences.sort(key = lambda x: x[2], reverse = True)\n",
    "    most_significant_sentence_data = process_sentence(scored_sentences[0][1])\n",
    "\n",
    "    return {\n",
    "        \"raw\" : body.text,\n",
    "        \"tokens\": tokens,\n",
    "        \"bigrams\": bigram_str,\n",
    "        \"nouns\": nouns,\n",
    "        \"verbs\": verbs,\n",
    "        \"first_sentence\": first_sentence_data,\n",
    "        \"significant_sentence\": most_significant_sentence_data,\n",
    "        \"vocabulary\": list(set(tokens)),\n",
    "        \"common_tokens\": common_tokens,\n",
    "        \"common_nouns\": common_nouns,\n",
    "        \"common_verbs\": common_verbs,\n",
    "        \"common_bigrams\": common_bigrams,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0\n",
      "Processed 2500\n",
      "Processed 5000\n",
      "Processed 7500\n",
      "Processed 10000\n",
      "Processed 12500\n",
      "Processed 15000\n",
      "Processed 17500\n",
      "Processed 20000\n",
      "Processed 22500\n",
      "Processed 25000\n",
      "Done!\n",
      "Processed 0\n",
      "Processed 100\n",
      "Processed 200\n",
      "Processed 300\n",
      "Processed 400\n",
      "Processed 500\n",
      "Processed 600\n",
      "Processed 700\n",
      "Processed 800\n",
      "Processed 900\n",
      "Done!\n",
      "339\n"
     ]
    }
   ],
   "source": [
    "headline_info_rel = {}\n",
    "body_info_rel = {}\n",
    "start = time.time()\n",
    "stance_data = list(test_stances.values)\n",
    "body_data = list(test_bodies.values)\n",
    "for headline in range(len(stance_data)):\n",
    "    if headline % 2500 == 0:\n",
    "        print(\"Processed \"+str(headline))\n",
    "    h, b_id, s = tuple(stance_data[headline])\n",
    "    if h not in headline_info_rel:\n",
    "        nlp_h = nlp(preprocess(h))\n",
    "        headline_processed = process_sentence(nlp_h)\n",
    "        headline_info_rel[h] = headline_processed\n",
    "print(\"Done!\")\n",
    "for body in range(len(body_data)):\n",
    "    if body % 100 == 0:\n",
    "        print(\"Processed \"+str(body))\n",
    "    b_id, txt = tuple(body_data[body])\n",
    "    nlp_a = coref(preprocess(txt))\n",
    "    nlp_b = nlp(nlp_a._.coref_resolved.lower())\n",
    "    body_processed = process_body(nlp_b, idf)\n",
    "    body_info_rel[b_id] = body_processed\n",
    "print(\"Done!\")\n",
    "end = time.time()\n",
    "print(int(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "def get_features_rel(stance_df):\n",
    "    start = time.time()\n",
    "    data = list(stance_df.values)\n",
    "    features = []\n",
    "    actual = []\n",
    "    for item in data:\n",
    "        h, b, s = tuple(item)\n",
    "        fts = get_feats_rel(h, b)\n",
    "        features.append(fts)\n",
    "        actual.append(s)\n",
    "    end = time.time()\n",
    "    print(int(end-start))\n",
    "    return features, actual\n",
    "\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "def bow_cos_similarity(a, b):\n",
    "    vocab = list(set(a).union(set(b)))\n",
    "    a_bow, b_bow = set(a), set(b)\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return -1\n",
    "    a_vec = [(1 if i in a_bow else 0) for i in vocab]\n",
    "    b_vec = [(1 if i in b_bow else 0) for i in vocab]\n",
    "    return 1 - distance.cosine(a_vec, b_vec)\n",
    "\n",
    "def get_feats_rel(headline, body_id):\n",
    "    headline_data = headline_info_rel[headline]\n",
    "    body_data = body_info_rel[body_id]\n",
    "\n",
    "    shared_common_nouns = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['common_nouns'])))\n",
    "    shared_common_verbs = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['common_verbs'])))\n",
    "    shared_common_tokens = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['common_tokens'])))\n",
    "    shared_bigrams = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['common_bigrams'])))\n",
    "\n",
    "    shared_nouns_first = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['first_sentence']['nouns'])))\n",
    "    shared_verbs_first = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['first_sentence']['verbs'])))\n",
    "    shared_bigrams_first = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['first_sentence']['bigrams'])))\n",
    "    shared_tokens_first = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['first_sentence']['tokens'])))\n",
    "\n",
    "    shared_nouns_sig = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['significant_sentence']['nouns'])))\n",
    "    shared_verbs_sig = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['significant_sentence']['verbs'])))\n",
    "    shared_bigrams_sig = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['significant_sentence']['bigrams'])))\n",
    "    shared_tokens_sig = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['significant_sentence']['tokens'])))\n",
    "\n",
    "    headline_svo = headline_data['svo']\n",
    "    body_fst_svo = body_data['first_sentence']['svo']\n",
    "    body_sig_svo = body_data['significant_sentence']['svo']\n",
    "\n",
    "    # cosine similarity - no verbs because relatively few per sentence\n",
    "    cos_nouns_first = bow_cos_similarity(\n",
    "        headline_data['nouns'], body_data['first_sentence']['nouns'])\n",
    "    cos_bigrams_first = bow_cos_similarity(\n",
    "        headline_data['bigrams'], body_data['first_sentence']['bigrams'])\n",
    "    cos_tokens_first = bow_cos_similarity(\n",
    "        headline_data['tokens'], body_data['first_sentence']['tokens'])\n",
    "\n",
    "    cos_nouns_sig = bow_cos_similarity(\n",
    "        headline_data['nouns'], body_data['significant_sentence']['nouns'])\n",
    "    cos_bigrams_sig = bow_cos_similarity(\n",
    "        headline_data['bigrams'], body_data['significant_sentence']['bigrams'])\n",
    "    cos_tokens_sig = bow_cos_similarity(\n",
    "        headline_data['tokens'], body_data['significant_sentence']['tokens'])\n",
    "    \n",
    "    svo_cos_sim_fst = bow_cos_similarity(\n",
    "        body_fst_svo[0]+body_fst_svo[1]+body_fst_svo[2], \n",
    "        headline_svo[0]+headline_svo[1]+headline_svo[2])\n",
    "\n",
    "    svo_cos_sim_sig = bow_cos_similarity(\n",
    "        body_sig_svo[0]+body_sig_svo[1]+body_sig_svo[2], \n",
    "        headline_svo[0]+headline_svo[1]+headline_svo[2])\n",
    "    \n",
    "    svo_s_fst = len(set(body_fst_svo[0]).intersection(set(headline_svo[0]))) \n",
    "    svo_v_fst = len(set(body_fst_svo[1]).intersection(set(headline_svo[1])))\n",
    "    svo_o_fst = len(set(body_fst_svo[2]).intersection(set(headline_svo[2])))\n",
    "    svo_s_sig = len(set(body_sig_svo[0]).intersection(set(headline_svo[0])))\n",
    "    svo_v_sig = len(set(body_sig_svo[1]).intersection(set(headline_svo[1])))\n",
    "    svo_o_sig = len(set(body_sig_svo[2]).intersection(set(headline_svo[2])))\n",
    "    \n",
    "    return {\n",
    "        'shared_nouns': shared_common_nouns,\n",
    "        'shared_verbs': shared_common_verbs,\n",
    "        'shared_bigrams': shared_bigrams,\n",
    "        'shared_tokens': shared_common_tokens,\n",
    "\n",
    "        'shared_nouns_fst': shared_nouns_first,\n",
    "        'shared_verbs_fst': shared_verbs_first,\n",
    "        'shared_bigrams_fst': shared_bigrams_first,\n",
    "        'shared_tokens_fst': shared_tokens_first,\n",
    "\n",
    "        'shared_nouns_sig': shared_nouns_sig,\n",
    "        'shared_verbs_sig': shared_verbs_sig,\n",
    "        'shared_bigrams_sig': shared_bigrams_sig,\n",
    "        'shared_tokens_sig': shared_tokens_sig,\n",
    "\n",
    "        'cos_nouns_sig': cos_nouns_sig,\n",
    "        'cos_bigrams_sig': cos_bigrams_sig,\n",
    "        'cos_tokens_sig': cos_tokens_sig,\n",
    "\n",
    "        'cos_nouns_fst': cos_nouns_first,\n",
    "        'cos_bigrams_fst': cos_bigrams_first,\n",
    "        'cos_tokens_fst': cos_tokens_first,\n",
    "\n",
    "        'svo_cos_sim_fst' : svo_cos_sim_fst,\n",
    "        'svo_cos_sim_sig' : svo_cos_sim_sig,\n",
    "        \n",
    "        'svo_s_fst': svo_s_fst,\n",
    "        'svo_v_fst': svo_v_fst,\n",
    "        'svo_o_fst': svo_o_fst,\n",
    "\n",
    "        'svo_s_sig': svo_s_sig,\n",
    "        'svo_v_sig': svo_v_sig,\n",
    "        'svo_o_sig': svo_o_sig,\n",
    "    }\n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "stance_info_rel = get_features_rel(test_stances)\n",
    "stance_dict_rel = {}\n",
    "for idx, d in enumerate(list(test_stances.values)):\n",
    "    h, b, s = d\n",
    "    stance_dict_rel[(h, b)] = stance_info_rel[0][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:120: RuntimeWarning: overflow encountered in double_scalars\n",
      "/Users/dannyyang/Library/Python/3.6/lib/python/site-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_vec(s):\n",
    "    vecs = [token.vector for token in s]\n",
    "    return np.nan_to_num(np.product(vecs, axis = 0))\n",
    "\n",
    "def get_features_stance(stance_df, n_sent = 5):\n",
    "    start = time.time()\n",
    "    data = list(stance_df.values)\n",
    "    features = []\n",
    "    actual = []\n",
    "    for item in data:\n",
    "        h, b, s = tuple(item)\n",
    "        headline, headline_graph, headline_subjs, headline_svo, headline_root_dist, headline_neg_ancestors, headline_edges  = headline_info[h]\n",
    "        body, body_graph = body_info[b]\n",
    "        \n",
    "        h_important_words = set(headline_subjs[\"_vocab\"]).union(set(headline_svo[0])).union(set(headline_svo[1])).union(set(headline_svo[2]))\n",
    "        \n",
    "        #sometimes the coref deletes bodies that are one sentence\n",
    "        if len(body) == 0:\n",
    "            body = nlp(preprocess(get_body(b)))\n",
    "            body_graph = build_graph(body)\n",
    "\n",
    "        #return the shortest path to negating word for each subject in headline_subjs, if one exists\n",
    "        neg_h = get_shortest_path_to_negating(headline_graph, h_important_words)\n",
    "        neg_b = get_shortest_path_to_negating(body_graph, h_important_words)\n",
    "\n",
    "        #body summary\n",
    "        summary = get_summary(body, h_important_words, n_sent)\n",
    "        first_summ_sentence = summary[0]\n",
    "        \n",
    "        summary_svos = [get_svos(s) for s in summary]\n",
    "        summary_root_dist = [root_distance(body_graph, list(s[1].keys())[0]) for s in summary_svos]\n",
    "        summary_neg_ancestors = [get_neg_ancestors(s) for s in summary]\n",
    "        summary_neg_counts = [s[3:] for s in summary_neg_ancestors]\n",
    "        \n",
    "        summary_neg_ancestors_superset = [set(), set(), set()]\n",
    "        for a in summary_neg_ancestors:\n",
    "            summary_neg_ancestors_superset[0] = summary_neg_ancestors_superset[0].union(a[0])\n",
    "            summary_neg_ancestors_superset[1] = summary_neg_ancestors_superset[1].union(a[1])\n",
    "            summary_neg_ancestors_superset[2] = summary_neg_ancestors_superset[2].union(a[2])\n",
    "            \n",
    "        #ancestors\n",
    "        h_anc = [[1 if w in headline_neg_ancestors[0] else -1 for w in h_important_words],\n",
    "                [1 if w in headline_neg_ancestors[1] else -1 for w in h_important_words],\n",
    "                [1 if w in headline_neg_ancestors[2] else -1 for w in h_important_words]]\n",
    "        b_anc = [[1 if w in summary_neg_ancestors_superset[0] else -1 for w in h_important_words],\n",
    "                [1 if w in summary_neg_ancestors_superset[1] else -1 for w in h_important_words],\n",
    "                [1 if w in summary_neg_ancestors_superset[2] else -1 for w in h_important_words]]    \n",
    "        neg_anc_sim = cosine_similarity(h_anc[0], b_anc[0])\n",
    "        doubt_anc_sim = cosine_similarity(h_anc[1], b_anc[1])\n",
    "        hedge_anc_sim = cosine_similarity(h_anc[2], b_anc[2])\n",
    "        neg_anc_overlap = len(headline_neg_ancestors[0].union(summary_neg_ancestors_superset[0]))\n",
    "        doubt_anc_overlap = len(headline_neg_ancestors[1].union(summary_neg_ancestors_superset[1]))\n",
    "        hedge_anc_overlap = len(headline_neg_ancestors[2].union(summary_neg_ancestors_superset[2]))\n",
    "        #svo\n",
    "        body_s, body_v, body_o = {}, {}, {}\n",
    "        headline_s, headline_v, headline_o = headline_svo\n",
    "        for svo in summary_svos:\n",
    "            body_s.update(svo[0])\n",
    "            body_v.update(svo[1])\n",
    "            body_o.update(svo[2])\n",
    "        body_s_vec = list(np.sum([body_s[s].vector for s in body_s], axis = 0)) if len(body_s) > 0 else np.zeros(384)\n",
    "        body_v_vec = list(np.sum([body_v[s].vector for s in body_v], axis = 0)) if len(body_v) > 0 else np.zeros(384)\n",
    "        body_o_vec = list(np.sum([body_o[s].vector for s in body_o], axis = 0)) if len(body_o) > 0 else np.zeros(384)\n",
    "    \n",
    "        headline_s_vec = list(np.sum([headline_s[s].vector for s in headline_s], axis = 0)) if len(headline_s) > 0 else np.zeros(384)\n",
    "        headline_v_vec = list(np.sum([headline_v[s].vector for s in headline_v], axis = 0)) if len(headline_v) > 0 else np.zeros(384)\n",
    "        headline_o_vec = list(np.sum([headline_o[s].vector for s in headline_o], axis = 0)) if len(headline_o) > 0 else np.zeros(384)\n",
    "        \n",
    "        cos_sim_s = cosine_similarity(body_s_vec, headline_s_vec)\n",
    "        cos_sim_v = cosine_similarity(body_v_vec, headline_v_vec)\n",
    "        cos_sim_o = cosine_similarity(body_o_vec, headline_o_vec)\n",
    "        \n",
    "        #negating paths\n",
    "        headline_paths = [neg_h[x] for x in neg_h]\n",
    "        headline_neg_paths = [1 if x[0] != None else -1 for x in headline_paths]\n",
    "        headline_doubt_paths = [1 if x[1] != None else -1 for x in headline_paths]\n",
    "        headline_hedge_paths = [1 if x[2] != None else -1 for x in headline_paths]\n",
    "        body_paths = [neg_h[x] for x in neg_h]\n",
    "        body_neg_paths = [1 if x[0] != None else -1 for x in body_paths]\n",
    "        body_doubt_paths = [1 if x[1] != None else -1 for x in body_paths]\n",
    "        body_hedge_paths = [1 if x[2] != None else -1 for x in body_paths]\n",
    "        \n",
    "        neg_path_cos_sim = cosine_similarity(headline_neg_paths, body_neg_paths)\n",
    "        hedge_path_cos_sim = cosine_similarity(headline_hedge_paths, body_hedge_paths)\n",
    "        doubt_path_cos_sim = cosine_similarity(headline_doubt_paths, body_doubt_paths)\n",
    "        \n",
    "        #root distance\n",
    "        summary_root_dists = [x if x != None else 15 for x in summary_root_dist]\n",
    "        avg_summary_root_dist = sum(summary_root_dists)/len(summary_root_dists)\n",
    "        root_dist_feats = [headline_root_dist, avg_summary_root_dist]\n",
    "        root_dist_feats = [x/15 if x != None else 1 for x in root_dist_feats]\n",
    "        root_dist_feats = root_dist_feats + [int(headline_root_dist == None), len([x for x in summary_root_dist if x != None])]\n",
    "    \n",
    "        #sentiment\n",
    "        headline_sent = get_sentiment(headline.text)\n",
    "        body_sents = [get_sentiment(s.text) for s in summary]\n",
    "        avg_body_sent = list(np.mean(body_sents, axis = 0))\n",
    "        diff_avg_sents = list(np.array(headline_sent) - avg_body_sent)\n",
    "        diff_sents = list(np.sum([get_diff_sentiment(headline_sent, s) for s in body_sents], axis = 0))\n",
    "        sent_cos_sim = cosine_similarity(headline_sent, avg_body_sent)\n",
    "\n",
    "        headline_sentics = get_sentics(headline.text)\n",
    "        body_sentics = [get_sentics(s.text) for s in summary]\n",
    "        avg_body_sentics = list(np.mean(body_sentics, axis = 0))\n",
    "        diff_avg_sentics = list(np.array(headline_sentics) - avg_body_sentics)\n",
    "        diff_sentics = list(np.sum([get_diff_sentiment(headline_sentics, s) for s in body_sentics], axis = 0))\n",
    "        sentics_cos_sim = cosine_similarity(headline_sentics, avg_body_sentics)\n",
    "        \n",
    "        #bow\n",
    "        headline_vocab = set([tok.lemma_.lower() for tok in headline])\n",
    "        fst_summ_vocab = set([tok.lemma_.lower() for tok in first_summ_sentence])\n",
    "        total_vocab = list(headline_vocab.union(fst_summ_vocab))\n",
    "        headline_embedding = [1 if tok in headline_vocab else -1 for tok in total_vocab]\n",
    "        fst_summ_embedding = [1 if tok in fst_summ_vocab else -1 for tok in total_vocab]\n",
    "        bow_cos_sim = cosine_similarity(headline_embedding, fst_summ_embedding)\n",
    "        \n",
    "        #word vecs\n",
    "        cos_sims = [cosine_similarity(get_sentence_vec(s), headline.vector) for s in summary]\n",
    "        fst_cos_sim = cos_sims[0]\n",
    "        avg_cos_sim = sum(cos_sims)/len(cos_sims)\n",
    "        \n",
    "        #neg_hedge_doubt distributions\n",
    "        hd_dist = list(headline_neg_ancestors[3:])\n",
    "        body_dist = list(np.sum(summary_neg_counts, axis = 0))\n",
    "        dist_sim = cosine_similarity(hd_dist, body_dist)\n",
    "        #build final features list\n",
    "        fts = (\n",
    "            [fst_cos_sim, avg_cos_sim, bow_cos_sim, \n",
    "                neg_path_cos_sim, hedge_path_cos_sim, doubt_path_cos_sim,\n",
    "                neg_anc_sim, hedge_anc_sim, doubt_anc_sim,\n",
    "                neg_anc_overlap, hedge_anc_overlap, doubt_anc_overlap,\n",
    "                cos_sim_s, cos_sim_v, cos_sim_o,\n",
    "                dist_sim, sent_cos_sim, sentics_cos_sim] + \n",
    "            diff_avg_sents + diff_sents + diff_avg_sentics + diff_sentics + \n",
    "            root_dist_feats + hd_dist + body_dist +\n",
    "            headline_sent + avg_body_sent + headline_sentics + avg_body_sentics\n",
    "        )\n",
    "        features.append(fts)\n",
    "        actual.append(s)\n",
    "    end = time.time()\n",
    "    print(int(end-start))\n",
    "    return features, actual\n",
    "\n",
    "\n",
    "# # Generate Features from headline/body pairs\n",
    "\n",
    "# In[96]:\n",
    "\n",
    "\n",
    "stance_data_stance = get_features_stance(test_stances, 5)\n",
    "stance_dict_stance = {}\n",
    "for idx, d in enumerate(list(test_stances.values)):\n",
    "    h, b, s = d\n",
    "    stance_dict_stance[(h, b)] = stance_data_stance[0][idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "rel_model = load('saved_models/relevance_detection_trained.joblib')\n",
    "stance_model = load('saved_models/stance_detection_trained.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = [[],[],[]]\n",
    "for h,b,s in list(test_stances.values):\n",
    "    testing_data[0].append(list(stance_dict_rel[(h,b)].values()))\n",
    "    testing_data[1].append(stance_dict_stance[(h,b)])\n",
    "    testing_data[2].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial)\n"
     ]
    }
   ],
   "source": [
    "rel_predicted = rel_model.predict(testing_data[0])\n",
    "stance_predicted = stance_model.predict(np.nan_to_num(np.array(testing_data[1]).astype(\"float32\")))\n",
    "predicted = []\n",
    "for i in range(len(rel_predicted)):\n",
    "    r = rel_predicted[i]\n",
    "    s = stance_predicted[i]\n",
    "    if r == \"unrelated\":\n",
    "        predicted.append(r)\n",
    "    else:\n",
    "        predicted.append(s)\n",
    "actual = testing_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    406    |    57     |   1200    |    240    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    68     |    80     |    358    |    191    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    343    |    168    |   3240    |    713    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    40     |    27     |    376    |   17906   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8751.0 out of 11651.25\t(75.10782104924364%)\n",
      "F1 Score\n",
      "[0.2942029  0.15549077 0.67233866 0.95756571]\n",
      "Avg Precision Score\n",
      "[0.47374562 0.24096386 0.62620796 0.93994751]\n",
      "Normalized confusion matrix\n",
      "[[0.21334735 0.02995271 0.63058329 0.12611666]\n",
      " [0.09756098 0.11477762 0.51362984 0.27403156]\n",
      " [0.07683692 0.03763441 0.72580645 0.15972222]\n",
      " [0.00217996 0.00147147 0.02049158 0.97585699]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAEYCAYAAAApuP8NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1af03ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.report_score(actual, predicted)\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(actual, predicted, average = None))\n",
    "print(\"Avg Precision Score\")\n",
    "print(precision_score(actual, predicted, average = None))\n",
    "matrix = confusion_matrix(actual,predicted)\n",
    "plot_confusion_matrix(matrix, classes=[\"agree\",\"disagree\", \"discuss\"],\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
