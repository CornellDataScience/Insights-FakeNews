{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import score as sc\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, pairwise, f1_score, precision_score\n",
    "from scipy.spatial import distance\n",
    "from preprocessing.utils import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import torch\n",
    "import importlib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "importlib.reload(sys.modules['preprocessing.utils'])\n",
    "from preprocessing.utils import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/huggingface/neuralcoref\n",
    "#note: this NEEDS spacy 2.0.12 to work! downgrade with pip install spacy=2.0.12\n",
    "import en_coref_md\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "coref = en_coref_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negating_words = set([\n",
    "    \"n't\", \"not\", \"no\", \n",
    "    \"never\", \"nobody\", \"non\", \"nope\"])\n",
    "doubting_words = set([\n",
    "    'fake','fraud', 'hoax', \n",
    "    'false', 'deny', 'denies', \n",
    "    'despite', 'doubt', \n",
    "    'bogus', 'debunk', 'prank', \n",
    "    'retract', 'scam', \"withdrawn\",\n",
    "    \"misinformation\"])\n",
    "hedging_words = set([\n",
    "    'allege', 'allegedly','apparently',\n",
    "    'appear','claim','could',\n",
    "    'evidently','largely','likely',\n",
    "    'mainly','may', 'maybe', 'might',\n",
    "    'mostly','perhaps','presumably',\n",
    "    'probably','purport', 'purportedly',\n",
    "    'reported', 'reportedly',\n",
    "    'rumor', 'rumour', 'rumored', 'rumoured',\n",
    "    'says','seem','somewhat',\n",
    "    'unconfirmed'])\n",
    "sus_words = doubting_words.union(hedging_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(sentence):\n",
    "    sent =  vader.polarity_scores(sentence.text)\n",
    "    return [sent[\"pos\"],sent[\"neg\"],sent[\"neu\"],sent[\"compound\"]]\n",
    "\n",
    "def get_avg_sentiment(lst):\n",
    "    sents = np.array([get_sentiment(s) for s in lst])\n",
    "    return list(np.mean(sents, axis = 0))\n",
    "\n",
    "def get_diff_sentiment(a,b):\n",
    "    return list(np.absolute(np.array(a) - np.array(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(bodies, stances, split=0.8):\n",
    "    idx = np.random.permutation(np.arange(len(bodies)))\n",
    "    bodies = bodies.values[idx]\n",
    "    train = int(len(bodies)*0.8)\n",
    "    bodies_tr = set([i[0] for i in bodies[:train]])\n",
    "    bodies_val = set([i[0] for i in bodies[train:]])\n",
    "    stances_tr = stances.loc[stances[\"Body ID\"].isin(bodies_tr), :]\n",
    "    stances_val = stances.loc[stances[\"Body ID\"].isin(bodies_val), :]\n",
    "    return stances_tr, stances_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49972, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>158</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
       "      <td>137</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
       "      <td>1034</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>1923</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'Nasa Confirms Earth Will Experience 6 Days of...</td>\n",
       "      <td>154</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Accused Boston Marathon Bomber Severely Injure...</td>\n",
       "      <td>962</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Identity of ISIS terrorist known as 'Jihadi Jo...</td>\n",
       "      <td>2033</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Banksy 'Arrested &amp; Real Identity Revealed' Is ...</td>\n",
       "      <td>1739</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>British Aid Worker Confirmed Murdered By ISIS</td>\n",
       "      <td>882</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance\n",
       "0  Police find mass graves with at least '15 bodi...      712  unrelated\n",
       "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree\n",
       "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated\n",
       "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated\n",
       "4  Spider burrowed through tourist's stomach and ...     1923   disagree\n",
       "5  'Nasa Confirms Earth Will Experience 6 Days of...      154      agree\n",
       "6  Accused Boston Marathon Bomber Severely Injure...      962  unrelated\n",
       "7  Identity of ISIS terrorist known as 'Jihadi Jo...     2033  unrelated\n",
       "8  Banksy 'Arrested & Real Identity Revealed' Is ...     1739      agree\n",
       "9      British Aid Worker Confirmed Murdered By ISIS      882  unrelated"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stances = pd.read_csv(\"fn_data/train_stances.csv\")\n",
    "print(train_stances.shape)\n",
    "train_stances.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1683, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Posting photos of a gun-toting child online, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody\n",
       "0        0  A small meteorite crashed into a wooded area i...\n",
       "1        4  Last week we hinted at what was to come as Ebo...\n",
       "2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
       "3        6  Posting photos of a gun-toting child online, I...\n",
       "4        7  At least 25 suspected Boko Haram insurgents we..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bodies = pd.read_csv(\"fn_data/train_bodies.csv\")\n",
    "print(train_bodies.shape)\n",
    "train_bodies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_body(n):\n",
    "    return train_bodies.loc[lambda x: x[\"Body ID\"] == n, \"articleBody\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \",' ')\n",
    "    text = text.replace(\" '\",' ')\n",
    "    text = text.replace(\":\", \". \")\n",
    "    text = text.replace(\";\", \". \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(x,y):\n",
    "    return 1 - np.nan_to_num(distance.cosine(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topics(doc):\n",
    "    \"\"\"\n",
    "    get topics of a sentence\n",
    "    input: spacy doc\n",
    "    output: dictionary with nouns as the key, and the set of noun chunks that contain the noun as the value\n",
    "    special entry _vocab has the set of all tokens in the dict\n",
    "    \"\"\"\n",
    "    subjs = {}\n",
    "    for token in doc:\n",
    "        if token.dep_ in [\"nsubj\", \"nsubjpass\", \"csubj\",\"csubjpass\", \"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\", \"compound\"]:\n",
    "            txt = token.lemma_.lower()\n",
    "            if txt not in subjs:\n",
    "                subjs[txt] = set([txt])      \n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk.root.text) > 2:\n",
    "            txt = chunk.root.text.lower()\n",
    "            if txt not in subjs:\n",
    "                subjs[txt] = set([txt])\n",
    "            subjs[txt].add(chunk.text.lower())\n",
    "    subjects_= []\n",
    "    for word in subjs:\n",
    "        for phrase in subjs[word]:\n",
    "            subjects_ += phrase.split(\" \")\n",
    "    subjs[\"_vocab\"] = set(subjects_)\n",
    "    return subjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_svos(sent):\n",
    "    \"\"\"\n",
    "    input: Spacy processed sentence\n",
    "    output: dict of subj, dict of v, dict of obj (each word is lemmatized and lowercased)\n",
    "    each entry in dict has key of lemmatized token, value is actual token (to do traversals with later if needed)\n",
    "    \"\"\"\n",
    "    s = {}\n",
    "    v = {}\n",
    "    o = {}\n",
    "    for token in sent:\n",
    "        if token.dep_ == 'ROOT':\n",
    "            v[token.lemma_.lower()] = token\n",
    "        elif token.dep_ in [\"nsubj\", \"nsubjpass\", \"csubj\",\"csubjpass\", \"agent\",\"compound\"]:\n",
    "            s[token.lemma_.lower()] = token\n",
    "        elif token.dep_ in [\"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\"]:\n",
    "            o[token.lemma_.lower()] = token\n",
    "    # https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md\n",
    "    return (s,v,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_graph(doc):\n",
    "    \"\"\"\n",
    "    build a NetworkX graph of the dependency tree\n",
    "    input: spacy Doc\n",
    "    output: networkx graph\n",
    "    \"\"\"\n",
    "    edges = set()\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.add((token.lemma_.lower(),child.lemma_.lower()))\n",
    "    graph = nx.DiGraph(list(edges))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_sentence(sent):\n",
    "    # not very robust right now\n",
    "    score = 0\n",
    "    word_count = 0\n",
    "    for token in sent:\n",
    "        word_count += 1\n",
    "        t = token.lemma_.lower()\n",
    "        if t in subjects_:\n",
    "            score += 1\n",
    "        elif t in negating_words or t in doubting_words or t in hedging_words:\n",
    "            score += 0.5\n",
    "    return score/word_count if word_count > 4 else 0\n",
    "    \n",
    "def get_summary(doc, subjects, n = 5, score_sentence = score_sentence):\n",
    "    \"\"\"\n",
    "    get summary of n sentences in document\n",
    "    first meaningful sentence will always be returned\n",
    "    \"\"\"\n",
    "    subjects_ = subjects[\"_vocab\"]\n",
    "    sentences = [s for s in doc.sents]\n",
    "    scored_sentences = [[idx, sent, score_sentence(sent)] for idx, sent in enumerate(sentences)]\n",
    "    scored_sentences = [s for s in scored_sentences if s[2] > 0] #filter out non-scoring sentences\n",
    "    scored_sentences.sort(key = lambda x: x[2], reverse = True)\n",
    "    top = scored_sentences[:n-1]\n",
    "    top.sort(key = lambda x: x[0])\n",
    "    scored_sentences.sort(key = lambda x: x[0])\n",
    "    result = [scored_sentences[0][1]] + [s[1] for s in top]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shortest_path_to_negating(graph, subjects):\n",
    "    \"\"\"\n",
    "    get the shortest path from each subject to any negating or doubting/hedging word\n",
    "    returns: dictionary with subject as key, and 2-element list of path lengths [negating, doubting]\n",
    "    - if a subject does not exist in graph or have a path to any negating word, then the value will be [None, None]\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for s in subjects:\n",
    "        results[s] = [None, None]\n",
    "        if graph.has_node(s):\n",
    "            for word in negating_words:\n",
    "                if word in graph:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(graph, source = s, target = word)\n",
    "                        if results[s][0] == None or len(path) < results[s][0]:\n",
    "                            results[s][0] = len(path)\n",
    "                    except:\n",
    "                        continue\n",
    "            for word in sus_words:\n",
    "                if word in graph:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(graph, source = s, target = word)\n",
    "                        if results[s][1] == None or len(path) < results[s][1]:\n",
    "                            results[s][1] = len(path)\n",
    "                    except:\n",
    "                        continue\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def root_distance(graph, root):\n",
    "    \"\"\"\n",
    "    as implemented in the Emergent paper - return the shortest distance between the given root and any \n",
    "    doubting or hedging words in the graph, or None if no such path exists\n",
    "    \"\"\"\n",
    "    if root == None:\n",
    "        return None\n",
    "    min_dist = None\n",
    "    for word in sus_words:\n",
    "        if word in graph:\n",
    "            try:\n",
    "                path = nx.shortest_path(graph, source = root, target = word)\n",
    "                if min_dist == None or len(path) < min_dist:\n",
    "                    min_dist = len(path)\n",
    "            except:\n",
    "                continue\n",
    "    return min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neg_ancestors(doc):\n",
    "    \"\"\"\n",
    "    get the ancestors of every negating word\n",
    "    input: spacy Doc\n",
    "    returns: tuple  - set of words that were in the ancestor list of negating words, \n",
    "    set of words that were in ancestor list of refuting words, # negating words, # refuting words\n",
    "    \"\"\"\n",
    "    results = [set(), set(), 0, 0]\n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() in negating_words:\n",
    "            results[0] = results[0].union(\n",
    "                set([ancestor.lemma_.lower() for ancestor in token.ancestors if len(ancestor) > 2])\n",
    "            )\n",
    "            results[2] += 1\n",
    "        elif token.lemma_.lower() in sus_words:\n",
    "            results[1] = results[1].union(\n",
    "                set([ancestor.lemma_.lower() for ancestor in token.ancestors if len(ancestor) > 2])\n",
    "            )\n",
    "            results[3] += 1\n",
    "    return tuple(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def build_idf(body_data):\n",
    "    idf = {}\n",
    "    for body in range(len(body_data)):\n",
    "        if body % 100 == 0:\n",
    "            print(\"Processed \"+str(body))\n",
    "        b_id, txt = tuple(body_data[body])\n",
    "        nlp_b = nlp(preprocess(txt))\n",
    "        tokens = [t for t in nlp_b if not t.is_stop and t.pos_ not in ['PUNCT','NUM','SYM','SPACE','PART']]\n",
    "        lemmatized = set([token.lemma_.lower() for token in tokens])\n",
    "        for tok in lemmatized:\n",
    "            if tok not in idf:\n",
    "                idf[tok] = 0\n",
    "            idf[tok] += 1\n",
    "    avg = float(sum(idf.values())) / len(idf)\n",
    "    for i in idf:\n",
    "        idf[i] = math.log(len(body_data)/idf[i])\n",
    "    idf[\"_avg\"] = math.log(len(body_data)/avg)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_data = list(train_stances.values)\n",
    "body_data = list(train_bodies.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0\n",
      "Processed 100\n",
      "Processed 200\n",
      "Processed 300\n",
      "Processed 400\n",
      "Processed 500\n",
      "Processed 600\n",
      "Processed 700\n",
      "Processed 800\n",
      "Processed 900\n",
      "Processed 1000\n",
      "Processed 1100\n",
      "Processed 1200\n",
      "Processed 1300\n",
      "Processed 1400\n",
      "Processed 1500\n",
      "Processed 1600\n"
     ]
    }
   ],
   "source": [
    "idf = build_idf(body_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    svo = get_svos(sentence)\n",
    "\n",
    "    # list of words that belong to that part of speech\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    tokens = []\n",
    "    for token in sentence:\n",
    "        if not token.is_stop and token.pos_ not in ['PUNCT', 'NUM', 'SYM','SPACE','PART']:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nouns.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADV\":\n",
    "                adverbs.append(token.lemma_.lower())\n",
    "            tokens.append(token.lemma_.lower())   \n",
    "    \n",
    "    bigram = list(nltk.bigrams(tokens))\n",
    "    bigram_str = [x[0]+' '+x[1] for x in bigram]\n",
    "\n",
    "    return {\n",
    "        \"raw\": sentence,\n",
    "        \"tokens\": tokens,\n",
    "        \"bigrams\": bigram_str,\n",
    "        \"nouns\": nouns,\n",
    "        \"verbs\": verbs,\n",
    "        \"adjectives\": adjectives,\n",
    "        \"adverbs\": adverbs,\n",
    "        \"svo\": [list(item) for item in svo]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_sentence_idf(sent, idf):\n",
    "    # not very robust right now\n",
    "    score = 0\n",
    "    word_count = 0\n",
    "    for token in sent:\n",
    "        word_count += 1\n",
    "        t = token.lemma_.lower()\n",
    "        if t in idf:\n",
    "            score += idf[t]\n",
    "    return score/word_count if word_count > 4 else 0\n",
    "\n",
    "def process_body(body, idf):\n",
    "    sentences = [s for s in body.sents]\n",
    "    if len(sentences) == 0:\n",
    "        sentences = [body]\n",
    "\n",
    "    # first sentence of article\n",
    "    first_sentence_data = process_sentence(sentences[0])\n",
    "\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    tokens = []\n",
    "    for token in body:\n",
    "        if not token.is_stop and token.pos_ not in ['PUNCT', 'NUM', 'SYM','SPACE','PART']:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nouns.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADV\":\n",
    "                adverbs.append(token.lemma_.lower())\n",
    "            tokens.append(token.lemma_.lower())   \n",
    "    \n",
    "    bigram = list(nltk.bigrams(tokens))\n",
    "    bigram_str = [x[0]+' '+x[1] for x in bigram]\n",
    "\n",
    "    doc_len = len(tokens)\n",
    "    n_counter = Counter(nouns)\n",
    "    v_counter = Counter(verbs)\n",
    "    b_counter = Counter(bigram)\n",
    "    t_counter = Counter(tokens)\n",
    "\n",
    "    avg_idf = idf[\"_avg\"]\n",
    "    n_tfidf, v_tfidf, t_tfidf = {}, {}, {}\n",
    "    for n in n_counter:\n",
    "        n_tfidf[n] = (n_counter[n]/doc_len) * \\\n",
    "            (idf[n] if n in idf else avg_idf)\n",
    "    for v in v_counter:\n",
    "        v_tfidf[v] = (v_counter[v]/doc_len) * \\\n",
    "            (idf[v] if v in idf else avg_idf)\n",
    "    for t in t_counter:\n",
    "        t_tfidf[t] = (t_counter[t]/doc_len) * \\\n",
    "            (idf[t] if t in idf else avg_idf)\n",
    "    \n",
    "    common_nouns = sorted(n_tfidf, key=n_tfidf.get, reverse=True)[:5]\n",
    "    common_verbs = sorted(v_tfidf, key=v_tfidf.get, reverse=True)[:5]\n",
    "    common_tokens = sorted(t_tfidf, key=t_tfidf.get, reverse=True)[:5]\n",
    "\n",
    "    # no idf for bigrams increase \"common\" count to 10\n",
    "    common_bigrams = [x[0] for x in b_counter.most_common(10)]\n",
    "    \n",
    "    scored_sentences = [[idx, sent, score_sentence_idf(sent, idf)] for idx, sent in enumerate(sentences)]\n",
    "    scored_sentences = [s for s in scored_sentences] #filter out non-scoring sentences\n",
    "    scored_sentences.sort(key = lambda x: x[2], reverse = True)\n",
    "    most_significant_sentence_data = process_sentence(scored_sentences[0][1])\n",
    "\n",
    "    return {\n",
    "        \"raw\" : body.text,\n",
    "        \"tokens\": tokens,\n",
    "        \"bigrams\": bigram_str,\n",
    "        \"nouns\": nouns,\n",
    "        \"verbs\": verbs,\n",
    "        \"first_sentence\": first_sentence_data,\n",
    "        \"significant_sentence\": most_significant_sentence_data,\n",
    "        \"vocabulary\": list(set(tokens)),\n",
    "        \"common_tokens\": common_tokens,\n",
    "        \"common_nouns\": common_nouns,\n",
    "        \"common_verbs\": common_verbs,\n",
    "        \"common_bigrams\": common_bigrams,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0\n",
      "Processed 100\n",
      "Processed 200\n",
      "Processed 300\n",
      "Processed 400\n",
      "Processed 500\n",
      "Processed 600\n",
      "Processed 700\n",
      "Processed 800\n",
      "Processed 900\n",
      "Processed 1000\n",
      "Processed 1100\n",
      "Processed 1200\n",
      "Processed 1300\n",
      "Processed 1400\n",
      "Processed 1500\n",
      "Processed 1600\n",
      "Done!\n",
      "Processed 0\n",
      "Processed 2500\n",
      "Processed 5000\n",
      "Processed 7500\n",
      "Processed 10000\n",
      "Processed 12500\n",
      "Processed 15000\n",
      "Processed 17500\n",
      "Processed 20000\n",
      "Processed 22500\n",
      "Processed 25000\n",
      "Processed 27500\n",
      "Processed 30000\n",
      "Processed 32500\n",
      "Processed 35000\n",
      "Processed 37500\n",
      "Processed 40000\n",
      "Processed 42500\n",
      "Processed 45000\n",
      "Processed 47500\n",
      "Done!\n",
      "661\n"
     ]
    }
   ],
   "source": [
    "headline_info = {}\n",
    "body_info = {}\n",
    "start = time.time()\n",
    "for body in range(len(body_data)):\n",
    "    if body % 100 == 0:\n",
    "        print(\"Processed \"+str(body))\n",
    "    b_id, txt = tuple(body_data[body])\n",
    "    nlp_a = coref(preprocess(txt))\n",
    "    nlp_b = nlp(nlp_a._.coref_resolved.lower())\n",
    "    body_processed = process_body(nlp_b, idf)\n",
    "    body_info[b_id] = body_processed\n",
    "print(\"Done!\")\n",
    "for headline in range(len(stance_data)):\n",
    "    if headline % 2500 == 0:\n",
    "        print(\"Processed \"+str(headline))\n",
    "    h, b_id, s = tuple(stance_data[headline])\n",
    "    if h not in headline_info:\n",
    "        nlp_h = nlp(preprocess(h))\n",
    "        headline_processed = process_sentence(nlp_h)\n",
    "        headline_info[h] = headline_processed\n",
    "print(\"Done!\")\n",
    "end = time.time()\n",
    "print(int(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_vec(s):\n",
    "    vecs = [token.vector for token in s]\n",
    "    return np.nan_to_num(np.sum(vecs, axis = 0))\n",
    "\n",
    "def get_features(stance_df, n_sent = 5):\n",
    "    start = time.time()\n",
    "    data = list(stance_df.values)\n",
    "    features = []\n",
    "    actual = []\n",
    "    for item in data:\n",
    "        h, b, s = tuple(item)\n",
    "        fts = get_feats(h, b)\n",
    "        features.append(fts)\n",
    "        actual.append(s)\n",
    "    end = time.time()\n",
    "    print(int(end-start))\n",
    "    return features, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_cos_similarity(a, b):\n",
    "    vocab = list(set(a).union(set(b)))\n",
    "    a_bow, b_bow = set(a), set(b)\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return -1\n",
    "    a_vec = [(1 if i in a_bow else 0) for i in vocab]\n",
    "    b_vec = [(1 if i in b_bow else 0) for i in vocab]\n",
    "    return 1 - distance.cosine(a_vec, b_vec)\n",
    "\n",
    "def get_feats(headline, body_id):\n",
    "    headline_data = headline_info[headline]\n",
    "    body_data = body_info[body_id]\n",
    "\n",
    "    shared_common_nouns = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['common_nouns'])))\n",
    "    shared_common_verbs = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['common_verbs'])))\n",
    "    shared_common_tokens = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['common_tokens'])))\n",
    "    shared_bigrams = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['common_bigrams'])))\n",
    "\n",
    "    shared_nouns_first = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['first_sentence']['nouns'])))\n",
    "    shared_verbs_first = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['first_sentence']['verbs'])))\n",
    "    shared_bigrams_first = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['first_sentence']['bigrams'])))\n",
    "    shared_tokens_first = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['first_sentence']['tokens'])))\n",
    "\n",
    "    shared_nouns_sig = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['significant_sentence']['nouns'])))\n",
    "    shared_verbs_sig = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['significant_sentence']['verbs'])))\n",
    "    shared_bigrams_sig = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['significant_sentence']['bigrams'])))\n",
    "    shared_tokens_sig = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['significant_sentence']['tokens'])))\n",
    "\n",
    "    headline_svo = headline_data['svo']\n",
    "    body_fst_svo = body_data['first_sentence']['svo']\n",
    "    body_sig_svo = body_data['significant_sentence']['svo']\n",
    "\n",
    "    # cosine similarity - no verbs because relatively few per sentence\n",
    "    cos_nouns_first = bow_cos_similarity(\n",
    "        headline_data['nouns'], body_data['first_sentence']['nouns'])\n",
    "    cos_bigrams_first = bow_cos_similarity(\n",
    "        headline_data['bigrams'], body_data['first_sentence']['bigrams'])\n",
    "    cos_tokens_first = bow_cos_similarity(\n",
    "        headline_data['tokens'], body_data['first_sentence']['tokens'])\n",
    "\n",
    "    cos_nouns_sig = bow_cos_similarity(\n",
    "        headline_data['nouns'], body_data['significant_sentence']['nouns'])\n",
    "    cos_bigrams_sig = bow_cos_similarity(\n",
    "        headline_data['bigrams'], body_data['significant_sentence']['bigrams'])\n",
    "    cos_tokens_sig = bow_cos_similarity(\n",
    "        headline_data['tokens'], body_data['significant_sentence']['tokens'])\n",
    "    \n",
    "    svo_cos_sim_fst = bow_cos_similarity(\n",
    "        body_fst_svo[0]+body_fst_svo[1]+body_fst_svo[2], \n",
    "        headline_svo[0]+headline_svo[1]+headline_svo[2])\n",
    "\n",
    "    svo_cos_sim_sig = bow_cos_similarity(\n",
    "        body_sig_svo[0]+body_sig_svo[1]+body_sig_svo[2], \n",
    "        headline_svo[0]+headline_svo[1]+headline_svo[2])\n",
    "    \n",
    "    return list({\n",
    "        'shared_nouns': shared_common_nouns,\n",
    "        'shared_verbs': shared_common_verbs,\n",
    "        'shared_bigrams': shared_bigrams,\n",
    "        'shared_tokens': shared_common_tokens,\n",
    "\n",
    "        'shared_nouns_fst': shared_nouns_first,\n",
    "        'shared_verbs_fst': shared_verbs_first,\n",
    "        'shared_bigrams_fst': shared_bigrams_first,\n",
    "        'shared_tokens_fst': shared_tokens_first,\n",
    "\n",
    "        'shared_nouns_sig': shared_nouns_sig,\n",
    "        'shared_verbs_sig': shared_verbs_sig,\n",
    "        'shared_bigrams_sig': shared_bigrams_sig,\n",
    "        'shared_tokens_sig': shared_tokens_sig,\n",
    "\n",
    "        'cos_nouns_sig': cos_nouns_sig,\n",
    "        'cos_bigrams_sig': cos_bigrams_sig,\n",
    "        'cos_tokens_sig': cos_tokens_sig,\n",
    "\n",
    "        'cos_nouns_fst': cos_nouns_first,\n",
    "        'cos_bigrams_fst': cos_bigrams_first,\n",
    "        'cos_tokens_fst': cos_tokens_first,\n",
    "\n",
    "        'svo_cos_sim_fst' : svo_cos_sim_fst,\n",
    "        'svo_cos_sim_sig' : svo_cos_sim_sig,\n",
    "    }.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "stance_data = get_features(train_stances, 5)\n",
    "stance_dict = {}\n",
    "for idx, d in enumerate(list(train_stances.values)):\n",
    "    h, b, s = d\n",
    "    stance_dict[(h, b)] = stance_data[0][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_to_int(labels):\n",
    "    return [(1 if l == \"agree\" else (0 if l == \"discuss\" else -1)) for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29983724851076954, 0.5771482666970632, 4.096128684015602)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf['say'], idf['report'], idf['spider']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'unrelated': 28786, 'discuss': 10886}) Counter({'unrelated': 7759, 'discuss': 2541})\n",
      "Training Baseline 72.56% Testing Baseline 75.33%\n",
      "94.18% training accuracy\n",
      "96.39% validation accuracy\n",
      "Baseline comparison: TR 21.62% VAL 21.06%\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |     0     |     0     |     0     |     0     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     0     |     0     |     0     |     0     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |     0     |     0     |   2340    |    201    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     0     |     0     |    171    |   7588    |\n",
      "-------------------------------------------------------------\n",
      "Score: 4237.0 out of 4480.75\t(94.56006248953858%)\n",
      "F1 Score\n",
      "[0.9263658  0.97607409]\n",
      "Avg Precision Score\n",
      "[0.93189964 0.97419438]\n"
     ]
    }
   ],
   "source": [
    "stances_tr, stances_val = train_test_split(train_bodies, train_stances)\n",
    "\n",
    "training_data = [[],[]]\n",
    "for h,b,s in list(stances_tr.values):\n",
    "    training_data[0].append(stance_dict[(h,b)])\n",
    "    training_data[1].append(s if s == \"unrelated\" else \"discuss\")\n",
    "\n",
    "testing_data = [[],[]]\n",
    "for h,b,s in list(stances_val.values):\n",
    "    testing_data[0].append(stance_dict[(h,b)])\n",
    "    testing_data[1].append(s if s == \"unrelated\" else \"discuss\")\n",
    "\n",
    "c1, c2 = Counter(training_data[1]), Counter(testing_data[1])\n",
    "baseline_tr = max(c1.values())/sum(c1.values())\n",
    "baseline_val = max(c2.values())/sum(c2.values())\n",
    "print(c1, c2)\n",
    "print(\"Training Baseline {0:.2f}% Testing Baseline {1:.2f}%\".format(baseline_tr * 100, baseline_val * 100))\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 500, min_samples_split = 10, min_samples_leaf = 5, max_depth = 6)\n",
    "# model = LogisticRegression(max_iter = 200)\n",
    "# model = SVC()\n",
    "# model = GradientBoostingClassifier(n_estimators = 300, subsample = 0.1, learning_rate = 0.025, random_state=0)\n",
    "\n",
    "model.fit(training_data[0], training_data[1])\n",
    "tr_acc = model.score(training_data[0], training_data[1])\n",
    "print('{0:.2f}% training accuracy'.format(tr_acc*100))\n",
    "\n",
    "val_acc = model.score(testing_data[0], testing_data[1])\n",
    "print('{0:.2f}% validation accuracy'.format(val_acc*100))\n",
    "print(\"Baseline comparison: TR {0:.2f}% VAL {1:.2f}%\".format((tr_acc-baseline_tr)*100,(val_acc-baseline_val)*100))\n",
    "\n",
    "actual = testing_data[1]\n",
    "predicted = model.predict(testing_data[0])\n",
    "sc.report_score(actual, predicted)\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(actual, predicted, average = None))\n",
    "print(\"Avg Precision Score\")\n",
    "print(precision_score(actual, predicted, average = None))\n",
    "matrix = confusion_matrix(actual,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
