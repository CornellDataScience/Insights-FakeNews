{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannyyang/Library/Python/3.6/lib/python/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import score as sc\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, pairwise, f1_score, precision_score\n",
    "from scipy.spatial import distance\n",
    "from preprocessing.utils import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import torch\n",
    "import importlib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "importlib.reload(sys.modules['preprocessing.utils'])\n",
    "from preprocessing.utils import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/huggingface/neuralcoref\n",
    "#note: this NEEDS spacy 2.0.12 to work! downgrade with pip install spacy=2.0.12\n",
    "import en_coref_md\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "coref = en_coref_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negating_words = set([\n",
    "    \"n't\", \"not\", \"no\", \n",
    "    \"never\", \"nobody\", \"non\", \"nope\"])\n",
    "doubting_words = set([\n",
    "    'fake','fraud', 'hoax', \n",
    "    'false', 'deny', 'denies', \n",
    "    'despite', 'doubt', \n",
    "    'bogus', 'debunk', 'prank', \n",
    "    'retract', 'scam', \"withdrawn\",\n",
    "    \"misinformation\"])\n",
    "hedging_words = set([\n",
    "    'allege', 'allegedly','apparently',\n",
    "    'appear','claim','could',\n",
    "    'evidently','largely','likely',\n",
    "    'mainly','may', 'maybe', 'might',\n",
    "    'mostly','perhaps','presumably',\n",
    "    'probably','purport', 'purportedly',\n",
    "    'reported', 'reportedly',\n",
    "    'rumor', 'rumour', 'rumored', 'rumoured',\n",
    "    'says','seem','somewhat',\n",
    "    'unconfirmed'])\n",
    "sus_words = doubting_words.union(hedging_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(sentence):\n",
    "    sent =  vader.polarity_scores(sentence.text)\n",
    "    return [sent[\"pos\"],sent[\"neg\"],sent[\"neu\"],sent[\"compound\"]]\n",
    "\n",
    "def get_avg_sentiment(lst):\n",
    "    sents = np.array([get_sentiment(s) for s in lst])\n",
    "    return list(np.mean(sents, axis = 0))\n",
    "\n",
    "def get_diff_sentiment(a,b):\n",
    "    return list(np.array(a) - np.array(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(bodies, stances, split=0.8):\n",
    "    idx = np.random.permutation(np.arange(len(bodies)))\n",
    "    bodies = bodies.values[idx]\n",
    "    train = int(len(bodies)*0.8)\n",
    "    bodies_tr = set([i[0] for i in bodies[:train]])\n",
    "    bodies_val = set([i[0] for i in bodies[train:]])\n",
    "    stances_tr = stances.loc[stances[\"Body ID\"].isin(bodies_tr), :]\n",
    "    stances_val = stances.loc[stances[\"Body ID\"].isin(bodies_val), :]\n",
    "    return stances_tr, stances_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49972, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>158</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
       "      <td>137</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
       "      <td>1034</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>1923</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'Nasa Confirms Earth Will Experience 6 Days of...</td>\n",
       "      <td>154</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Accused Boston Marathon Bomber Severely Injure...</td>\n",
       "      <td>962</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Identity of ISIS terrorist known as 'Jihadi Jo...</td>\n",
       "      <td>2033</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Banksy 'Arrested &amp; Real Identity Revealed' Is ...</td>\n",
       "      <td>1739</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>British Aid Worker Confirmed Murdered By ISIS</td>\n",
       "      <td>882</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance\n",
       "0  Police find mass graves with at least '15 bodi...      712  unrelated\n",
       "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree\n",
       "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated\n",
       "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated\n",
       "4  Spider burrowed through tourist's stomach and ...     1923   disagree\n",
       "5  'Nasa Confirms Earth Will Experience 6 Days of...      154      agree\n",
       "6  Accused Boston Marathon Bomber Severely Injure...      962  unrelated\n",
       "7  Identity of ISIS terrorist known as 'Jihadi Jo...     2033  unrelated\n",
       "8  Banksy 'Arrested & Real Identity Revealed' Is ...     1739      agree\n",
       "9      British Aid Worker Confirmed Murdered By ISIS      882  unrelated"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stances = pd.read_csv(\"fn_data/train_stances.csv\")\n",
    "print(train_stances.shape)\n",
    "train_stances.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1683, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Posting photos of a gun-toting child online, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody\n",
       "0        0  A small meteorite crashed into a wooded area i...\n",
       "1        4  Last week we hinted at what was to come as Ebo...\n",
       "2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
       "3        6  Posting photos of a gun-toting child online, I...\n",
       "4        7  At least 25 suspected Boko Haram insurgents we..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bodies = pd.read_csv(\"fn_data/train_bodies.csv\")\n",
    "print(train_bodies.shape)\n",
    "train_bodies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_body(n):\n",
    "    return train_bodies.loc[lambda x: x[\"Body ID\"] == n, \"articleBody\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \",' ')\n",
    "    text = text.replace(\"'\\n\",'\\n')\n",
    "    text = text.replace(\" '\",' ')\n",
    "    text = text.replace('\"',' ')\n",
    "    text = text.replace('“',' ')\n",
    "    text = text.replace('”', ' ')\n",
    "    text = text.replace(\":\", \". \")\n",
    "    text = text.replace(\";\", \". \")\n",
    "    text = text.replace(\"...\", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(x,y):\n",
    "    if all([a == 0 for a in x]) or all([a == 0 for a in y]):\n",
    "        return 0\n",
    "    return 1 - np.nan_to_num(distance.cosine(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topics(doc):\n",
    "    \"\"\"\n",
    "    get topics of a sentence\n",
    "    input: spacy doc\n",
    "    output: dictionary with nouns as the key, and the set of noun chunks that contain the noun as the value\n",
    "    special entry _vocab has the set of all tokens in the dict\n",
    "    \"\"\"\n",
    "    subjs = {}\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk.root.text) > 2 and chunk.root.pos_ not in [\"NUM\", \"SYM\",\"PUNCT\"]:\n",
    "            txt = chunk.root.lemma_.lower()\n",
    "            if txt not in subjs:\n",
    "                subjs[txt] = set([txt])\n",
    "            subjs[txt].add(chunk.text.lower())\n",
    "    subjects_= []\n",
    "    for word in subjs:\n",
    "        for phrase in subjs[word]:\n",
    "            subjects_ += phrase.split(\" \")\n",
    "    subjs[\"_vocab\"] = set(subjects_)\n",
    "    return subjss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_svos(sent):\n",
    "    \"\"\"\n",
    "    input: Spacy processed sentence\n",
    "    output: dict of subj, dict of v, dict of obj (each word is lemmatized and lowercased)\n",
    "    each entry in dict has key of lemmatized token, value is actual token (to do traversals with later if needed)\n",
    "    \"\"\"\n",
    "    s = {}\n",
    "    v = {}\n",
    "    o = {}\n",
    "    for token in sent:\n",
    "        if token.dep_ == 'ROOT':\n",
    "            v[token.lemma_.lower()] = token\n",
    "        elif token.dep_ in [\"nsubj\", \"nsubjpass\", \"csubj\",\"csubjpass\", \"agent\",\"compound\"]:\n",
    "            s[token.lemma_.lower()] = token\n",
    "        elif token.dep_ in [\"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\"]:\n",
    "            o[token.lemma_.lower()] = token\n",
    "    # https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md\n",
    "    return (s,v,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_graph(doc):\n",
    "    \"\"\"\n",
    "    build a NetworkX graph of the dependency tree\n",
    "    input: spacy Doc\n",
    "    output: networkx graph\n",
    "    \"\"\"\n",
    "    edges = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE']:\n",
    "            for child in token.children:\n",
    "                if child.pos_ not in ['SPACE']:\n",
    "                    edges.add((token.lemma_.lower(),child.lemma_.lower()))\n",
    "    graph = nx.DiGraph(list(edges))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_edges(doc):\n",
    "    \"\"\"\n",
    "    return list of edges\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT', 'SYM']:\n",
    "            for child in token.children:\n",
    "                if child.pos_ not in ['SPACE', 'PUNCT', 'SYM']:\n",
    "                    edges.append((\n",
    "                        {\"token\":token.lemma_.lower(), \"dep\":token.dep_ , \"pos\":token.pos_},\n",
    "                        {\"token\":child.lemma_.lower(), \"dep\":child.dep_ , \"pos\":child.pos_}\n",
    "                    ))\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summary(doc, subjects, n = 5):\n",
    "    \"\"\"\n",
    "    get summary of n sentences in document\n",
    "    first meaningful sentence will always be returned\n",
    "    \"\"\"\n",
    "    subjects_ = subjects\n",
    "    def score_sentence(sent):\n",
    "        # not very robust right now\n",
    "        score = 0\n",
    "        word_count = 0\n",
    "        for token in sent:\n",
    "            word_count += 1\n",
    "            t = token.lemma_.lower()\n",
    "            if t in subjects_:\n",
    "                score += 1\n",
    "            elif t in negating_words or t in doubting_words or t in hedging_words:\n",
    "                score += 1\n",
    "        return score/word_count if word_count > 4 else 0\n",
    "    sentences = [s for s in doc.sents]\n",
    "    scored_sentences = [[idx, sent, score_sentence(sent)] for idx, sent in enumerate(sentences)]\n",
    "    scored_sentences = [s for s in scored_sentences if s[2] > 0 and s[0] > 0] #filter out non-scoring sentences\n",
    "    scored_sentences.sort(key = lambda x: x[2], reverse = True)\n",
    "    top = scored_sentences[:n]\n",
    "    top.sort(key = lambda x: x[0])\n",
    "    scored_sentences.sort(key = lambda x: x[0])\n",
    "    result = None\n",
    "    if len(scored_sentences) == 0:\n",
    "        result = [sentences[0]]\n",
    "    else:\n",
    "        result = [scored_sentences[0][1]] + [s[1] for s in top]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shortest_path_to_negating(graph, subjects):\n",
    "    \"\"\"\n",
    "    get the shortest path from each subject to any negating or doubting/hedging word\n",
    "    returns: dictionary with subject as key, and 2-element list of path lengths [negating, doubting]\n",
    "    - if a subject does not exist in graph or have a path to any negating word, then the value will be [None, None]\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for s in subjects:\n",
    "        results[s] = [None, None, None]\n",
    "        if graph.has_node(s):\n",
    "            for word in negating_words:\n",
    "                if word in graph:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(graph, source = s, target = word)\n",
    "                        if results[s][0] == None or len(path) < results[s][0]:\n",
    "                            results[s][0] = len(path)\n",
    "                    except:\n",
    "                        continue\n",
    "            for word in hedging_words:\n",
    "                if word in graph:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(graph, source = s, target = word)\n",
    "                        if results[s][1] == None or len(path) < results[s][1]:\n",
    "                            results[s][1] = len(path)\n",
    "                    except:\n",
    "                        continue\n",
    "            for word in doubting_words:\n",
    "                if word in graph:\n",
    "                    try:\n",
    "                        path = nx.shortest_path(graph, source = s, target = word)\n",
    "                        if results[s][2] == None or len(path) < results[s][2]:\n",
    "                            results[s][2] = len(path)\n",
    "                    except:\n",
    "                        continue\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def root_distance(graph, root):\n",
    "    \"\"\"\n",
    "    as implemented in the Emergent paper - return the shortest distance between the given root and any \n",
    "    doubting or hedging words in the graph, or None if no such path exists\n",
    "    \"\"\"\n",
    "    if root == None:\n",
    "        return None\n",
    "    min_dist = None\n",
    "    for word in sus_words:\n",
    "        if word in graph:\n",
    "            try:\n",
    "                path = nx.shortest_path(graph, source = root, target = word)\n",
    "                if min_dist == None or len(path) < min_dist:\n",
    "                    min_dist = len(path)\n",
    "            except:\n",
    "                continue\n",
    "    return min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neg_ancestors(doc):\n",
    "    \"\"\"\n",
    "    get the ancestors of every negating word\n",
    "    input: spacy Doc\n",
    "    returns: tuple  - set of words that were in the ancestor list of negating words, \n",
    "    set of words that were in ancestor list of refuting words, # negating words, # refuting words\n",
    "    \"\"\"\n",
    "    results = [set(), set(), set(), 0, 0, 0]\n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() in negating_words:\n",
    "            results[0] = results[0].union(\n",
    "                set([ancestor.lemma_.lower() for ancestor in token.ancestors if len(ancestor) > 2]).union(\n",
    "                    set([child.lemma_.lower() for child in token.head.children if child.text != token.text and len(child) > 2])\n",
    "                )\n",
    "            )\n",
    "            results[3] += 1\n",
    "        elif token.lemma_.lower() in doubting_words:\n",
    "            results[1] = results[1].union(\n",
    "                set([ancestor.lemma_.lower() for ancestor in token.ancestors if len(ancestor) > 2]).union(\n",
    "                    set([child.lemma_.lower() for child in token.head.children if child.text != token.text and len(child) > 2])\n",
    "                )\n",
    "            )\n",
    "            results[4] += 1\n",
    "        elif token.lemma_.lower() in hedging_words:\n",
    "            results[2] = results[1].union(\n",
    "                set([ancestor.lemma_.lower() for ancestor in token.ancestors if len(ancestor) > 2]).union(\n",
    "                    set([child.lemma_.lower() for child in token.head.children if child.text != token.text and len(child) > 2])\n",
    "                )\n",
    "            )\n",
    "            results[5] += 1\n",
    "    return tuple(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def build_idf(body_data):\n",
    "    idf = {}\n",
    "    for body in range(len(body_data)):\n",
    "        if body % 100 == 0:\n",
    "            print(\"Processed \"+str(body))\n",
    "        b_id, txt = tuple(body_data[body])\n",
    "        nlp_b = nlp(preprocess(txt))\n",
    "        tokens = [t for t in nlp_b if not t.is_stop and t.pos_ not in ['PUNCT','NUM','SYM','SPACE','PART']]\n",
    "        lemmatized = set([token.lemma_.lower() for token in tokens])\n",
    "        for tok in lemmatized:\n",
    "            if tok not in idf:\n",
    "                idf[tok] = 0\n",
    "            idf[tok] += 1\n",
    "    avg = float(sum(idf.values())) / len(idf)\n",
    "    for i in idf:\n",
    "        idf[i] = math.log(len(body_data)/idf[i])\n",
    "    idf[\"_avg\"] = math.log(len(body_data)/avg)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stance_data = list(train_stances.values)\n",
    "body_data = list(train_bodies.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0\n",
      "Processed 100\n",
      "Processed 200\n",
      "Processed 300\n",
      "Processed 400\n",
      "Processed 500\n",
      "Processed 600\n",
      "Processed 700\n",
      "Processed 800\n",
      "Processed 900\n",
      "Processed 1000\n",
      "Processed 1100\n",
      "Processed 1200\n",
      "Processed 1300\n",
      "Processed 1400\n",
      "Processed 1500\n",
      "Processed 1600\n"
     ]
    }
   ],
   "source": [
    "idf = build_idf(body_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('saved_data/idf.json', 'w') as fp:\n",
    "    json.dump(idf, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    svo = get_svos(sentence)\n",
    "\n",
    "    # list of words that belong to that part of speech\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    tokens = []\n",
    "    for token in sentence:\n",
    "        if not token.is_stop and token.pos_ not in ['PUNCT', 'NUM', 'SYM','SPACE','PART']:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nouns.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADV\":\n",
    "                adverbs.append(token.lemma_.lower())\n",
    "            tokens.append(token.lemma_.lower())   \n",
    "    \n",
    "    bigram = list(nltk.bigrams(tokens))\n",
    "    bigram_str = [x[0]+' '+x[1] for x in bigram]\n",
    "\n",
    "    return {\n",
    "        \"raw\": sentence.text,\n",
    "        \"tokens\": tokens,\n",
    "        \"bigrams\": bigram_str,\n",
    "        \"nouns\": nouns,\n",
    "        \"verbs\": verbs,\n",
    "        \"adjectives\": adjectives,\n",
    "        \"adverbs\": adverbs,\n",
    "        \"svo\": [list(item) for item in svo]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_sentence_idf(sent, idf):\n",
    "    # not very robust right now\n",
    "    score = 0\n",
    "    word_count = 0\n",
    "    for token in sent:\n",
    "        word_count += 1\n",
    "        t = token.lemma_.lower()\n",
    "        if t in idf:\n",
    "            score += idf[t]\n",
    "    return score/word_count if word_count > 4 else 0\n",
    "\n",
    "def process_body(body, idf):\n",
    "    sentences = [s for s in body.sents]\n",
    "    if len(sentences) == 0:\n",
    "        sentences = [body]\n",
    "\n",
    "    # first sentence of article\n",
    "    first_sentence_data = process_sentence(sentences[0])\n",
    "\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    tokens = []\n",
    "    for token in body:\n",
    "        if not token.is_stop and token.pos_ not in ['PUNCT', 'NUM', 'SYM','SPACE','PART']:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nouns.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_.lower())\n",
    "            elif token.pos_ == \"ADV\":\n",
    "                adverbs.append(token.lemma_.lower())\n",
    "            tokens.append(token.lemma_.lower())   \n",
    "    \n",
    "    bigram = list(nltk.bigrams(tokens))\n",
    "    bigram_str = [x[0]+' '+x[1] for x in bigram]\n",
    "\n",
    "    doc_len = len(tokens)\n",
    "    n_counter = Counter(nouns)\n",
    "    v_counter = Counter(verbs)\n",
    "    b_counter = Counter(bigram)\n",
    "    t_counter = Counter(tokens)\n",
    "\n",
    "    avg_idf = idf[\"_avg\"]\n",
    "    n_tfidf, v_tfidf, t_tfidf = {}, {}, {}\n",
    "    for n in n_counter:\n",
    "        n_tfidf[n] = (n_counter[n]/doc_len) * \\\n",
    "            (idf[n] if n in idf else avg_idf)\n",
    "    for v in v_counter:\n",
    "        v_tfidf[v] = (v_counter[v]/doc_len) * \\\n",
    "            (idf[v] if v in idf else avg_idf)\n",
    "    for t in t_counter:\n",
    "        t_tfidf[t] = (t_counter[t]/doc_len) * \\\n",
    "            (idf[t] if t in idf else avg_idf)\n",
    "    \n",
    "    common_nouns = sorted(n_tfidf, key=n_tfidf.get, reverse=True)[:5]\n",
    "    common_verbs = sorted(v_tfidf, key=v_tfidf.get, reverse=True)[:5]\n",
    "    common_tokens = sorted(t_tfidf, key=t_tfidf.get, reverse=True)[:5]\n",
    "\n",
    "    # no idf for bigrams increase \"common\" count to 10\n",
    "    common_bigrams = [x[0] for x in b_counter.most_common(10)]\n",
    "    \n",
    "    scored_sentences = [[idx, sent, score_sentence_idf(sent, idf)] for idx, sent in enumerate(sentences)]\n",
    "    scored_sentences = [s for s in scored_sentences] #filter out non-scoring sentences\n",
    "    scored_sentences.sort(key = lambda x: x[2], reverse = True)\n",
    "    most_significant_sentence_data = process_sentence(scored_sentences[0][1])\n",
    "\n",
    "    return {\n",
    "        \"raw\" : body.text,\n",
    "        \"tokens\": tokens,\n",
    "        \"bigrams\": bigram_str,\n",
    "        \"nouns\": nouns,\n",
    "        \"verbs\": verbs,\n",
    "        \"first_sentence\": first_sentence_data,\n",
    "        \"significant_sentence\": most_significant_sentence_data,\n",
    "        \"vocabulary\": list(set(tokens)),\n",
    "        \"common_tokens\": common_tokens,\n",
    "        \"common_nouns\": common_nouns,\n",
    "        \"common_verbs\": common_verbs,\n",
    "        \"common_bigrams\": common_bigrams,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0\n",
      "Processed 2500\n",
      "Processed 5000\n",
      "Processed 7500\n",
      "Processed 10000\n",
      "Processed 12500\n",
      "Processed 15000\n",
      "Processed 17500\n",
      "Processed 20000\n",
      "Processed 22500\n",
      "Processed 25000\n",
      "Processed 27500\n",
      "Processed 30000\n",
      "Processed 32500\n",
      "Processed 35000\n",
      "Processed 37500\n",
      "Processed 40000\n",
      "Processed 42500\n",
      "Processed 45000\n",
      "Processed 47500\n",
      "Done!\n",
      "Processed 0\n",
      "Processed 100\n",
      "Processed 200\n",
      "Processed 300\n",
      "Processed 400\n",
      "Processed 500\n",
      "Processed 600\n",
      "Processed 700\n",
      "Processed 800\n",
      "Processed 900\n",
      "Processed 1000\n",
      "Processed 1100\n",
      "Processed 1200\n",
      "Processed 1300\n",
      "Processed 1400\n",
      "Processed 1500\n",
      "Processed 1600\n",
      "Done!\n",
      "673\n"
     ]
    }
   ],
   "source": [
    "headline_info = {}\n",
    "body_info = {}\n",
    "start = time.time()\n",
    "for headline in range(len(stance_data)):\n",
    "    if headline % 2500 == 0:\n",
    "        print(\"Processed \"+str(headline))\n",
    "    h, b_id, s = tuple(stance_data[headline])\n",
    "    if h not in headline_info:\n",
    "        nlp_h = nlp(preprocess(h))\n",
    "        headline_processed = process_sentence(nlp_h)\n",
    "        headline_info[h] = headline_processed\n",
    "print(\"Done!\")\n",
    "for body in range(len(body_data)):\n",
    "    if body % 100 == 0:\n",
    "        print(\"Processed \"+str(body))\n",
    "    b_id, txt = tuple(body_data[body])\n",
    "    nlp_a = coref(preprocess(txt))\n",
    "    nlp_b = nlp(nlp_a._.coref_resolved.lower())\n",
    "    body_processed = process_body(nlp_b, idf)\n",
    "    body_info[b_id] = body_processed\n",
    "print(\"Done!\")\n",
    "end = time.time()\n",
    "print(int(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('saved_data/relevance_headline_info.json', 'w') as fp:\n",
    "    json.dump(headline_info, fp)\n",
    "    \n",
    "json_body_info = {}\n",
    "for k in body_info:\n",
    "    body_info[k]['vocabulary'] = list(body_info[k]['vocabulary'])\n",
    "    json_body_info[str(k)] = body_info[k]\n",
    "with open('saved_data/relevance_body_info.json', 'w') as fp:\n",
    "    json.dump(json_body_info, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw': \"a small meteorite crashed into a wooded area in nicaragua's capital of managua overnight, nicaragua said sunday. residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the associated press reports. \\n\\ngovernment spokeswoman rosario murillo said a committee formed by nicaragua to study the event determined a committee formed by the government to study the event was a  relatively small  meteorite that  appears to have come off an asteroid that was passing close to earth.  house-sized asteroid 2014 rc, which measured 60 feet in diameter, skimmed earth this weekend, abc news reports. \\ngovernment spokeswoman rosario murillo said nicaragua will ask international experts to help local scientists in understanding what happened.\\n\\nthe crater left by a  relatively small  meteorite that  appears to have come off an asteroid that was passing close to earth had a radius of 39 feet and a depth of 16 feet,  said humberto saballos, a volcanologist with the nicaraguan institute of territorial studies who was on the committee. humberto saballos, a volcanologist with the nicaraguan institute of territorial studies who was on the committee said it is still not clear if a  relatively small  meteorite that  appears to have come off an asteroid that was passing close to earth disintegrated or was buried.\\n\\nhumberto garcia, of the astronomy center at the national autonomous university of nicaragua, said a  relatively small  meteorite that  appears to have come off an asteroid that was passing close to earth could be related to an asteroid that was forecast to pass by the planet saturday night.\\n\\n we have to study a  relatively small  meteorite that  appears to have come off an asteroid that was passing close to earth more because a  relatively small  meteorite that  appears to have come off an asteroid that was passing close to earth could be ice or rock,  \\n\\n  said.\\n\\nwilfried strauch, an adviser to the institute of territorial studies, said it was  very strange that no one reported a streak of light. we have to ask if anyone has a photo or something. \\n\\nlocal residents reported hearing a loud boom saturday night, but said local residents didn't see anything strange in the sky.\\n\\n i was sitting on my porch and i saw nothing, then all of a sudden i heard a large blast. we thought it was a bomb because we felt an expansive wave,  jorge santamaria told the associated press.\\n\\nthe site of the crater is near managua's international airport and an air force base. only journalists from state media were allowed to visit the site of the crater.\", 'tokens': ['small', 'meteorite', 'crash', 'wooded', 'area', 'nicaragua', 'capital', 'managua', 'overnight', 'nicaragua', 'say', 'sunday', 'resident', 'report', 'hear', 'mysterious', 'boom', 'leave', 'deep', 'crater', 'near', 'city', 'airport', 'associate', 'press', 'report', 'government', 'spokeswoman', 'rosario', 'murillo', 'say', 'committee', 'form', 'nicaragua', 'study', 'event', 'determine', 'committee', 'form', 'government', 'study', 'event', 'relatively', 'small', 'meteorite', 'appear', 'come', 'asteroid', 'pass', 'close', 'earth', 'house', 'sized', 'asteroid', 'rc', 'measure', 'foot', 'diameter', 'skim', 'earth', 'weekend', 'abc', 'news', 'report', 'government', 'spokeswoman', 'rosario', 'murillo', 'say', 'nicaragua', 'ask', 'international', 'expert', 'help', 'local', 'scientist', 'understand', 'happen', 'crater', 'leave', 'relatively', 'small', 'meteorite', 'appear', 'come', 'asteroid', 'pass', 'close', 'earth', 'radius', 'foot', 'depth', 'foot', 'say', 'humberto', 'saballo', 'volcanologist', 'nicaraguan', 'institute', 'territorial', 'study', 'committee', 'humberto', 'saballo', 'volcanologist', 'nicaraguan', 'institute', 'territorial', 'study', 'committee', 'say', 'clear', 'relatively', 'small', 'meteorite', 'appear', 'come', 'asteroid', 'pass', 'close', 'earth', 'disintegrate', 'bury', 'humberto', 'garcia', 'astronomy', 'center', 'national', 'autonomous', 'university', 'nicaragua', 'say', 'relatively', 'small', 'meteorite', 'appear', 'come', 'asteroid', 'pass', 'close', 'earth', 'relate', 'asteroid', 'forecast', 'pass', 'planet', 'saturday', 'night', 'study', 'relatively', 'small', 'meteorite', 'appear', 'come', 'asteroid', 'pass', 'close', 'earth', 'relatively', 'small', 'meteorite', 'appear', 'come', 'asteroid', 'pass', 'close', 'earth', 'ice', 'rock', 'say', 'wilfri', 'strauch', 'adviser', 'institute', 'territorial', 'study', 'say', 'strange', 'report', 'streak', 'light', 'ask', 'photo', 'local', 'resident', 'report', 'hear', 'loud', 'boom', 'saturday', 'night', 'say', 'local', 'resident', 'not', 'strange', 'sky', 'sit', 'porch', 'see', 'sudden', 'hear', 'large', 'blast', 'think', 'bomb', 'feel', 'expansive', 'wave', 'jorge', 'santamaria', 'tell', 'associate', 'press', 'site', 'crater', 'near', 'managua', 'international', 'airport', 'air', 'force', 'base', 'journalist', 'state', 'medium', 'allow', 'visit', 'site', 'crater'], 'bigrams': ['small meteorite', 'meteorite crash', 'crash wooded', 'wooded area', 'area nicaragua', 'nicaragua capital', 'capital managua', 'managua overnight', 'overnight nicaragua', 'nicaragua say', 'say sunday', 'sunday resident', 'resident report', 'report hear', 'hear mysterious', 'mysterious boom', 'boom leave', 'leave deep', 'deep crater', 'crater near', 'near city', 'city airport', 'airport associate', 'associate press', 'press report', 'report government', 'government spokeswoman', 'spokeswoman rosario', 'rosario murillo', 'murillo say', 'say committee', 'committee form', 'form nicaragua', 'nicaragua study', 'study event', 'event determine', 'determine committee', 'committee form', 'form government', 'government study', 'study event', 'event relatively', 'relatively small', 'small meteorite', 'meteorite appear', 'appear come', 'come asteroid', 'asteroid pass', 'pass close', 'close earth', 'earth house', 'house sized', 'sized asteroid', 'asteroid rc', 'rc measure', 'measure foot', 'foot diameter', 'diameter skim', 'skim earth', 'earth weekend', 'weekend abc', 'abc news', 'news report', 'report government', 'government spokeswoman', 'spokeswoman rosario', 'rosario murillo', 'murillo say', 'say nicaragua', 'nicaragua ask', 'ask international', 'international expert', 'expert help', 'help local', 'local scientist', 'scientist understand', 'understand happen', 'happen crater', 'crater leave', 'leave relatively', 'relatively small', 'small meteorite', 'meteorite appear', 'appear come', 'come asteroid', 'asteroid pass', 'pass close', 'close earth', 'earth radius', 'radius foot', 'foot depth', 'depth foot', 'foot say', 'say humberto', 'humberto saballo', 'saballo volcanologist', 'volcanologist nicaraguan', 'nicaraguan institute', 'institute territorial', 'territorial study', 'study committee', 'committee humberto', 'humberto saballo', 'saballo volcanologist', 'volcanologist nicaraguan', 'nicaraguan institute', 'institute territorial', 'territorial study', 'study committee', 'committee say', 'say clear', 'clear relatively', 'relatively small', 'small meteorite', 'meteorite appear', 'appear come', 'come asteroid', 'asteroid pass', 'pass close', 'close earth', 'earth disintegrate', 'disintegrate bury', 'bury humberto', 'humberto garcia', 'garcia astronomy', 'astronomy center', 'center national', 'national autonomous', 'autonomous university', 'university nicaragua', 'nicaragua say', 'say relatively', 'relatively small', 'small meteorite', 'meteorite appear', 'appear come', 'come asteroid', 'asteroid pass', 'pass close', 'close earth', 'earth relate', 'relate asteroid', 'asteroid forecast', 'forecast pass', 'pass planet', 'planet saturday', 'saturday night', 'night study', 'study relatively', 'relatively small', 'small meteorite', 'meteorite appear', 'appear come', 'come asteroid', 'asteroid pass', 'pass close', 'close earth', 'earth relatively', 'relatively small', 'small meteorite', 'meteorite appear', 'appear come', 'come asteroid', 'asteroid pass', 'pass close', 'close earth', 'earth ice', 'ice rock', 'rock say', 'say wilfri', 'wilfri strauch', 'strauch adviser', 'adviser institute', 'institute territorial', 'territorial study', 'study say', 'say strange', 'strange report', 'report streak', 'streak light', 'light ask', 'ask photo', 'photo local', 'local resident', 'resident report', 'report hear', 'hear loud', 'loud boom', 'boom saturday', 'saturday night', 'night say', 'say local', 'local resident', 'resident not', 'not strange', 'strange sky', 'sky sit', 'sit porch', 'porch see', 'see sudden', 'sudden hear', 'hear large', 'large blast', 'blast think', 'think bomb', 'bomb feel', 'feel expansive', 'expansive wave', 'wave jorge', 'jorge santamaria', 'santamaria tell', 'tell associate', 'associate press', 'press site', 'site crater', 'crater near', 'near managua', 'managua international', 'international airport', 'airport air', 'air force', 'force base', 'base journalist', 'journalist state', 'state medium', 'medium allow', 'allow visit', 'visit site', 'site crater'], 'nouns': ['meteorite', 'area', 'nicaragua', 'capital', 'managua', 'overnight', 'nicaragua', 'sunday', 'resident', 'boom', 'crater', 'city', 'airport', 'press', 'report', 'government', 'spokeswoman', 'rosario', 'murillo', 'committee', 'nicaragua', 'event', 'committee', 'government', 'event', 'meteorite', 'asteroid', 'earth', 'house', 'asteroid', 'rc', 'foot', 'diameter', 'earth', 'weekend', 'news', 'report', 'government', 'spokeswoman', 'rosario', 'murillo', 'nicaragua', 'expert', 'scientist', 'crater', 'meteorite', 'asteroid', 'earth', 'radius', 'foot', 'depth', 'foot', 'humberto', 'saballo', 'volcanologist', 'institute', 'study', 'committee', 'humberto', 'saballo', 'volcanologist', 'institute', 'study', 'committee', 'meteorite', 'asteroid', 'earth', 'humberto', 'garcia', 'astronomy', 'center', 'university', 'nicaragua', 'meteorite', 'asteroid', 'earth', 'asteroid', 'planet', 'saturday', 'night', 'meteorite', 'asteroid', 'earth', 'meteorite', 'asteroid', 'earth', 'ice', 'rock', 'adviser', 'institute', 'study', 'streak', 'light', 'photo', 'resident', 'boom', 'saturday', 'night', 'resident', 'sky', 'porch', 'blast', 'bomb', 'wave', 'jorge', 'santamaria', 'press', 'site', 'crater', 'managua', 'airport', 'air', 'force', 'base', 'journalist', 'state', 'medium', 'site', 'crater'], 'verbs': ['crash', 'say', 'report', 'hear', 'leave', 'associate', 'say', 'form', 'study', 'determine', 'form', 'study', 'appear', 'come', 'pass', 'measure', 'skim', 'say', 'ask', 'help', 'understand', 'happen', 'leave', 'appear', 'come', 'pass', 'say', 'say', 'appear', 'come', 'pass', 'disintegrate', 'bury', 'say', 'appear', 'come', 'pass', 'relate', 'forecast', 'pass', 'study', 'appear', 'come', 'pass', 'appear', 'come', 'pass', 'say', 'wilfri', 'say', 'report', 'ask', 'report', 'hear', 'say', 'sit', 'see', 'hear', 'think', 'feel', 'tell', 'associate', 'allow', 'visit'], 'first_sentence': {'raw': \"a small meteorite crashed into a wooded area in nicaragua's capital of managua overnight, nicaragua said sunday.\", 'tokens': ['small', 'meteorite', 'crash', 'wooded', 'area', 'nicaragua', 'capital', 'managua', 'overnight', 'nicaragua', 'say', 'sunday'], 'bigrams': ['small meteorite', 'meteorite crash', 'crash wooded', 'wooded area', 'area nicaragua', 'nicaragua capital', 'capital managua', 'managua overnight', 'overnight nicaragua', 'nicaragua say', 'say sunday'], 'nouns': ['meteorite', 'area', 'nicaragua', 'capital', 'managua', 'overnight', 'nicaragua', 'sunday'], 'verbs': ['crash', 'say'], 'adjectives': ['small', 'wooded'], 'adverbs': [], 'svo': [['meteorite', 'nicaragua'], ['say'], ['area', 'capital', 'managua']]}, 'significant_sentence': {'raw': 'i was sitting on my porch and i saw nothing, then all of a sudden i heard a large blast.', 'tokens': ['sit', 'porch', 'see', 'sudden', 'hear', 'large', 'blast'], 'bigrams': ['sit porch', 'porch see', 'see sudden', 'sudden hear', 'hear large', 'large blast'], 'nouns': ['porch', 'blast'], 'verbs': ['sit', 'see', 'hear'], 'adjectives': ['sudden', 'large'], 'adverbs': [], 'svo': [['i', 'all'], ['hear'], ['porch', 'nothing', 'sudden', 'blast']]}, 'vocabulary': ['expansive', 'air', 'city', 'airport', 'house', 'rc', 'report', 'loud', 'measure', 'mysterious', 'relate', 'strange', 'disintegrate', 'nicaraguan', 'see', 'committee', 'close', 'wooded', 'visit', 'jorge', 'associate', 'foot', 'hear', 'autonomous', 'force', 'earth', 'forecast', 'large', 'sky', 'adviser', 'area', 'feel', 'sit', 'santamaria', 'porch', 'understand', 'weekend', 'appear', 'ask', 'resident', 'help', 'saturday', 'abc', 'university', 'small', 'deep', 'murillo', 'streak', 'nicaragua', 'press', 'medium', 'planet', 'capital', 'astronomy', 'diameter', 'pass', 'bomb', 'wilfri', 'leave', 'say', 'bury', 'center', 'volcanologist', 'local', 'photo', 'crash', 'territorial', 'allow', 'meteorite', 'asteroid', 'boom', 'come', 'saballo', 'rock', 'spokeswoman', 'skim', 'managua', 'base', 'state', 'ice', 'national', 'crater', 'tell', 'news', 'think', 'study', 'event', 'night', 'sized', 'humberto', 'not', 'garcia', 'clear', 'light', 'journalist', 'form', 'institute', 'near', 'strauch', 'sunday', 'rosario', 'radius', 'blast', 'relatively', 'government', 'expert', 'scientist', 'wave', 'international', 'depth', 'determine', 'sudden', 'overnight', 'happen', 'site'], 'common_tokens': ['asteroid', 'meteorite', 'relatively', 'earth', 'nicaragua'], 'common_nouns': ['asteroid', 'meteorite', 'earth', 'nicaragua', 'crater'], 'common_verbs': ['pass', 'study', 'appear', 'skim', 'wilfri'], 'common_bigrams': [('small', 'meteorite'), ('relatively', 'small'), ('meteorite', 'appear'), ('appear', 'come'), ('come', 'asteroid'), ('asteroid', 'pass'), ('pass', 'close'), ('close', 'earth'), ('institute', 'territorial'), ('territorial', 'study')]}\n"
     ]
    }
   ],
   "source": [
    "print(body_info[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(stance_df):\n",
    "    start = time.time()\n",
    "    data = list(stance_df.values)\n",
    "    features = []\n",
    "    actual = []\n",
    "    for item in data:\n",
    "        h, b, s = tuple(item)\n",
    "        fts = get_feats(h, b)\n",
    "        features.append(fts)\n",
    "        actual.append(s)\n",
    "    end = time.time()\n",
    "    print(int(end-start))\n",
    "    return features, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_cos_similarity(a, b):\n",
    "    vocab = list(set(a).union(set(b)))\n",
    "    a_bow, b_bow = set(a), set(b)\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return -1\n",
    "    a_vec = [(1 if i in a_bow else 0) for i in vocab]\n",
    "    b_vec = [(1 if i in b_bow else 0) for i in vocab]\n",
    "    return 1 - distance.cosine(a_vec, b_vec)\n",
    "\n",
    "def get_feats(headline, body_id):\n",
    "    headline_data = headline_info[headline]\n",
    "    body_data = body_info[body_id]\n",
    "\n",
    "    shared_common_nouns = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['common_nouns'])))\n",
    "    shared_common_verbs = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['common_verbs'])))\n",
    "    shared_common_tokens = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['common_tokens'])))\n",
    "    shared_bigrams = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['common_bigrams'])))\n",
    "\n",
    "    shared_nouns_first = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['first_sentence']['nouns'])))\n",
    "    shared_verbs_first = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['first_sentence']['verbs'])))\n",
    "    shared_bigrams_first = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['first_sentence']['bigrams'])))\n",
    "    shared_tokens_first = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['first_sentence']['tokens'])))\n",
    "\n",
    "    shared_nouns_sig = len(set(headline_data['nouns']).intersection(\n",
    "        set(body_data['significant_sentence']['nouns'])))\n",
    "    shared_verbs_sig = len(set(headline_data['verbs']).intersection(\n",
    "        set(body_data['significant_sentence']['verbs'])))\n",
    "    shared_bigrams_sig = len(set(headline_data['bigrams']).intersection(\n",
    "        set(body_data['significant_sentence']['bigrams'])))\n",
    "    shared_tokens_sig = len(set(headline_data['tokens']).intersection(\n",
    "        set(body_data['significant_sentence']['tokens'])))\n",
    "\n",
    "    headline_svo = headline_data['svo']\n",
    "    body_fst_svo = body_data['first_sentence']['svo']\n",
    "    body_sig_svo = body_data['significant_sentence']['svo']\n",
    "\n",
    "    # cosine similarity - no verbs because relatively few per sentence\n",
    "    cos_nouns_first = bow_cos_similarity(\n",
    "        headline_data['nouns'], body_data['first_sentence']['nouns'])\n",
    "    cos_bigrams_first = bow_cos_similarity(\n",
    "        headline_data['bigrams'], body_data['first_sentence']['bigrams'])\n",
    "    cos_tokens_first = bow_cos_similarity(\n",
    "        headline_data['tokens'], body_data['first_sentence']['tokens'])\n",
    "\n",
    "    cos_nouns_sig = bow_cos_similarity(\n",
    "        headline_data['nouns'], body_data['significant_sentence']['nouns'])\n",
    "    cos_bigrams_sig = bow_cos_similarity(\n",
    "        headline_data['bigrams'], body_data['significant_sentence']['bigrams'])\n",
    "    cos_tokens_sig = bow_cos_similarity(\n",
    "        headline_data['tokens'], body_data['significant_sentence']['tokens'])\n",
    "    \n",
    "    svo_cos_sim_fst = bow_cos_similarity(\n",
    "        body_fst_svo[0]+body_fst_svo[1]+body_fst_svo[2], \n",
    "        headline_svo[0]+headline_svo[1]+headline_svo[2])\n",
    "\n",
    "    svo_cos_sim_sig = bow_cos_similarity(\n",
    "        body_sig_svo[0]+body_sig_svo[1]+body_sig_svo[2], \n",
    "        headline_svo[0]+headline_svo[1]+headline_svo[2])\n",
    "    \n",
    "    svo_s_fst = len(set(body_fst_svo[0]).intersection(set(headline_svo[0]))) \n",
    "    svo_v_fst = len(set(body_fst_svo[1]).intersection(set(headline_svo[1])))\n",
    "    svo_o_fst = len(set(body_fst_svo[2]).intersection(set(headline_svo[2])))\n",
    "    svo_s_sig = len(set(body_sig_svo[0]).intersection(set(headline_svo[0])))\n",
    "    svo_v_sig = len(set(body_sig_svo[1]).intersection(set(headline_svo[1])))\n",
    "    svo_o_sig = len(set(body_sig_svo[2]).intersection(set(headline_svo[2])))\n",
    "    \n",
    "    return {\n",
    "        'shared_nouns': shared_common_nouns,\n",
    "        'shared_verbs': shared_common_verbs,\n",
    "        'shared_bigrams': shared_bigrams,\n",
    "        'shared_tokens': shared_common_tokens,\n",
    "\n",
    "        'shared_nouns_fst': shared_nouns_first,\n",
    "        'shared_verbs_fst': shared_verbs_first,\n",
    "        'shared_bigrams_fst': shared_bigrams_first,\n",
    "        'shared_tokens_fst': shared_tokens_first,\n",
    "\n",
    "        'shared_nouns_sig': shared_nouns_sig,\n",
    "        'shared_verbs_sig': shared_verbs_sig,\n",
    "        'shared_bigrams_sig': shared_bigrams_sig,\n",
    "        'shared_tokens_sig': shared_tokens_sig,\n",
    "\n",
    "        'cos_nouns_sig': cos_nouns_sig,\n",
    "        'cos_bigrams_sig': cos_bigrams_sig,\n",
    "        'cos_tokens_sig': cos_tokens_sig,\n",
    "\n",
    "        'cos_nouns_fst': cos_nouns_first,\n",
    "        'cos_bigrams_fst': cos_bigrams_first,\n",
    "        'cos_tokens_fst': cos_tokens_first,\n",
    "\n",
    "        'svo_cos_sim_fst' : svo_cos_sim_fst,\n",
    "        'svo_cos_sim_sig' : svo_cos_sim_sig,\n",
    "        \n",
    "        'svo_s_fst': svo_s_fst,\n",
    "        'svo_v_fst': svo_v_fst,\n",
    "        'svo_o_fst': svo_o_fst,\n",
    "\n",
    "        'svo_s_sig': svo_s_sig,\n",
    "        'svo_v_sig': svo_v_sig,\n",
    "        'svo_o_sig': svo_o_sig,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "stance_info = get_features(train_stances)\n",
    "stance_dict = {}\n",
    "for idx, d in enumerate(list(train_stances.values)):\n",
    "    h, b, s = d\n",
    "    stance_dict[(h, b)] = stance_info[0][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[1, 1, 0, 3, 2, 1, 1, 5, 0, 0, 0, 1, 0.0, 0.0, 0.09449111825230683, 0.3086066999241839, 0.08247860988423228, 0.3857583749052298, 0.30304576336566325, 0.13363062095621214] agree\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0.20412414523193156, 0.0, 0.10846522890932808, 0.0, 0.0, 0.0, 0.0, 0.16666666666666663] disagree\n",
      "[1, 0, 0, 5, 1, 0, 2, 8, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.33333333333333326, 0.15384615384615385, 0.5714285714285714, 0.5017452060042545, 0.0] agree\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307687, 0.14433756729740643, 0.0] agree\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.10482848367219177, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] discuss\n",
      "[2, 1, 0, 4, 2, 1, 3, 7, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.3779644730092272, 0.24743582965269673, 0.5400617248673216, 0.5270462766947299, 0.0] agree\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0.0, 0.0, 0.09622504486493766, 0.0, 0.0, 0.0, 0.0, 0.0] discuss\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 2, 0.0, 0.11785113019775795, 0.20965696734438366, 0.0, 0.0, 0.09449111825230683, 0.0, 0.2519763153394847] discuss\n",
      "[0, 1, 0, 1, 0, 1, 2, 4, 0, 1, 0, 2, -1, 0.0, 0.4364357804719847, -1, 0.18257418583505536, 0.329914439536929, 0.3872983346207417, 0.2581988897471611] agree\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10846522890932808, 0.0, 0.0] discuss\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 2, 1, 0, 1, 3, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.158113883008419, 0.09759000729485334, 0.26516504294495524, 0.2672612419124243, 0.0] discuss\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09622504486493766, 0.0, 0.0] agree\n",
      "[1, 0, 0, 2, 2, 0, 0, 4, 2, 0, 0, 4, 0.3481553119113957, 0.0, 0.35355339059327373, 0.3481553119113957, 0.0, 0.35355339059327373, 0.32732683535398865, 0.32732683535398865] agree\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0.0, 0.0, 0.0716114874039433, 0.0, 0.0, 0.0716114874039433, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 1, 0, 0, 1, 3, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.09759000729485334, 0.26516504294495524, 0.23904572186687867, 0.0] discuss\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.08908708063747484, 0.16222142113076254, 0.2581988897471611, 0.0] discuss\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 4, 0, 0, 5, 6, 0, 0, 5, 6, -1, 0.3834824944236852, 0.4522670168666454, -1, 0.3834824944236852, 0.4522670168666454, 0.50709255283711, 0.50709255283711] discuss\n",
      "[1, 1, 0, 1, 2, 0, 3, 5, 1, 0, 0, 1, 0.35355339059327373, 0.0, 0.15430334996209194, 0.3651483716701107, 0.2672612419124244, 0.42257712736425823, 0.5270462766947299, 0.18257418583505536] agree\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1290994448735806, 0.14142135623730945] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0.16903085094570325, 0.0, 0.18898223650461365, 0.22360679774997894, 0.0, 0.15430334996209194, 0.16666666666666674, 0.1290994448735806] agree\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17677669529663687] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 1, 0.0, 0.0, 0.10540925533894596, 0.0, 0.1290994448735805, 0.22645540682891918, 0.3380617018914065, 0.14907119849998596] disagree\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0] agree\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[1, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.40824829046386313, 0.0, 0.25, 0.4285714285714285, 0.1428571428571428] disagree\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, -1, -1, 0.0, 0.0, 0.11396057645963797] discuss\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[1, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21081851067789192, 0.0, 0.0] agree\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.0, 0.0, 0.06900655593423544, 0.0, 0.0, 0.0, 0.0, 0.09534625892455928] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[1, 2, 0, 2, 1, 0, 1, 3, 2, 0, 0, 2, 0.33333333333333337, 0.0, 0.1290994448735805, 0.18257418583505536, 0.06741998624632417, 0.19364916731037085, 0.21821789023599236, 0.17817416127494956] discuss\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09534625892455928] unrelated\n",
      "[0, 1, 0, 2, 0, 1, 0, 3, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.2672612419124244, 0.23904572186687867, 0.0] agree\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.0, 0.0, 0.08838834764831849, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0.0, 0.0, 0.10660035817780522, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10050378152592121, 0.1290994448735806, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08606629658238707, 0.0, 0.0] discuss\n",
      "[0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 2, 0.13363062095621214, 0.0, 0.12171612389003694, 0.0, 0.0, 0.15569978883230462, 0.21320071635561044, 0.10050378152592121] agree\n",
      "[0, 0, 0, 2, 0, 0, 2, 3, 0, 0, 1, 2, 0.0, 0.10910894511799618, 0.20965696734438366, 0.0, 0.1740776559556978, 0.2508726030021273, 0.20100756305184242, 0.3481553119113957] discuss\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.10660035817780522, 0.14433756729740643, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] discuss\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0.0, 0.0, -1, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15430334996209194, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] unrelated\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(list(stance_info[0][i].values()), stance_info[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_to_int(labels):\n",
    "    return [(1 if l == \"agree\" else (0 if l == \"discuss\" else -1)) for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('saved_data/headline_info.json', 'w') as fp:\n",
    "    json.dump(headline_info, fp)\n",
    "with open('saved_data/body_info.json', 'w') as fp:\n",
    "    json.dump(body_info, fp)\n",
    "with open('saved_data/idf.json', 'w') as fp:\n",
    "    json.dump(idf, fp)\n",
    "with open('saved_data/stance_info.json', 'w') as fp:\n",
    "    json.dump(stance_info, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29983724851076954, 0.5771482666970632, 4.096128684015602)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf['say'], idf['report'], idf['spider']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'unrelated': 29275, 'discuss': 10734}) Counter({'unrelated': 7270, 'discuss': 2693})\n",
      "Training Baseline 73.17% Testing Baseline 72.97%\n",
      "94.90% training accuracy\n",
      "94.62% validation accuracy\n",
      "Baseline comparison: TR 21.73% VAL 21.65%\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |     0     |     0     |     0     |     0     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     0     |     0     |     0     |     0     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |     0     |     0     |   2360    |    333    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     0     |     0     |    203    |   7067    |\n",
      "-------------------------------------------------------------\n",
      "Score: 4126.75 out of 4510.5\t(91.49207404944019%)\n",
      "F1 Score\n",
      "[0.89802131 0.96346285]\n",
      "Avg Precision Score\n",
      "[0.92079594 0.955     ]\n"
     ]
    }
   ],
   "source": [
    "stances_tr, stances_val = train_test_split(train_bodies, train_stances)\n",
    "\n",
    "training_data = [[],[]]\n",
    "for h,b,s in list(stances_tr.values):\n",
    "    training_data[0].append(list(stance_dict[(h,b)].values()))\n",
    "    training_data[1].append(s if s == \"unrelated\" else \"discuss\")\n",
    "\n",
    "testing_data = [[],[]]\n",
    "for h,b,s in list(stances_val.values):\n",
    "    testing_data[0].append(list(stance_dict[(h,b)].values()))\n",
    "    testing_data[1].append(s if s == \"unrelated\" else \"discuss\")\n",
    "\n",
    "c1, c2 = Counter(training_data[1]), Counter(testing_data[1])\n",
    "baseline_tr = max(c1.values())/sum(c1.values())\n",
    "baseline_val = max(c2.values())/sum(c2.values())\n",
    "print(c1, c2)\n",
    "print(\"Training Baseline {0:.2f}% Testing Baseline {1:.2f}%\".format(baseline_tr * 100, baseline_val * 100))\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 500, min_samples_split = 10, min_samples_leaf = 5, max_depth = 8)\n",
    "# model = LogisticRegression(max_iter = 200)\n",
    "# model = SVC()\n",
    "# model = GradientBoostingClassifier(n_estimators = 500, subsample = 0.1, learning_rate = 0.01, random_state=0)\n",
    "\n",
    "model.fit(training_data[0], training_data[1])\n",
    "tr_acc = model.score(training_data[0], training_data[1])\n",
    "print('{0:.2f}% training accuracy'.format(tr_acc*100))\n",
    "\n",
    "val_acc = model.score(testing_data[0], testing_data[1])\n",
    "print('{0:.2f}% validation accuracy'.format(val_acc*100))\n",
    "print(\"Baseline comparison: TR {0:.2f}% VAL {1:.2f}%\".format((tr_acc-baseline_tr)*100,(val_acc-baseline_val)*100))\n",
    "\n",
    "actual = testing_data[1]\n",
    "predicted = model.predict(testing_data[0])\n",
    "sc.report_score(actual, predicted)\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(actual, predicted, average = None))\n",
    "print(\"Avg Precision Score\")\n",
    "print(precision_score(actual, predicted, average = None))\n",
    "matrix = confusion_matrix(actual,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_models/relevance_detection_trained.joblib']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stances_tr = train_stances\n",
    "\n",
    "training_data = [[],[]]\n",
    "for h,b,s in list(stances_tr.values):\n",
    "    training_data[0].append(list(stance_dict[(h,b)].values()))\n",
    "    training_data[1].append(s if s == \"unrelated\" else \"discuss\")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 500, min_samples_split = 10, min_samples_leaf = 5, max_depth = 10)\n",
    "\n",
    "model.fit(training_data[0], training_data[1])\n",
    "\n",
    "from joblib import dump, load\n",
    "dump(model, 'saved_models/relevance_detection_trained.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
