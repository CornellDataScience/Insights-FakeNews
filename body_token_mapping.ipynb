{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "#Run nltk.download() if you encounter nltk issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bodies = pandas.read_csv('./fn_data/train_bodies.csv', index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_bodies(data, body_col):\n",
    "    \"\"\"\n",
    "    data: DataFrame\n",
    "    body_col : string\n",
    "    \n",
    "    data represents a DataFrame containing Body IDs and actual text bodies.\n",
    "    \n",
    "    Ex:    Body ID                                        articleBody\n",
    "    0           0  A small meteorite crashed into a wooded area i...\n",
    "    1           4  Last week we hinted at what was to come as Ebo...\n",
    "    2           5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
    "    3           6  Posting photos of a gun-toting child online, I...\n",
    "    4           7  At least 25 suspected Boko Haram insurgents we...\n",
    "    5           8  There is so much fake stuff on the Internet in...\n",
    "    6           9  (CNN) -- A meteorite crashed down in Managua, ...\n",
    "\n",
    "\n",
    "\n",
    "    body_col is the name of the column containing article text bodies\n",
    "    \n",
    "    Returns: dictionary such that {Body ID : Body Text}\n",
    "    \"\"\"\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for x in range(len(data[body_col])):\n",
    "        dictionary.update({data.iloc[x,0] : data.iloc[x,1]})\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bodydict = map_bodies(bodies, 'articleBody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dict(dictionary):\n",
    "    \"\"\"\n",
    "    dictionary : dictionary\n",
    "    \n",
    "    Takes in a dictionary containing mappings from Body ID to Body.\n",
    "    Returns a dictionary containing mappings from Body ID to Tokenized Bodies.\n",
    "    \"\"\"\n",
    "    new_dict = dict()\n",
    "    for x in dictionary:\n",
    "        tokens = nltk.word_tokenize(dictionary.get(x))\n",
    "        new_dict.update({x:tokens})\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bodydict = tokenize_dict(bodydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(bodydict.get(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_tokens(dictionary):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary containing mappings from Body ID to tokenized bodies.\n",
    "    Returns a dictionary containing mappings from Body ID to tagged tokenized bodies.\n",
    "    \"\"\"\n",
    "    new_dict = dict()\n",
    "    for x in dictionary:\n",
    "        tagged = nltk.pos_tag(dictionary.get(x))\n",
    "        new_dict.update({x:tagged})\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headlines = pandas.read_csv('./fn_data/train_stances.csv', index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_dict = dict()\n",
    "    \n",
    "for x in range(len(headlines['Headline'])):\n",
    "    headline_dict.update({headlines.iloc[x,1] : headlines.iloc[x,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Mass Grave Points to a Student Massacre in Mexico'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_dict.get(712)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizeddict = tag_tokens(tokenize_dict(map_bodies(bodies, 'articleBody')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_dict = dict()\n",
    "for x in headline_dict:\n",
    "    master_dict.update({x:[headline_dict.get(x), tokenizeddict.get(x)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_embedding = nltk.pos_tag(nltk.word_tokenize(master_dict.get(295)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_nouns(headline, body):\n",
    "    noun_count = 0\n",
    "    head_nouns = set()\n",
    "    head_embedding = nltk.pos_tag(nltk.word_tokenize(headline))\n",
    "    for x in head_embedding:\n",
    "        if x[1] == \"NN\" or x[1] == \"NNP\" or x[1] == \"NNS\":\n",
    "            head_nouns.add(x[0])\n",
    "    for y in body:\n",
    "        if y[1] == \"NN\" or y[1] == \"NNP\" or y[1] == \"NNS\":\n",
    "            if y[0] in head_nouns:\n",
    "                noun_count += 1\n",
    "    return noun_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complete_dictionary(bodydata, body_col, headlinedata):\n",
    "    dictionary = dict()\n",
    "    dictionary = map_bodies(bodydata, body_col)\n",
    "    dictionary = tag_tokens(tokenize_dict(dictionary))\n",
    "    \n",
    "    headline_dict = dict()\n",
    "    for x in range(len(headlinedata['Headline'])):\n",
    "        headline_dict.update({headlines.iloc[x,1] : headlines.iloc[x,0]})\n",
    "    \n",
    "    master_dict = dict()\n",
    "    for x in headline_dict:\n",
    "        master_dict.update({x:[headline_dict.get(x), dictionary.get(x), match_nouns(headline_dict.get(x), dictionary.get(x))]})\n",
    "    \n",
    "    return master_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final = complete_dictionary(bodies, 'articleBody', headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['’6 Days Darkness in December 2014′ 100% Fake; NASA Confirmed 3 Days Total Darkness Hoax as Well', [('Thousands', 'NNS'), ('of', 'IN'), ('people', 'NNS'), ('have', 'VBP'), ('been', 'VBN'), ('duped', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('fake', 'JJ'), ('news', 'NN'), ('story', 'NN'), ('claiming', 'VBG'), ('that', 'IN'), ('Nasa', 'NNP'), ('has', 'VBZ'), ('forecast', 'VBN'), ('a', 'DT'), ('total', 'JJ'), ('blackout', 'NN'), ('of', 'IN'), ('earth', 'NN'), ('for', 'IN'), ('six', 'CD'), ('days', 'NNS'), ('in', 'IN'), ('December', 'NNP'), ('.', '.'), ('The', 'DT'), ('story', 'NN'), (',', ','), ('entitled', 'VBN'), ('``', '``'), ('Nasa', 'NNP'), ('Confirms', 'NNP'), ('Earth', 'NNP'), ('Will', 'NNP'), ('Experience', 'NNP'), ('6', 'CD'), ('Days', 'NNP'), ('of', 'IN'), ('Total', 'NNP'), ('Darkness', 'NNP'), ('in', 'IN'), ('December', 'NNP'), ('2014', 'CD'), ('!', '.'), (\"''\", \"''\"), ('originated', 'VBD'), ('from', 'IN'), ('Huzlers.com', 'NNP'), (',', ','), ('a', 'DT'), ('website', 'JJ'), ('well', 'NN'), ('known', 'VBN'), ('for', 'IN'), ('publishing', 'VBG'), ('fake', 'JJ'), ('stories', 'NNS'), ('with', 'IN'), ('sensational', 'JJ'), ('headlines', 'NNS'), ('.', '.'), ('The', 'DT'), ('bogus', 'JJ'), ('report', 'NN'), ('read', 'VBD'), (':', ':'), ('``', '``'), ('Nasa', 'NNP'), ('has', 'VBZ'), ('confirmed', 'VBN'), ('that', 'IN'), ('the', 'DT'), ('Earth', 'NNP'), ('will', 'MD'), ('experience', 'VB'), ('6', 'CD'), ('days', 'NNS'), ('of', 'IN'), ('almost', 'RB'), ('complete', 'JJ'), ('darkness', 'NN'), ('and', 'CC'), ('will', 'MD'), ('happen', 'VB'), ('from', 'IN'), ('the', 'DT'), ('dates', 'NNS'), ('Tuesday', 'NNP'), ('the', 'DT'), ('16', 'CD'), ('–', 'NN'), ('Monday', 'NNP'), ('the', 'DT'), ('22', 'CD'), ('in', 'IN'), ('December', 'NNP'), ('.', '.'), ('The', 'DT'), ('world', 'NN'), ('will', 'MD'), ('remain', 'VB'), (',', ','), ('during', 'IN'), ('these', 'DT'), ('three', 'CD'), ('days', 'NNS'), (',', ','), ('without', 'IN'), ('sunlight', 'JJ'), ('due', 'JJ'), ('to', 'TO'), ('a', 'DT'), ('solar', 'JJ'), ('storm', 'NN'), (',', ','), ('which', 'WDT'), ('will', 'MD'), ('cause', 'VB'), ('dust', 'NN'), ('and', 'CC'), ('space', 'NN'), ('debris', 'NN'), ('to', 'TO'), ('become', 'VB'), ('plentiful', 'JJ'), ('and', 'CC'), ('thus', 'RB'), (',', ','), ('block', 'VB'), ('90', 'CD'), ('%', 'NN'), ('sunlight', 'NN'), ('.', '.'), ('``', '``'), ('The', 'DT'), ('head', 'NN'), ('of', 'IN'), ('Nasa', 'NNP'), ('Charles', 'NNP'), ('Bolden', 'NNP'), ('who', 'WP'), ('made', 'VBD'), ('the', 'DT'), ('announcement', 'NN'), ('and', 'CC'), ('asked', 'VBD'), ('everyone', 'NN'), ('to', 'TO'), ('remain', 'VB'), ('calm', 'JJ'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('be', 'VB'), ('the', 'DT'), ('product', 'NN'), ('of', 'IN'), ('a', 'DT'), ('solar', 'JJ'), ('storm', 'NN'), (',', ','), ('the', 'DT'), ('largest', 'JJS'), ('in', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('250', 'CD'), ('years', 'NNS'), ('for', 'IN'), ('a', 'DT'), ('period', 'NN'), ('of', 'IN'), ('216', 'CD'), ('hours', 'NNS'), ('total', 'JJ'), ('.', '.'), ('``', '``'), ('Despite', 'IN'), ('the', 'DT'), ('six', 'CD'), ('days', 'NNS'), ('of', 'IN'), ('darkness', 'JJ'), ('soon', 'RB'), ('to', 'TO'), ('come', 'VB'), (',', ','), ('officials', 'NNS'), ('say', 'VBP'), ('that', 'IN'), ('the', 'DT'), ('earth', 'NN'), ('will', 'MD'), ('not', 'RB'), ('experience', 'VB'), ('any', 'DT'), ('major', 'JJ'), ('problems', 'NNS'), (',', ','), ('since', 'IN'), ('six', 'CD'), ('days', 'NNS'), ('of', 'IN'), ('darkness', 'NN'), ('is', 'VBZ'), ('nowhere', 'RB'), ('near', 'RB'), ('enough', 'RB'), ('to', 'TO'), ('cause', 'VB'), ('major', 'JJ'), ('damage', 'NN'), ('to', 'TO'), ('anything', 'NN'), ('.', '.'), (\"''\", \"''\"), ('Adding', 'VBG'), ('on', 'IN'), (',', ','), ('the', 'DT'), ('article', 'NN'), ('also', 'RB'), ('carried', 'VBD'), ('a', 'DT'), ('made-up', 'JJ'), ('quote', 'NN'), ('from', 'IN'), ('Nasa', 'NNP'), ('scientist', 'NN'), ('Earl', 'NNP'), ('Godoy', 'NNP'), (',', ','), ('saying', 'VBG'), (':', ':'), ('``', '``'), ('We', 'PRP'), ('will', 'MD'), ('solely', 'RB'), ('rely', 'VB'), ('on', 'IN'), ('artificial', 'JJ'), ('light', 'NN'), ('for', 'IN'), ('the', 'DT'), ('six', 'CD'), ('days', 'NNS'), (',', ','), ('which', 'WDT'), ('is', 'VBZ'), ('not', 'RB'), ('a', 'DT'), ('problem', 'NN'), ('at', 'IN'), ('all', 'DT'), ('.', '.'), (\"''\", \"''\"), ('Many', 'JJ'), ('Twitter', 'NNP'), ('users', 'NNS'), ('believed', 'VBD'), ('the', 'DT'), ('fake', 'JJ'), ('news', 'NN'), ('report', 'NN'), (',', ','), ('and', 'CC'), ('expressed', 'VBD'), ('their', 'PRP$'), ('shock', 'NN'), ('.', '.'), ('We', 'PRP'), (\"'re\", 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('complete', 'JJ'), ('6', 'CD'), ('days', 'NNS'), ('of', 'IN'), ('darkness', 'NN'), ('due', 'JJ'), ('to', 'TO'), ('a', 'DT'), ('solar', 'JJ'), ('storm', 'NN'), ('in', 'IN'), ('Dec', 'NNP'), ('!', '.'), ('SO', 'RB'), ('NERVOUS', 'JJ'), ('ABOUT', 'IN'), ('THIS', 'NN'), ('!', '.'), ('Ahhh', 'NNP'), ('.', '.'), ('#', '#'), ('ThePurge', 'NNP'), ('http', 'NN'), (':', ':'), ('//t.co/0L2Sis54hv—', 'JJ'), ('Janella', 'NNP'), ('(', '('), ('@', 'NNP'), ('hijanellamarie', 'NN'), (')', ')'), ('October', 'NNP'), ('26', 'CD'), (',', ','), ('2014', 'CD'), ('6', 'CD'), ('days', 'NNS'), ('of', 'IN'), ('total', 'JJ'), ('darkness', 'NN'), ('in', 'IN'), ('December', 'NNP'), ('?', '.'), ('?', '.'), ('http', 'NN'), (':', ':'), ('//t.co/eTN60TnXft—', 'JJ'), ('Jammie', 'NNP'), ('Macaranas', 'NNP'), ('(', '('), ('@', 'NNP'), ('JammiePeach', 'NNP'), (')', ')'), ('October', 'NNP'), ('26', 'CD'), (',', ','), ('2014', 'CD'), ('``', '``'), ('NASA', 'NNP'), ('Confirms', 'NNP'), ('Earth', 'NNP'), ('will', 'MD'), ('experience', 'VB'), ('6', 'CD'), ('Days', 'NNPS'), ('of', 'IN'), ('total', 'JJ'), ('DARKNESS', 'NNP'), ('in', 'IN'), ('December', 'NNP'), ('2014', 'CD'), ('.', '.'), (\"''\", \"''\"), ('Me', 'NN'), (':', ':'), ('pic.twitter.com/xZG1xaxqdw—', 'NN'), ('[', 'NN'), ('Hiatus', 'NNP'), (']', 'NNP'), ('TT', 'NNP'), ('(', '('), ('@', 'NNP'), ('sarangBCES', 'NN'), (')', ')'), ('October', 'NNP'), ('26', 'CD'), (',', ','), ('2014', 'CD'), ('``', '``'), ('NASA', 'NNP'), ('Confirms', 'NNP'), ('Earth', 'NNP'), ('Will', 'NNP'), ('Experience', 'NNP'), ('6', 'CD'), ('Days', 'NNP'), ('of', 'IN'), ('Total', 'NNP'), ('Darkness', 'NNP'), ('in', 'IN'), ('December', 'NNP'), ('2014', 'CD'), ('!', '.'), (\"''\", \"''\"), ('omg', 'VBZ'), ('what', 'WP'), ('?', '.'), ('—', 'JJ'), ('查理', 'NN'), ('(', '('), ('@', 'NNP'), ('Chxrliecutie', 'NNP'), (')', ')'), ('October', 'NNP'), ('26', 'CD'), (',', ','), ('2014', 'CD'), ('islam', 'NN'), ('know', 'VB'), ('what', 'WP'), ('this', 'DT'), ('means', 'VBZ'), ('im', 'JJ'), ('scared', 'VBN'), ('``', '``'), ('NASA', 'NNP'), ('Confirms', 'NNP'), ('Earth', 'NNP'), ('Will', 'NNP'), ('Experience', 'NNP'), ('6', 'CD'), ('Days', 'NNP'), ('of', 'IN'), ('Total', 'NNP'), ('Darkness', 'NNP'), ('in', 'IN'), ('December', 'NNP'), ('2014', 'CD'), ('!', '.'), ('http', 'NN'), (':', ':'), ('//t.co/GQGeGLmElZ', 'NN'), (\"''\", \"''\"), ('—', 'NN'), ('hiatus', 'NN'), ('(', '('), ('@', 'JJ'), ('taobby', 'NN'), (')', ')'), ('October', 'NNP'), ('26', 'CD'), (',', ','), ('2014', 'CD'), ('The', 'DT'), ('website', 'NN'), ('has', 'VBZ'), ('previously', 'RB'), ('published', 'VBN'), ('a', 'DT'), ('fake', 'JJ'), ('report', 'NN'), ('about', 'IN'), ('American', 'JJ'), ('rapper', 'NN'), ('and', 'CC'), ('actor', 'NN'), ('Tupac', 'NNP'), ('Shakur', 'NNP'), (',', ','), ('claiming', 'VBG'), ('that', 'IN'), ('he', 'PRP'), ('is', 'VBZ'), ('alive', 'JJ'), ('.', '.'), ('RelatedHalloween', 'JJ'), ('2014', 'CD'), ('on', 'IN'), ('Friday', 'NNP'), ('the', 'DT'), ('13th', 'NNS'), ('for', 'IN'), ('First', 'JJ'), ('Time', 'NNP'), ('in', 'IN'), ('666', 'CD'), ('Years', 'NNS'), ('Declared', 'VBD'), ('a', 'DT'), ('HoaxShah', 'NNP'), ('Rukh', 'NNP'), ('Khan', 'NNP'), (\"'s\", 'POS'), ('Son', 'NNP'), ('Aryan', 'NNP'), ('and', 'CC'), ('Aishwarya', 'NNP'), ('Rai', 'NNP'), ('Bachchan', 'NNP'), (\"'s\", 'POS'), ('Niece', 'NNP'), ('Navya', 'NNP'), (\"'s\", 'POS'), ('Leaked', 'NNP'), ('Sex', 'NNP'), ('Tape', 'NNP'), ('is', 'VBZ'), ('FakeEbola', 'NNP'), ('Zombies', 'NNS'), (':', ':'), ('Victims', 'NNS'), (\"'Rising\", 'VBG'), ('from', 'IN'), ('the', 'DT'), ('Dead', 'NNP'), (\"'\", 'POS'), ('Fake', 'NNP'), ('News', 'NNP'), ('Story', 'NNP'), ('Goes', 'NNP'), ('Viral', 'NNP'), (',', ','), ('Sparks', 'NNP'), ('Outrage', 'NNP'), ('on', 'IN'), ('Social', 'NNP'), ('MediaEminem', 'NNP'), (\"'Quits\", 'POS'), ('Music', 'NNP'), ('After', 'IN'), ('Checking', 'VBG'), ('Into', 'NNP'), ('Rehab', 'NNP'), ('Again', 'NNP'), ('For', 'IN'), ('Heroin', 'NNP'), ('Addiction', 'NNP'), (\"'\", 'POS'), ('is', 'VBZ'), ('Hoax', 'NNP'), (':', ':'), ('Satirical', 'JJ'), ('Article', 'NNP'), ('Creates', 'NNP'), ('Stir', 'NNP'), ('on', 'IN'), ('Social', 'NNP'), ('Media', 'NNP')], 22]\n"
     ]
    }
   ],
   "source": [
    "print(final.get(154))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stance_dict = dict()\n",
    "for x in range(len(headlines['Headline'])):\n",
    "    #Incorporating stance\n",
    "    stance_dict.update({headlines.iloc[x,1] : headlines.iloc[x,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "noun_count = []\n",
    "stance = []\n",
    "for x in final:\n",
    "    ids.append(x)\n",
    "    noun_count.append(final.get(x)[2])\n",
    "    stance.append(stance_dict.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'ID': ids, 'Noun Matches': noun_count, 'Stance': stance}\n",
    "df = pandas.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID  Noun Matches     Stance\n",
      "0      712             0  unrelated\n",
      "1      158             0  unrelated\n",
      "2      137             0  unrelated\n",
      "3     1034             0  unrelated\n",
      "4     1923             0  unrelated\n",
      "5      154            22      agree\n",
      "6      962             2  unrelated\n",
      "7     2033             0  unrelated\n",
      "8     1739             0  unrelated\n",
      "9      882             3  unrelated\n",
      "10    2327             0  unrelated\n",
      "11    1468             9   disagree\n",
      "12    1003             0  unrelated\n",
      "13    2132             0      agree\n",
      "14      47             0  unrelated\n",
      "15     615             0  unrelated\n",
      "16    2463             0  unrelated\n",
      "17     295             1  unrelated\n",
      "18     570             0  unrelated\n",
      "19     608             0  unrelated\n",
      "20    1500            19   disagree\n",
      "21    1681             0  unrelated\n",
      "22    1545             0  unrelated\n",
      "23    1196             0  unrelated\n",
      "24    1014             3      agree\n",
      "25     633             0  unrelated\n",
      "26      56             0  unrelated\n",
      "27    1658            13    discuss\n",
      "28    1157             0  unrelated\n",
      "29     132             0  unrelated\n",
      "...    ...           ...        ...\n",
      "1653  1129            23    discuss\n",
      "1654   200            26    discuss\n",
      "1655   476            11      agree\n",
      "1656  2032            22    discuss\n",
      "1657   243            21    discuss\n",
      "1658   285             0      agree\n",
      "1659   877            42    discuss\n",
      "1660    76            16    discuss\n",
      "1661   307            19    discuss\n",
      "1662     9             7      agree\n",
      "1663   355            22      agree\n",
      "1664   302            16    discuss\n",
      "1665  1085            12      agree\n",
      "1666   219            11    discuss\n",
      "1667   352            15    discuss\n",
      "1668   159             4    discuss\n",
      "1669   907             1      agree\n",
      "1670   828             2      agree\n",
      "1671   146            17      agree\n",
      "1672   854            26    discuss\n",
      "1673   797            15      agree\n",
      "1674    74            11    discuss\n",
      "1675   135            11      agree\n",
      "1676   175             0    discuss\n",
      "1677   553            32      agree\n",
      "1678   464             3      agree\n",
      "1679   362            18      agree\n",
      "1680   915            17      agree\n",
      "1681   407            38    discuss\n",
      "1682  1066            27    discuss\n",
      "\n",
      "[1683 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame.to_csv(df, 'noun_count_vs_stance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
