{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "#Run nltk.download() if you encounter nltk issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bodies = pandas.read_csv('./fn_data/train_bodies.csv', index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_bodies(data, body_col):\n",
    "    \"\"\"\n",
    "    data: DataFrame\n",
    "    body_col : string\n",
    "    \n",
    "    data represents a DataFrame containing Body IDs and actual text bodies.\n",
    "    \n",
    "    Ex:    Body ID                                        articleBody\n",
    "    0           0  A small meteorite crashed into a wooded area i...\n",
    "    1           4  Last week we hinted at what was to come as Ebo...\n",
    "    2           5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
    "    3           6  Posting photos of a gun-toting child online, I...\n",
    "    4           7  At least 25 suspected Boko Haram insurgents we...\n",
    "    5           8  There is so much fake stuff on the Internet in...\n",
    "    6           9  (CNN) -- A meteorite crashed down in Managua, ...\n",
    "\n",
    "\n",
    "\n",
    "    body_col is the name of the column containing article text bodies\n",
    "    \n",
    "    Returns: dictionary such that {Body ID : Body Text}\n",
    "    \"\"\"\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for x in range(len(data[body_col])):\n",
    "        dictionary.update({data.iloc[x,0] : data.iloc[x,1]})\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bodydict = map_bodies(bodies, 'articleBody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dict(dictionary):\n",
    "    \"\"\"\n",
    "    dictionary : dictionary\n",
    "    \n",
    "    Takes in a dictionary containing mappings from Body ID to Body.\n",
    "    Returns a dictionary containing mappings from Body ID to Tokenized Bodies.\n",
    "    \"\"\"\n",
    "    new_dict = dict()\n",
    "    for x in dictionary:\n",
    "        #tokens = nltk.word_tokenize(dictionary.get(x))\n",
    "        tokens = preprocessing.get_tokenized_lemmas(dictionary.get(x))\n",
    "        new_dict.update({x:tokens})\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_tokens(dictionary):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary containing mappings from Body ID to tokenized bodies.\n",
    "    Returns a dictionary containing mappings from Body ID to tagged tokenized bodies.\n",
    "    \"\"\"\n",
    "    new_dict = dict()\n",
    "    for x in dictionary:\n",
    "        tagged = nltk.pos_tag(dictionary.get(x))\n",
    "        new_dict.update({x:tagged})\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headlines = pandas.read_csv('./fn_data/train_stances.csv', index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headline_dict = dict()\n",
    "    \n",
    "for x in range(len(headlines['Headline'])):\n",
    "    headline_dict.update({headlines.iloc[x,1] : headlines.iloc[x,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Mass Grave Points to a Student Massacre in Mexico'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_dict.get(712)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizeddict = tag_tokens(tokenize_dict(map_bodies(bodies, 'articleBody')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_dict = dict()\n",
    "for x in headline_dict:\n",
    "    master_dict.update({x:[headline_dict.get(x), tokenizeddict.get(x)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headline_embedding = nltk.pos_tag(nltk.word_tokenize(master_dict.get(295)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_nouns(headline, body):\n",
    "    noun_count = 0\n",
    "    head_nouns = set()\n",
    "    #head_embedding = nltk.pos_tag(nltk.word_tokenize(headline))\n",
    "    head_embedding = nltk.pos_tag(preprocessing.get_tokenized_lemmas(headline))\n",
    "    for x in head_embedding:\n",
    "        if x[1] == \"NN\" or x[1] == \"NNP\" or x[1] == \"NNS\":\n",
    "            head_nouns.add(x[0])\n",
    "    for y in body:\n",
    "        if y[1] == \"NN\" or y[1] == \"NNP\" or y[1] == \"NNS\":\n",
    "            if y[0] in head_nouns:\n",
    "                noun_count += 1\n",
    "    return noun_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complete_dictionary(bodydata, body_col, headlinedata):\n",
    "    dictionary = dict()\n",
    "    dictionary = map_bodies(bodydata, body_col)\n",
    "    dictionary = tag_tokens(tokenize_dict(dictionary))\n",
    "    \n",
    "    headline_dict = dict()\n",
    "    for x in range(len(headlinedata['Headline'])):\n",
    "        headline_dict.update({headlines.iloc[x,1] : headlines.iloc[x,0]})\n",
    "    \n",
    "    master_dict = dict()\n",
    "    for x in headline_dict:\n",
    "        master_dict.update({x:[headline_dict.get(x), dictionary.get(x), match_nouns(headline_dict.get(x), dictionary.get(x))]})\n",
    "    \n",
    "    return master_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = complete_dictionary(bodies, 'articleBody', headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "’6 Days Darkness in December 2014′ 100% Fake; NASA Confirmed 3 Days Total Darkness Hoax as Well\n"
     ]
    }
   ],
   "source": [
    "print(final.get(154)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stance_dict = dict()\n",
    "for x in range(len(headlines['Headline'])):\n",
    "    #Incorporating stance\n",
    "    stance_dict.update({headlines.iloc[x,1] : headlines.iloc[x,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = []\n",
    "noun_count = []\n",
    "stance = []\n",
    "for x in final:\n",
    "    ids.append(x)\n",
    "    noun_count.append(final.get(x)[2])\n",
    "    stance.append(stance_dict.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'ID': ids, 'Noun Matches': noun_count, 'Stance': stance}\n",
    "df = pandas.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID  Noun Matches     Stance\n",
      "0      712             0  unrelated\n",
      "1      158             0  unrelated\n",
      "2      137             0  unrelated\n",
      "3     1034             0  unrelated\n",
      "4     1923             1  unrelated\n",
      "5      154            14      agree\n",
      "6      962             2  unrelated\n",
      "7     2033             0  unrelated\n",
      "8     1739             0  unrelated\n",
      "9      882             8  unrelated\n",
      "10    2327             0  unrelated\n",
      "11    1468             4   disagree\n",
      "12    1003             0  unrelated\n",
      "13    2132            15      agree\n",
      "14      47             0  unrelated\n",
      "15     615             0  unrelated\n",
      "16    2463             1  unrelated\n",
      "17     295             1  unrelated\n",
      "18     570             0  unrelated\n",
      "19     608             0  unrelated\n",
      "20    1500            17   disagree\n",
      "21    1681             3  unrelated\n",
      "22    1545             0  unrelated\n",
      "23    1196             0  unrelated\n",
      "24    1014             4      agree\n",
      "25     633             2  unrelated\n",
      "26      56             0  unrelated\n",
      "27    1658             9    discuss\n",
      "28    1157             0  unrelated\n",
      "29     132             0  unrelated\n",
      "...    ...           ...        ...\n",
      "1653  1129            16    discuss\n",
      "1654   200            24    discuss\n",
      "1655   476            11      agree\n",
      "1656  2032            11    discuss\n",
      "1657   243            22    discuss\n",
      "1658   285             1      agree\n",
      "1659   877            46    discuss\n",
      "1660    76            18    discuss\n",
      "1661   307            23    discuss\n",
      "1662     9            10      agree\n",
      "1663   355            21      agree\n",
      "1664   302            16    discuss\n",
      "1665  1085             9      agree\n",
      "1666   219             6    discuss\n",
      "1667   352             8    discuss\n",
      "1668   159             5    discuss\n",
      "1669   907             1      agree\n",
      "1670   828            11      agree\n",
      "1671   146            19      agree\n",
      "1672   854            25    discuss\n",
      "1673   797            15      agree\n",
      "1674    74            11    discuss\n",
      "1675   135             6      agree\n",
      "1676   175             3    discuss\n",
      "1677   553            25      agree\n",
      "1678   464            10      agree\n",
      "1679   362            26      agree\n",
      "1680   915            14      agree\n",
      "1681   407            34    discuss\n",
      "1682  1066            27    discuss\n",
      "\n",
      "[1683 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas.DataFrame.to_csv(df, 'noun_count_vs_stance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['’',\n",
       " '6',\n",
       " 'days',\n",
       " 'darkness',\n",
       " 'in',\n",
       " 'december',\n",
       " '2014′',\n",
       " '100',\n",
       " '%',\n",
       " 'fake',\n",
       " ';',\n",
       " 'nasa',\n",
       " 'confirmed',\n",
       " '3',\n",
       " 'days',\n",
       " 'total',\n",
       " 'darkness',\n",
       " 'hoax',\n",
       " 'a',\n",
       " 'well']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.get_tokenized_lemmas(final.get(154)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'’6 Days Darkness in December 2014′ 100% Fake; NASA Confirmed 3 Days Total Darkness Hoax as Well'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.get(154)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
